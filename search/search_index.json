{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"distreqx \u00a4 distreqx (pronounced \"dist-rex\") is a JAX -based library providing implementations of distributions, bijectors, and tools for statistical and probabilistic machine learning with all benefits of jax (native GPU/TPU acceleration, differentiability, vectorization, distributing workloads, XLA compilation, etc.). The origin of this package is a reimplementation of distrax , (which is a subset of TensorFlow Probability (TFP) , with some new features and emphasis on jax compatibility) using equinox . As a result, much of the original code/comments/documentation/tests are directly taken or adapted from distrax (original distrax copyright available at end of README.) Current features include: Probability distributions Bijectors Installation \u00a4 pip install distreqx or git clone https://github.com/lockwo/distreqx.git cd distreqx pip install -e . Requires Python 3.9+, JAX 0.4.11+, and Equinox 0.11.0+. Documentation \u00a4 Available at https://lockwo.github.io/distreqx/. Quick example \u00a4 import jax from jax import numpy as jnp from distreqx import distributions key = jax . random . PRNGKey ( 1234 ) mu = jnp . array ([ - 1. , 0. , 1. ]) sigma = jnp . array ([ 0.1 , 0.2 , 0.3 ]) dist = distributions . MultivariateNormalDiag ( mu , sigma ) samples = dist . sample ( key ) print ( dist . log_prob ( samples )) Differences with Distrax \u00a4 No official support/interoperability with TFP The concept of a batch dimension is dropped. If you want to operate on a batch, use vmap (note, this can be used in construction as well, e.g. vmaping the construction of a ScalarAffine ) Broader pytree enablement Strict abstract/final design pattern","title":"distreqx"},{"location":"#distreqx","text":"distreqx (pronounced \"dist-rex\") is a JAX -based library providing implementations of distributions, bijectors, and tools for statistical and probabilistic machine learning with all benefits of jax (native GPU/TPU acceleration, differentiability, vectorization, distributing workloads, XLA compilation, etc.). The origin of this package is a reimplementation of distrax , (which is a subset of TensorFlow Probability (TFP) , with some new features and emphasis on jax compatibility) using equinox . As a result, much of the original code/comments/documentation/tests are directly taken or adapted from distrax (original distrax copyright available at end of README.) Current features include: Probability distributions Bijectors","title":"distreqx"},{"location":"#installation","text":"pip install distreqx or git clone https://github.com/lockwo/distreqx.git cd distreqx pip install -e . Requires Python 3.9+, JAX 0.4.11+, and Equinox 0.11.0+.","title":"Installation"},{"location":"#documentation","text":"Available at https://lockwo.github.io/distreqx/.","title":"Documentation"},{"location":"#quick-example","text":"import jax from jax import numpy as jnp from distreqx import distributions key = jax . random . PRNGKey ( 1234 ) mu = jnp . array ([ - 1. , 0. , 1. ]) sigma = jnp . array ([ 0.1 , 0.2 , 0.3 ]) dist = distributions . MultivariateNormalDiag ( mu , sigma ) samples = dist . sample ( key ) print ( dist . log_prob ( samples ))","title":"Quick example"},{"location":"#differences-with-distrax","text":"No official support/interoperability with TFP The concept of a batch dimension is dropped. If you want to operate on a batch, use vmap (note, this can be used in construction as well, e.g. vmaping the construction of a ScalarAffine ) Broader pytree enablement Strict abstract/final design pattern","title":"Differences with Distrax"},{"location":"api/bijectors/_bijector/","text":"Abstract Bijectors \u00a4 distreqx.bijectors._bijector.AbstractBijector \u00a4 Differentiable bijection that knows to compute its Jacobian determinant. A bijector implements a differentiable and bijective transformation f , whose inverse is also differentiable ( f is called a \"diffeomorphism\"). A bijector can be used to transform a continuous random variable X to a continuous random variable Y = f(X) in the context of TransformedDistribution . Typically, a bijector subclass will implement the following methods: forward_and_log_det(x) (required) inverse_and_log_det(y) (optional) The remaining methods are defined in terms of the above by default. Subclass requirements: Subclasses must ensure that f is differentiable and bijective, and that their methods correctly implement f^{-1} , J(f) and J(f^{-1}) . Distreqx will assume these properties hold, and will make no attempt to verify them. __init__ ( self ) \u00a4 Initialize self. See help(type(self)) for accurate signature. forward ( self , x : PyTree ) -> PyTree abstractmethod \u00a4 Computes \\(y = f(x)\\) . inverse ( self , y : PyTree ) -> PyTree abstractmethod \u00a4 Computes \\(x = f^{-1}(y)\\) . forward_log_det_jacobian ( self , x : PyTree ) -> PyTree abstractmethod \u00a4 Computes \\(\\log|\\det J(f)(x)|\\) . inverse_log_det_jacobian ( self , y : PyTree ) -> PyTree abstractmethod \u00a4 Computes \\(\\log|\\det J(f^{-1})(y)|\\) . forward_and_log_det ( self , x : PyTree ) -> tuple abstractmethod \u00a4 Computes \\(y = f(x)\\) and \\(\\log|\\det J(f)(x)|\\) . inverse_and_log_det ( self , y : Array ) -> tuple abstractmethod \u00a4 Computes \\(x = f^{-1}(y)\\) and \\(\\log|\\det J(f^{-1})(y)|\\) . same_as ( self , other ) -> bool abstractmethod \u00a4 Returns True if this bijector is guaranteed to be the same as other . distreqx.bijectors._bijector.AbstractInvLogDetJacBijector ( AbstractBijector ) \u00a4 AbstractBijector + concrete inverse_log_det_jacobian . inverse_log_det_jacobian ( self , y : PyTree ) -> PyTree \u00a4 Implements distreqx.bijectors._bijector.AbstractBijector.inverse_log_det_jacobian . distreqx.bijectors._bijector.AbstractFwdLogDetJacBijector ( AbstractBijector ) \u00a4 AbstractBijector + concrete forward_log_det_jacobian . forward_log_det_jacobian ( self , x : PyTree ) -> PyTree \u00a4 Implements distreqx.bijectors._bijector.AbstractBijector.forward_log_det_jacobian . distreqx.bijectors._bijector.AbstractFowardInverseBijector ( AbstractBijector ) \u00a4 AbstractBijector + concrete forward and reverse . forward ( self , x : PyTree ) -> PyTree \u00a4 Implements distreqx.bijectors._bijector.AbstractBijector.forward . inverse ( self , y : PyTree ) -> PyTree \u00a4 Implements distreqx.bijectors._bijector.AbstractBijector.inverse . distreqx.bijectors._linear.AbstractLinearBijector ( AbstractBijector ) \u00a4 Base class for linear bijectors. This class provides a base class for bijectors defined as f(x) = Ax , where A is a DxD matrix and x is a D -dimensional vector. matrix : Array property readonly \u00a4 The matrix A of the transformation. To be optionally implemented in a subclass. Returns: An array of shape (D, D) . __init__ ( self ) \u00a4 Initialize self. See help(type(self)) for accurate signature.","title":"Abstract Bijectors"},{"location":"api/bijectors/_bijector/#abstract-bijectors","text":"","title":"Abstract Bijectors"},{"location":"api/bijectors/_bijector/#distreqx.bijectors._bijector.AbstractBijector","text":"Differentiable bijection that knows to compute its Jacobian determinant. A bijector implements a differentiable and bijective transformation f , whose inverse is also differentiable ( f is called a \"diffeomorphism\"). A bijector can be used to transform a continuous random variable X to a continuous random variable Y = f(X) in the context of TransformedDistribution . Typically, a bijector subclass will implement the following methods: forward_and_log_det(x) (required) inverse_and_log_det(y) (optional) The remaining methods are defined in terms of the above by default. Subclass requirements: Subclasses must ensure that f is differentiable and bijective, and that their methods correctly implement f^{-1} , J(f) and J(f^{-1}) . Distreqx will assume these properties hold, and will make no attempt to verify them.","title":"AbstractBijector"},{"location":"api/bijectors/_bijector/#distreqx.bijectors._bijector.AbstractInvLogDetJacBijector","text":"AbstractBijector + concrete inverse_log_det_jacobian .","title":"AbstractInvLogDetJacBijector"},{"location":"api/bijectors/_bijector/#distreqx.bijectors._bijector.AbstractFwdLogDetJacBijector","text":"AbstractBijector + concrete forward_log_det_jacobian .","title":"AbstractFwdLogDetJacBijector"},{"location":"api/bijectors/_bijector/#distreqx.bijectors._bijector.AbstractFowardInverseBijector","text":"AbstractBijector + concrete forward and reverse .","title":"AbstractFowardInverseBijector"},{"location":"api/bijectors/_bijector/#distreqx.bijectors._linear.AbstractLinearBijector","text":"Base class for linear bijectors. This class provides a base class for bijectors defined as f(x) = Ax , where A is a DxD matrix and x is a D -dimensional vector.","title":"AbstractLinearBijector"},{"location":"api/bijectors/block/","text":"Block Bijector \u00a4 distreqx.bijectors.block.Block ( AbstractBijector ) \u00a4 A wrapper that promotes a bijector to a block bijector. A block bijector applies a bijector to a k-dimensional array of events, but considers that array of events to be a single event. In practical terms, this means that the log det Jacobian will be summed over its last k dimensions. For example, consider a scalar bijector (such as Tanh ) that operates on scalar events. We may want to apply this bijector identically to a 4D array of shape [N, H, W, C] representing a sequence of N images. Doing so naively with a vmap will produce a log det Jacobian of shape [N, H, W, C], because the scalar bijector will assume scalar events and so all 4 dimensions will be considered as batch. To promote the scalar bijector to a \"block scalar\" that operates on the 3D arrays can be done by Block(bijector, ndims=3) . Then, applying the block bijector will produce a log det Jacobian of shape [N] as desired. In general, suppose bijector operates on n-dimensional events. Then, Block(bijector, k) will promote bijector to a block bijector that operates on (k + n)-dimensional events, summing the log det Jacobian over its last k dimensions. __init__ ( self , bijector : AbstractBijector , ndims : int ) \u00a4 Initializes a Block. Arguments: bijector : the bijector to be promoted to a block bijector. It can be a distreqx bijector or a callable to be wrapped by Lambda . ndims : number of dimensions to promote to event dimensions.","title":"Block Bijector"},{"location":"api/bijectors/block/#block-bijector","text":"","title":"Block Bijector"},{"location":"api/bijectors/block/#distreqx.bijectors.block.Block","text":"A wrapper that promotes a bijector to a block bijector. A block bijector applies a bijector to a k-dimensional array of events, but considers that array of events to be a single event. In practical terms, this means that the log det Jacobian will be summed over its last k dimensions. For example, consider a scalar bijector (such as Tanh ) that operates on scalar events. We may want to apply this bijector identically to a 4D array of shape [N, H, W, C] representing a sequence of N images. Doing so naively with a vmap will produce a log det Jacobian of shape [N, H, W, C], because the scalar bijector will assume scalar events and so all 4 dimensions will be considered as batch. To promote the scalar bijector to a \"block scalar\" that operates on the 3D arrays can be done by Block(bijector, ndims=3) . Then, applying the block bijector will produce a log det Jacobian of shape [N] as desired. In general, suppose bijector operates on n-dimensional events. Then, Block(bijector, k) will promote bijector to a block bijector that operates on (k + n)-dimensional events, summing the log det Jacobian over its last k dimensions.","title":"Block"},{"location":"api/bijectors/chain/","text":"Chain Bijector \u00a4 distreqx.bijectors.chain.Chain ( AbstractFwdLogDetJacBijector , AbstractInvLogDetJacBijector ) \u00a4 Composition of a sequence of bijectors into a single bijector. Bijectors are composable: if f and g are bijectors, then g o f is also a bijector. Given a sequence of bijectors [f1, ..., fN] , this class implements the bijector defined by fN o ... o f1 . NOTE: the bijectors are applied in reverse order from the order they appear in the sequence. For example, consider the following code where f and g are two bijectors: layers = [] layers . append ( f ) layers . append ( g ) bijector = distrax . Chain ( layers ) y = bijector . forward ( x ) The above code will transform x by first applying g , then f , so that y = f(g(x)) . __init__ ( self , bijectors : Sequence [ AbstractBijector ]) \u00a4 Initializes a Chain bijector. Arguments: bijectors : a sequence of bijectors to be composed into one. Each bijector can be a distreqx bijector or a callable to be wrapped by Lambda . The sequence must contain at least one bijector.","title":"Chain Bijector"},{"location":"api/bijectors/chain/#chain-bijector","text":"","title":"Chain Bijector"},{"location":"api/bijectors/chain/#distreqx.bijectors.chain.Chain","text":"Composition of a sequence of bijectors into a single bijector. Bijectors are composable: if f and g are bijectors, then g o f is also a bijector. Given a sequence of bijectors [f1, ..., fN] , this class implements the bijector defined by fN o ... o f1 . NOTE: the bijectors are applied in reverse order from the order they appear in the sequence. For example, consider the following code where f and g are two bijectors: layers = [] layers . append ( f ) layers . append ( g ) bijector = distrax . Chain ( layers ) y = bijector . forward ( x ) The above code will transform x by first applying g , then f , so that y = f(g(x)) .","title":"Chain"},{"location":"api/bijectors/diag_linear/","text":"Diagonal Linear Bijector \u00a4 distreqx.bijectors.diag_linear.DiagLinear ( AbstractLinearBijector ) \u00a4 Linear bijector with a diagonal weight matrix. The bijector is defined as f(x) = Ax where A is a DxD diagonal matrix. Additional dimensions, if any, index batches. The Jacobian determinant is trivially computed by taking the product of the diagonal entries in A . The inverse transformation x = f^{-1}(y) is computed element-wise. The bijector is invertible if and only if the diagonal entries of A are all non-zero. It is the responsibility of the user to make sure that this is the case; the class will make no attempt to verify that the bijector is invertible. __init__ ( self , diag : Array ) \u00a4 Initializes the bijector. Arguments: diag : a vector of length D, the diagonal of matrix A .","title":"Diagonal Linear Bijector"},{"location":"api/bijectors/diag_linear/#diagonal-linear-bijector","text":"","title":"Diagonal Linear Bijector"},{"location":"api/bijectors/diag_linear/#distreqx.bijectors.diag_linear.DiagLinear","text":"Linear bijector with a diagonal weight matrix. The bijector is defined as f(x) = Ax where A is a DxD diagonal matrix. Additional dimensions, if any, index batches. The Jacobian determinant is trivially computed by taking the product of the diagonal entries in A . The inverse transformation x = f^{-1}(y) is computed element-wise. The bijector is invertible if and only if the diagonal entries of A are all non-zero. It is the responsibility of the user to make sure that this is the case; the class will make no attempt to verify that the bijector is invertible.","title":"DiagLinear"},{"location":"api/bijectors/scalar_affine/","text":"Scalar Affine Bijector \u00a4 distreqx.bijectors.scalar_affine.ScalarAffine ( AbstractBijector ) \u00a4 An affine bijector that acts elementwise. The bijector is defined as follows: Forward: y = scale * x + shift Forward Jacobian determinant: log|det J(x)| = log|scale| Inverse: x = (y - shift) / scale Inverse Jacobian determinant: log|det J(y)| = -log|scale| where scale and shift are the bijector's parameters. __init__ ( self , shift : Array , scale : Optional [ Array ] = None , log_scale : Optional [ Array ] = None ) \u00a4 Initializes a ScalarAffine bijector. Arguments: shift : the bijector's shift parameter. scale : the bijector's scale parameter. NOTE: scale must be non-zero, otherwise the bijector is not invertible. It is the user's responsibility to make sure scale is non-zero; the class will make no attempt to verify this. log_scale : the log of the scale parameter. If specified, the bijector's scale is set equal to exp(log_scale) . Unlike scale , log_scale is an unconstrained parameter. NOTE: either scale or log_scale can be specified, but not both. If neither is specified, the bijector's scale will default to 1. Raises: ValueError : if both scale and log_scale are not None.","title":"Scalar Affine Bijector"},{"location":"api/bijectors/scalar_affine/#scalar-affine-bijector","text":"","title":"Scalar Affine Bijector"},{"location":"api/bijectors/scalar_affine/#distreqx.bijectors.scalar_affine.ScalarAffine","text":"An affine bijector that acts elementwise. The bijector is defined as follows: Forward: y = scale * x + shift Forward Jacobian determinant: log|det J(x)| = log|scale| Inverse: x = (y - shift) / scale Inverse Jacobian determinant: log|det J(y)| = -log|scale| where scale and shift are the bijector's parameters.","title":"ScalarAffine"},{"location":"api/bijectors/shift/","text":"Shift Bijector \u00a4 distreqx.bijectors.shift.Shift ( AbstractBijector ) \u00a4 Bijector that translates its input elementwise. The bijector is defined as follows: Forward: y = x + shift Forward Jacobian determinant: log|det J(x)| = 0 Inverse: x = y - shift Inverse Jacobian determinant: log|det J(y)| = 0 where shift parameterizes the bijector. __init__ ( self , shift : Array ) \u00a4 Initializes a Shift bijector. Arguments: shift : the bijector's shift parameter.","title":"Shift  Bijector"},{"location":"api/bijectors/shift/#shift-bijector","text":"","title":"Shift Bijector"},{"location":"api/bijectors/shift/#distreqx.bijectors.shift.Shift","text":"Bijector that translates its input elementwise. The bijector is defined as follows: Forward: y = x + shift Forward Jacobian determinant: log|det J(x)| = 0 Inverse: x = y - shift Inverse Jacobian determinant: log|det J(y)| = 0 where shift parameterizes the bijector.","title":"Shift"},{"location":"api/bijectors/sigmoid/","text":"Sigmoid Bijector \u00a4 distreqx.bijectors.sigmoid.Sigmoid ( AbstractFowardInverseBijector , AbstractInvLogDetJacBijector ) \u00a4 A bijector that computes the logistic sigmoid. The log-determinant implementation in this bijector is more numerically stable than relying on the automatic differentiation approach used by Lambda, so this bijector should be preferred over Lambda(jax.nn.sigmoid) where possible. Note that the underlying implementation of jax.nn.sigmoid used by the forward function of this bijector does not support inputs of integer type. To invoke the forward function of this bijector on an argument of integer type, it should first be cast explicitly to a floating point type. When the absolute value of the input is large, Sigmoid becomes close to a constant, so that it is not possible to recover the input x from the output y within machine precision. In cases where it is needed to compute both the forward mapping and the backward mapping one after the other to recover the original input x , it is the user's responsibility to simplify the operation to avoid numerical issues. One example of such case is to use the bijector within a Transformed distribution and to obtain the log-probability of samples obtained from the distribution's sample method. For values of the samples for which it is not possible to apply the inverse bijector accurately, log_prob returns NaN. This can be avoided by using sample_and_log_prob instead of sample followed by log_prob .","title":"Sigmoid Bijector"},{"location":"api/bijectors/sigmoid/#sigmoid-bijector","text":"","title":"Sigmoid Bijector"},{"location":"api/bijectors/sigmoid/#distreqx.bijectors.sigmoid.Sigmoid","text":"A bijector that computes the logistic sigmoid. The log-determinant implementation in this bijector is more numerically stable than relying on the automatic differentiation approach used by Lambda, so this bijector should be preferred over Lambda(jax.nn.sigmoid) where possible. Note that the underlying implementation of jax.nn.sigmoid used by the forward function of this bijector does not support inputs of integer type. To invoke the forward function of this bijector on an argument of integer type, it should first be cast explicitly to a floating point type. When the absolute value of the input is large, Sigmoid becomes close to a constant, so that it is not possible to recover the input x from the output y within machine precision. In cases where it is needed to compute both the forward mapping and the backward mapping one after the other to recover the original input x , it is the user's responsibility to simplify the operation to avoid numerical issues. One example of such case is to use the bijector within a Transformed distribution and to obtain the log-probability of samples obtained from the distribution's sample method. For values of the samples for which it is not possible to apply the inverse bijector accurately, log_prob returns NaN. This can be avoided by using sample_and_log_prob instead of sample followed by log_prob .","title":"Sigmoid"},{"location":"api/bijectors/tanh/","text":"TanH Bijector \u00a4 distreqx.bijectors.tanh.Tanh ( AbstractFowardInverseBijector , AbstractInvLogDetJacBijector ) \u00a4 A bijector that computes the hyperbolic tangent. The log-determinant implementation in this bijector is more numerically stable than relying on the automatic differentiation approach used by Lambda, so this bijector should be preferred over Lambda(jnp.tanh) where possible. When the absolute value of the input is large, Tanh becomes close to a constant, so that it is not possible to recover the input x from the output y within machine precision. In cases where it is needed to compute both the forward mapping and the backward mapping one after the other to recover the original input x , it is the user's responsibility to simplify the operation to avoid numerical issues. One example of such case is to use the bijector within a Transformed distribution and to obtain the log-probability of samples obtained from the distribution's sample method. For values of the samples for which it is not possible to apply the inverse bijector accurately, log_prob returns NaN. This can be avoided by using sample_and_log_prob instead of sample followed by log_prob .","title":"TanH Bijector"},{"location":"api/bijectors/tanh/#tanh-bijector","text":"","title":"TanH Bijector"},{"location":"api/bijectors/tanh/#distreqx.bijectors.tanh.Tanh","text":"A bijector that computes the hyperbolic tangent. The log-determinant implementation in this bijector is more numerically stable than relying on the automatic differentiation approach used by Lambda, so this bijector should be preferred over Lambda(jnp.tanh) where possible. When the absolute value of the input is large, Tanh becomes close to a constant, so that it is not possible to recover the input x from the output y within machine precision. In cases where it is needed to compute both the forward mapping and the backward mapping one after the other to recover the original input x , it is the user's responsibility to simplify the operation to avoid numerical issues. One example of such case is to use the bijector within a Transformed distribution and to obtain the log-probability of samples obtained from the distribution's sample method. For values of the samples for which it is not possible to apply the inverse bijector accurately, log_prob returns NaN. This can be avoided by using sample_and_log_prob instead of sample followed by log_prob .","title":"Tanh"},{"location":"api/bijectors/triangular_linear/","text":"Triangular Linear Bijector \u00a4 distreqx.bijectors.triangular_linear.TriangularLinear ( AbstractLinearBijector ) \u00a4 A linear bijector whose weight matrix is triangular. The bijector is defined as f(x) = Ax where A is a DxD triangular matrix. The Jacobian determinant can be computed in O(D) as follows: log|det J(x)| = log|det A| = sum(log|diag(A)|) The inverse is computed in O(D^2) by solving the triangular system Ax = y . The bijector is invertible if and only if all diagonal elements of A are non-zero. It is the responsibility of the user to make sure that this is the case; the class will make no attempt to verify that the bijector is invertible. __init__ ( self , matrix : Array , is_lower : bool = True ) \u00a4 Initializes a TriangularLinear bijector. Arguments: matrix : a square matrix whose triangular part defines A . Can also be a batch of matrices. Whether A is the lower or upper triangular part of matrix is determined by is_lower . is_lower : if True, A is set to the lower triangular part of matrix . If False, A is set to the upper triangular part of matrix .","title":"Triangular Linear Bijector"},{"location":"api/bijectors/triangular_linear/#triangular-linear-bijector","text":"","title":"Triangular Linear Bijector"},{"location":"api/bijectors/triangular_linear/#distreqx.bijectors.triangular_linear.TriangularLinear","text":"A linear bijector whose weight matrix is triangular. The bijector is defined as f(x) = Ax where A is a DxD triangular matrix. The Jacobian determinant can be computed in O(D) as follows: log|det J(x)| = log|det A| = sum(log|diag(A)|) The inverse is computed in O(D^2) by solving the triangular system Ax = y . The bijector is invertible if and only if all diagonal elements of A are non-zero. It is the responsibility of the user to make sure that this is the case; the class will make no attempt to verify that the bijector is invertible.","title":"TriangularLinear"},{"location":"api/distributions/_distribution/","text":"Abstract Distributions \u00a4 distreqx.distributions._distribution.AbstractDistribution \u00a4 Base class for all distreqx distributions. log_prob ( self , value : PyTree [ Array ]) -> PyTree [ Array ] abstractmethod \u00a4 Calculates the log probability of an event. Arguments: value : An event. Returns: The log probability log P(value). prob ( self , value : PyTree [ Array ]) -> PyTree [ Array ] abstractmethod \u00a4 Calculates the probability of an event. Arguments: value : An event. Returns: The probability P(value). cdf ( self , value : PyTree [ Array ]) -> PyTree [ Array ] abstractmethod \u00a4 Evaluates the cumulative distribution function at value . Arguments: value : An event. Returns: The CDF evaluated at value, i.e. P[X <= value]. survival_function ( self , value : PyTree [ Array ]) -> PyTree [ Array ] abstractmethod \u00a4 Evaluates the survival function at value . Note that by default we use a numerically not necessarily stable definition of the survival function in terms of the CDF. More stable definitions should be implemented in subclasses for distributions for which they exist. Arguments: value : An event. Returns: The survival function evaluated at value , i.e. P[X > value] log_survival_function ( self , value : PyTree [ Array ]) -> PyTree [ Array ] abstractmethod \u00a4 Evaluates the log of the survival function at value . Note that by default we use a numerically not necessarily stable definition of the log of the survival function in terms of the CDF. More stable definitions should be implemented in subclasses for distributions for which they exist. Arguments: value : An event. Returns: The log of the survival function evaluated at value , i.e. log P[X > value] kl_divergence ( self , other_dist , ** kwargs ) -> PyTree [ Array ] abstractmethod \u00a4 Calculates the KL divergence to another distribution. Arguments: other_dist : A compatible distreqx Distribution. kwargs : Additional kwargs. Returns: The KL divergence KL(self || other_dist) . cross_entropy ( self , other_dist , ** kwargs ) -> Array \u00a4 Calculates the cross entropy to another distribution. Arguments: other_dist : A compatible distreqx Distribution. kwargs : Additional kwargs. Returns: The cross entropy H(self || other_dist) . distreqx.distributions._distribution.AbstractSampleLogProbDistribution ( AbstractDistribution ) \u00a4 Abstract distribution + concrete sample_and_log_prob . distreqx.distributions._distribution.AbstractProbDistribution ( AbstractDistribution ) \u00a4 Abstract distribution + concrete prob . prob ( self , value : PyTree [ Array ]) -> PyTree [ Array ] \u00a4 Implements distreqx.distributions._distribution.AbstractDistribution.prob . distreqx.distributions._distribution.AbstractCDFDistribution ( AbstractDistribution ) \u00a4 Abstract distribution + concrete cdf . cdf ( self , value : PyTree [ Array ]) -> PyTree [ Array ] \u00a4 Implements distreqx.distributions._distribution.AbstractDistribution.cdf . distreqx.distributions._distribution.AbstractSTDDistribution ( AbstractDistribution ) \u00a4 Abstract distribution + concrete stddev . stddev ( self ) -> PyTree [ Array ] \u00a4 Calculate the standard deviation. distreqx.distributions._distribution.AbstractSurvivalDistribution ( AbstractDistribution ) \u00a4 Abstract distribution + concrete survival_function and log_survival_function . survival_function ( self , value : PyTree [ Array ]) -> PyTree [ Array ] \u00a4 Implements distreqx.distributions._distribution.AbstractDistribution.survival_function . log_survival_function ( self , value : PyTree [ Array ]) -> PyTree [ Array ] \u00a4 Implements distreqx.distributions._distribution.AbstractDistribution.log_survival_function . distreqx.distributions.transformed.AbstractTransformed ( AbstractSurvivalDistribution , AbstractProbDistribution ) \u00a4 Distribution of a random variable transformed by a bijective function. Let X be a continuous random variable and Y = f(X) be a random variable transformed by a differentiable bijection f (a \"bijector\"). Given the distribution of X (the \"base distribution\") and the bijector f , this class implements the distribution of Y (also known as the pushforward of the base distribution through f ). The probability density of Y can be computed by: log p(y) = log p(x) - log|det J(f)(x)| where p(x) is the probability density of X (the \"base density\") and J(f)(x) is the Jacobian matrix of f , both evaluated at x = f^{-1}(y) . Sampling from a Transformed distribution involves two steps: sampling from the base distribution x ~ p(x) and then evaluating y = f(x) . For example: dist = distrax . Normal ( loc = 0. , scale = 1. ) bij = distrax . ScalarAffine ( shift = jnp . asarray ([ 3. , 3. , 3. ])) transformed_dist = distrax . Transformed ( distribution = dist , bijector = bij ) samples = transformed_dist . sample ( jax . random . PRNGKey ( 0 )) print ( samples ) # [2.7941577, 2.7941577, 2.7941577] This assumes that the forward function of the bijector is traceable; that is, it is a pure function that does not contain run-time branching. Functions that do not strictly meet this requirement can still be used, but we cannot guarantee that the shapes, dtype, and KL computations involving the transformed distribution can be correctly obtained. distribution property readonly \u00a4 The base distribution. bijector property readonly \u00a4 The bijector representing the transformation. dtype : dtype property readonly \u00a4 See Distribution.dtype . event_shape : tuple property readonly \u00a4 See Distribution.event_shape . log_prob ( self , value : Array ) -> Array \u00a4 See Distribution.log_prob . sample ( self , key : PRNGKeyArray ) -> Array \u00a4 Return a sample. sample_and_log_prob ( self , key : PRNGKeyArray ) -> tuple \u00a4 Return a sample and log prob. This function is more efficient than calling sample and log_prob separately, because it uses only the forward methods of the bijector. It also works for bijectors that don't implement inverse methods. Arguments: key : PRNG key. Returns: A tuple of a sample and its log probs. entropy ( self , input_hint : Optional [ Array ] = None ) -> Array \u00a4 Calculates the Shannon entropy (in Nats). Only works for bijectors with constant Jacobian determinant. Arguments: input_hint : an example sample from the base distribution, used to compute the constant forward log-determinant. If not specified, it is computed using a zero array of the shape and dtype of a sample from the base distribution. Returns: The entropy of the distribution. Raises: NotImplementedError : if bijector's Jacobian determinant is not known to be constant. distreqx.distributions.mvn_from_bijector.AbstractMultivariateNormalFromBijector ( AbstractTransformed ) \u00a4 AbstractMultivariateNormalFromBijector() scale : AbstractLinearBijector property readonly \u00a4 The scale bijector. loc : Array property readonly \u00a4 The loc parameter of the distribution. covariance ( self ) -> Array \u00a4 Calculates the covariance matrix. Returns: - The covariance matrix, of shape k x k . variance ( self ) -> Array \u00a4 Calculates the variance of all one-dimensional marginals. stddev ( self ) -> Array \u00a4 Calculates the standard deviation (the square root of the variance). kl_divergence ( self , other_dist , ** kwargs ) -> Array \u00a4 Calculates the KL divergence to another distribution. Arguments: other_dist : A compatible disteqx distribution. kwargs : Additional kwargs. Returns: The KL divergence KL(self || other_dist) .","title":"Abstract Distributions"},{"location":"api/distributions/_distribution/#abstract-distributions","text":"","title":"Abstract Distributions"},{"location":"api/distributions/_distribution/#distreqx.distributions._distribution.AbstractDistribution","text":"Base class for all distreqx distributions.","title":"AbstractDistribution"},{"location":"api/distributions/_distribution/#distreqx.distributions._distribution.AbstractSampleLogProbDistribution","text":"Abstract distribution + concrete sample_and_log_prob .","title":"AbstractSampleLogProbDistribution"},{"location":"api/distributions/_distribution/#distreqx.distributions._distribution.AbstractProbDistribution","text":"Abstract distribution + concrete prob .","title":"AbstractProbDistribution"},{"location":"api/distributions/_distribution/#distreqx.distributions._distribution.AbstractCDFDistribution","text":"Abstract distribution + concrete cdf .","title":"AbstractCDFDistribution"},{"location":"api/distributions/_distribution/#distreqx.distributions._distribution.AbstractSTDDistribution","text":"Abstract distribution + concrete stddev .","title":"AbstractSTDDistribution"},{"location":"api/distributions/_distribution/#distreqx.distributions._distribution.AbstractSurvivalDistribution","text":"Abstract distribution + concrete survival_function and log_survival_function .","title":"AbstractSurvivalDistribution"},{"location":"api/distributions/_distribution/#distreqx.distributions.transformed.AbstractTransformed","text":"Distribution of a random variable transformed by a bijective function. Let X be a continuous random variable and Y = f(X) be a random variable transformed by a differentiable bijection f (a \"bijector\"). Given the distribution of X (the \"base distribution\") and the bijector f , this class implements the distribution of Y (also known as the pushforward of the base distribution through f ). The probability density of Y can be computed by: log p(y) = log p(x) - log|det J(f)(x)| where p(x) is the probability density of X (the \"base density\") and J(f)(x) is the Jacobian matrix of f , both evaluated at x = f^{-1}(y) . Sampling from a Transformed distribution involves two steps: sampling from the base distribution x ~ p(x) and then evaluating y = f(x) . For example: dist = distrax . Normal ( loc = 0. , scale = 1. ) bij = distrax . ScalarAffine ( shift = jnp . asarray ([ 3. , 3. , 3. ])) transformed_dist = distrax . Transformed ( distribution = dist , bijector = bij ) samples = transformed_dist . sample ( jax . random . PRNGKey ( 0 )) print ( samples ) # [2.7941577, 2.7941577, 2.7941577] This assumes that the forward function of the bijector is traceable; that is, it is a pure function that does not contain run-time branching. Functions that do not strictly meet this requirement can still be used, but we cannot guarantee that the shapes, dtype, and KL computations involving the transformed distribution can be correctly obtained.","title":"AbstractTransformed"},{"location":"api/distributions/_distribution/#distreqx.distributions.mvn_from_bijector.AbstractMultivariateNormalFromBijector","text":"AbstractMultivariateNormalFromBijector()","title":"AbstractMultivariateNormalFromBijector"},{"location":"api/distributions/bernoulli/","text":"Bernoulli \u00a4 distreqx.distributions.bernoulli.Bernoulli ( AbstractSampleLogProbDistribution , AbstractSTDDistribution , AbstractSurvivalDistribution ) \u00a4 Bernoulli distribution of shape dims. Bernoulli distribution with parameter probs , the probability of outcome 1 . __init__ ( self , logits : Optional [ Array ] = None , probs : Optional [ Array ] = None ) \u00a4 Initializes a Bernoulli distribution. Arguments: logits : Logit transform of the probability of a 1 event ( 0 otherwise), i.e. probs = sigmoid(logits) . Only one of logits or probs can be specified. probs : Probability of a 1 event ( 0 otherwise). Only one of logits or probs can be specified.","title":"Bernoulli"},{"location":"api/distributions/bernoulli/#bernoulli","text":"","title":"Bernoulli"},{"location":"api/distributions/bernoulli/#distreqx.distributions.bernoulli.Bernoulli","text":"Bernoulli distribution of shape dims. Bernoulli distribution with parameter probs , the probability of outcome 1 .","title":"Bernoulli"},{"location":"api/distributions/beta/","text":"Beta \u00a4 distreqx.distributions.beta.Beta ( AbstractSampleLogProbDistribution , AbstractSTDDistribution , AbstractProbDistribution , AbstractSurvivalDistribution ) \u00a4 Beta distribution with parameters alpha and beta . The PDF of a Beta distributed random variable X is defined on the interval 0 <= X <= 1 and has the form: p(x; alpha, beta) = x ** {alpha - 1} * (1 - x) ** (beta - 1) / B(alpha, beta) where B(alpha, beta) is the beta function, and the alpha, beta > 0 are the shape parameters. Note that the support of the distribution does not include x = 0 or x = 1 if alpha < 1 or beta < 1 , respectively. __init__ ( self , alpha : Union [ float , Array ], beta : Union [ float , Array ]) \u00a4 Initializes a Beta distribution. Arguments: alpha : Shape parameter alpha of the distribution. Must be positive. beta : Shape parameter beta of the distribution. Must be positive.","title":"Beta"},{"location":"api/distributions/beta/#beta","text":"","title":"Beta"},{"location":"api/distributions/beta/#distreqx.distributions.beta.Beta","text":"Beta distribution with parameters alpha and beta . The PDF of a Beta distributed random variable X is defined on the interval 0 <= X <= 1 and has the form: p(x; alpha, beta) = x ** {alpha - 1} * (1 - x) ** (beta - 1) / B(alpha, beta) where B(alpha, beta) is the beta function, and the alpha, beta > 0 are the shape parameters. Note that the support of the distribution does not include x = 0 or x = 1 if alpha < 1 or beta < 1 , respectively.","title":"Beta"},{"location":"api/distributions/categorical/","text":"Categorical \u00a4 distreqx.distributions.categorical.Categorical ( AbstractSTDDistribution , AbstractSampleLogProbDistribution , AbstractSurvivalDistribution ) \u00a4 Categorical distribution over integers. The Categorical distribution is parameterized by either probabilities ( probs ) or unormalized log-probabilities ( logits ) of a set of K classes. It is defined over the integers {0, 1, ..., K-1} . __init__ ( self , logits : Optional [ Array ] = None , probs : Optional [ Array ] = None ) \u00a4 Initializes a Categorical distribution. Arguments: logits : Logit transform of the probability of each category. Only one of logits or probs can be specified. probs : Probability of each category. Only one of logits or probs can be specified. distreqx.distributions.one_hot_categorical.OneHotCategorical ( AbstractSTDDistribution , AbstractSampleLogProbDistribution , AbstractSurvivalDistribution ) \u00a4 OneHotCategorical distribution. __init__ ( self , logits : Optional [ Array ] = None , probs : Optional [ Array ] = None ) \u00a4 Initializes a OneHotCategorical distribution. Arguments: logits : Logit transform of the probability of each category. Only one of logits or probs can be specified. probs : Probability of each category. Only one of logits or probs can be specified.","title":"Categorical"},{"location":"api/distributions/categorical/#categorical","text":"","title":"Categorical"},{"location":"api/distributions/categorical/#distreqx.distributions.categorical.Categorical","text":"Categorical distribution over integers. The Categorical distribution is parameterized by either probabilities ( probs ) or unormalized log-probabilities ( logits ) of a set of K classes. It is defined over the integers {0, 1, ..., K-1} .","title":"Categorical"},{"location":"api/distributions/categorical/#distreqx.distributions.one_hot_categorical.OneHotCategorical","text":"OneHotCategorical distribution.","title":"OneHotCategorical"},{"location":"api/distributions/independent/","text":"Independent Distribution \u00a4 distreqx.distributions.independent.Independent ( AbstractProbDistribution , AbstractCDFDistribution , AbstractSurvivalDistribution ) \u00a4 Independent distribution obtained from child distributions. __init__ ( self , distribution : AbstractDistribution ) \u00a4 Initializes an Independent distribution. Arguments: distribution : Base distribution instance.","title":"Independent Distribution"},{"location":"api/distributions/independent/#independent-distribution","text":"","title":"Independent Distribution"},{"location":"api/distributions/independent/#distreqx.distributions.independent.Independent","text":"Independent distribution obtained from child distributions.","title":"Independent"},{"location":"api/distributions/mixture_same_family/","text":"Mixture Distribution \u00a4 distreqx.distributions.mixture_same_family.MixtureSameFamily ( AbstractSTDDistribution , AbstractSampleLogProbDistribution , AbstractSurvivalDistribution , AbstractProbDistribution , AbstractCDFDistribution ) \u00a4 Mixture with components provided from a single vmapped distribution. __init__ ( self , mixture_distribution : Categorical , components_distribution : AbstractDistribution ) \u00a4 Initializes a mixture distribution for components of a shared family. Arguments : mixture_distribution : Distribution over selecting components. components_distribution : Component distribution. posterior_marginal ( self , observation : Array ) -> Categorical \u00a4 Generate the posterior distribution given a datapoint. Arguments: observation : the data point to compute the distribution over Returns: The computed categorical distribution posterior_mode ( self , observation : Array ) -> Array \u00a4 Compute the most likely component a data point falls into. Arguments: observation : the data point to compute the mode of Returns: The computed mode","title":"Mixture Distribution"},{"location":"api/distributions/mixture_same_family/#mixture-distribution","text":"","title":"Mixture Distribution"},{"location":"api/distributions/mixture_same_family/#distreqx.distributions.mixture_same_family.MixtureSameFamily","text":"Mixture with components provided from a single vmapped distribution.","title":"MixtureSameFamily"},{"location":"api/distributions/mvn_diag/","text":"Diagonal Multivariate Normal \u00a4 distreqx.distributions.mvn_diag.MultivariateNormalDiag ( AbstractMultivariateNormalFromBijector ) \u00a4 Multivariate normal distribution on R^k with diagonal covariance. __init__ ( self , loc : Optional [ Array ] = None , scale_diag : Optional [ Array ] = None ) \u00a4 Initializes a MultivariateNormalDiag distribution. Arguments: loc : Mean vector of the distribution. If not specified, it defaults to zeros. At least one of loc and scale_diag must be specified. scale_diag : Vector of standard deviations. If not specified, it defaults to ones. At least one of loc and scale_diag must be specified.","title":"Diagonal Multivariate Normal"},{"location":"api/distributions/mvn_diag/#diagonal-multivariate-normal","text":"","title":"Diagonal Multivariate Normal"},{"location":"api/distributions/mvn_diag/#distreqx.distributions.mvn_diag.MultivariateNormalDiag","text":"Multivariate normal distribution on R^k with diagonal covariance.","title":"MultivariateNormalDiag"},{"location":"api/distributions/mvn_from_bijector/","text":"Multivariate Normal from Bijector \u00a4 distreqx.distributions.mvn_from_bijector.MultivariateNormalFromBijector ( AbstractMultivariateNormalFromBijector ) \u00a4 Multivariate normal distribution on R^k . The multivariate normal over x is characterized by an invertible affine transformation x = f(z) = A @ z + b , where z is a random variable that follows a standard multivariate normal on R^k , i.e., p(z) = N(0, I_k) , A is a k x k transformation matrix, and b is a k -dimensional vector. The resulting PDF on x is a multivariate normal, p(x) = N(b, C) , where C = A @ A.T is the covariance matrix. The transformation x = f(z) must be specified by a linear scale bijector implementing the operation A @ z and a shift (or location) term b . __init__ ( self , loc : Array , scale : AbstractLinearBijector ) \u00a4 Initializes the distribution. Arguments: loc : The term b , i.e., the mean of the multivariate normal distribution. scale : The bijector specifying the linear transformation A @ z , as described in the class docstring.","title":"Multivariate Normal from Bijector"},{"location":"api/distributions/mvn_from_bijector/#multivariate-normal-from-bijector","text":"","title":"Multivariate Normal from Bijector"},{"location":"api/distributions/mvn_from_bijector/#distreqx.distributions.mvn_from_bijector.MultivariateNormalFromBijector","text":"Multivariate normal distribution on R^k . The multivariate normal over x is characterized by an invertible affine transformation x = f(z) = A @ z + b , where z is a random variable that follows a standard multivariate normal on R^k , i.e., p(z) = N(0, I_k) , A is a k x k transformation matrix, and b is a k -dimensional vector. The resulting PDF on x is a multivariate normal, p(x) = N(b, C) , where C = A @ A.T is the covariance matrix. The transformation x = f(z) must be specified by a linear scale bijector implementing the operation A @ z and a shift (or location) term b .","title":"MultivariateNormalFromBijector"},{"location":"api/distributions/mvn_tri/","text":"Triangular Multivariate Normal \u00a4 distreqx.distributions.mvn_tri.MultivariateNormalTri ( AbstractMultivariateNormalFromBijector ) \u00a4 Multivariate normal distribution on R^k . The MultivariateNormalTri distribution is parameterized by a k -length location (mean) vector b and a (lower or upper) triangular scale matrix S of size k x k . The covariance matrix is C = S @ S.T . __init__ ( self , loc : Optional [ Array ] = None , scale_tri : Optional [ Array ] = None , is_lower : bool = True ) \u00a4 Initializes a MultivariateNormalTri distribution. Arguments: loc : Mean vector of the distribution of shape k . If not specified, it defaults to zeros. scale_tri : The scale matrix S . It must be a k x k triangular matrix. If scale_tri is not triangular, the entries above or below the main diagonal will be ignored. The parameter is_lower specifies if scale_tri is lower or upper triangular. It is the responsibility of the user to make sure that scale_tri only contains non-zero elements in its diagonal; this class makes no attempt to verify that. If scale_tri is not specified, it defaults to the identity. is_lower : Indicates if scale_tri is lower (if True) or upper (if False) triangular.","title":"Triangular Multivariate Normal"},{"location":"api/distributions/mvn_tri/#triangular-multivariate-normal","text":"","title":"Triangular Multivariate Normal"},{"location":"api/distributions/mvn_tri/#distreqx.distributions.mvn_tri.MultivariateNormalTri","text":"Multivariate normal distribution on R^k . The MultivariateNormalTri distribution is parameterized by a k -length location (mean) vector b and a (lower or upper) triangular scale matrix S of size k x k . The covariance matrix is C = S @ S.T .","title":"MultivariateNormalTri"},{"location":"api/distributions/normal/","text":"Normal \u00a4 distreqx.distributions.normal.Normal ( AbstractProbDistribution ) \u00a4 Normal distribution with location loc and scale parameters. __init__ ( self , loc : Array , scale : Array ) \u00a4 Initializes a Normal distribution. Arguments: loc : Mean of the distribution. scale : Standard deviation of the distribution. entropy ( self ) -> Array \u00a4 Calculates the Shannon entropy (in nats).","title":"Normal"},{"location":"api/distributions/normal/#normal","text":"","title":"Normal"},{"location":"api/distributions/normal/#distreqx.distributions.normal.Normal","text":"Normal distribution with location loc and scale parameters.","title":"Normal"},{"location":"api/distributions/transformed/","text":"Transformed \u00a4 distreqx.distributions.transformed.Transformed ( AbstractTransformed , AbstractSTDDistribution ) \u00a4 Distribution of a random variable transformed by a bijective function. Let X be a continuous random variable and Y = f(X) be a random variable transformed by a differentiable bijection f (a \"bijector\"). Given the distribution of X (the \"base distribution\") and the bijector f , this class implements the distribution of Y (also known as the pushforward of the base distribution through f ). The probability density of Y can be computed by: log p(y) = log p(x) - log|det J(f)(x)| where p(x) is the probability density of X (the \"base density\") and J(f)(x) is the Jacobian matrix of f , both evaluated at x = f^{-1}(y) . Sampling from a Transformed distribution involves two steps: sampling from the base distribution x ~ p(x) and then evaluating y = f(x) . For example: dist = distrax . Normal ( loc = 0. , scale = 1. ) bij = distrax . ScalarAffine ( shift = jnp . asarray ([ 3. , 3. , 3. ])) transformed_dist = distrax . Transformed ( distribution = dist , bijector = bij ) samples = transformed_dist . sample ( jax . random . PRNGKey ( 0 )) print ( samples ) # [2.7941577, 2.7941577, 2.7941577] This assumes that the forward function of the bijector is traceable; that is, it is a pure function that does not contain run-time branching. Functions that do not strictly meet this requirement can still be used, but we cannot guarantee that the shapes, dtype, and KL computations involving the transformed distribution can be correctly obtained. __init__ ( self , distribution : AbstractDistribution , bijector : AbstractBijector ) \u00a4 Initializes a Transformed distribution. Arguments: - distribution : the base distribution. - bijector : a differentiable bijective transformation. Can be a bijector or a callable to be wrapped by Lambda bijector. mean ( self ) -> Array \u00a4 Calculates the mean. mode ( self ) -> Array \u00a4 Calculates the mode. kl_divergence ( self , other_dist , ** kwargs ) -> Array \u00a4 Obtains the KL divergence between two Transformed distributions. This computes the KL divergence between two Transformed distributions with the same bijector. If the two Transformed distributions do not have the same bijector, an error is raised. To determine if the bijectors are equal, this method proceeds as follows: If both bijectors are the same instance of a distreqx bijector, then they are declared equal. If not the same instance, we check if they are equal according to their same_as predicate. Otherwise, the string representation of the Jaxpr of the forward method of each bijector is compared. If both string representations are equal, the bijectors are declared equal. Otherwise, the bijectors cannot be guaranteed to be equal and an error is raised. Arguments: other_dist : A Transformed distribution. input_hint : keyword argument, an example sample from the base distribution, used to trace the forward method. If not specified, it is computed using a zero array of the shape and dtype of a sample from the base distribution. Returns: KL(dist1 || dist2) . Raises: NotImplementedError : If bijectors are not known to be equal. ValueError : If the base distributions do not have the same event_shape .","title":"Transformed"},{"location":"api/distributions/transformed/#transformed","text":"","title":"Transformed"},{"location":"api/distributions/transformed/#distreqx.distributions.transformed.Transformed","text":"Distribution of a random variable transformed by a bijective function. Let X be a continuous random variable and Y = f(X) be a random variable transformed by a differentiable bijection f (a \"bijector\"). Given the distribution of X (the \"base distribution\") and the bijector f , this class implements the distribution of Y (also known as the pushforward of the base distribution through f ). The probability density of Y can be computed by: log p(y) = log p(x) - log|det J(f)(x)| where p(x) is the probability density of X (the \"base density\") and J(f)(x) is the Jacobian matrix of f , both evaluated at x = f^{-1}(y) . Sampling from a Transformed distribution involves two steps: sampling from the base distribution x ~ p(x) and then evaluating y = f(x) . For example: dist = distrax . Normal ( loc = 0. , scale = 1. ) bij = distrax . ScalarAffine ( shift = jnp . asarray ([ 3. , 3. , 3. ])) transformed_dist = distrax . Transformed ( distribution = dist , bijector = bij ) samples = transformed_dist . sample ( jax . random . PRNGKey ( 0 )) print ( samples ) # [2.7941577, 2.7941577, 2.7941577] This assumes that the forward function of the bijector is traceable; that is, it is a pure function that does not contain run-time branching. Functions that do not strictly meet this requirement can still be used, but we cannot guarantee that the shapes, dtype, and KL computations involving the transformed distribution can be correctly obtained.","title":"Transformed"},{"location":"api/distributions/uniform/","text":"Uniform \u00a4 distreqx.distributions.uniform.Uniform ( AbstractSTDDistribution , AbstractSurvivalDistribution ) \u00a4 Uniform distribution with low and high parameters.","title":"Uniform"},{"location":"api/distributions/uniform/#uniform","text":"","title":"Uniform"},{"location":"api/distributions/uniform/#distreqx.distributions.uniform.Uniform","text":"Uniform distribution with low and high parameters.","title":"Uniform"},{"location":"api/utils/math/","text":"Math \u00a4 distreqx . utils . math . multiply_no_nan : None \u00a4 distreqx . utils . math . power_no_nan : None \u00a4 distreqx . utils . math . mul_exp ( x : Array , logp : Array ) -> Array \u00a4 Returns x * exp(logp) with zero output if exp(logp) == 0 . Arguments: x : An array. logp : An array representing logarithms. Returns: The result of x * exp(logp) . distreqx . utils . math . normalize ( * , probs : Optional [ Array ] = None , logits : Optional [ Array ] = None ) -> Array \u00a4 Normalizes logits via log_softmax or probabilities to ensure they sum to one. Arguments: probs : Probability values. logits : Logit values. Returns: Normalized probabilities or logits. distreqx . utils . math . sum_last ( x : Array , ndims : int ) -> Array \u00a4 Sums the last ndims axes of array x . Arguments: x : An array. ndims : The number of last dimensions to sum. Returns: The sum of the last ndims dimensions of x . distreqx . utils . math . log_expbig_minus_expsmall ( big : Array , small : Array ) -> Array \u00a4 Stable implementation of log(exp(big) - exp(small)) . Arguments: big : First input. small : Second input. It must be small <= big . Returns: The resulting log(exp(big) - exp(small)) . distreqx . utils . math . log_beta ( a : Array , b : Array ) -> Array \u00a4 Obtains the log of the beta function log B(a, b) . Arguments: a : First input. It must be positive. b : Second input. It must be positive. Returns: The value log B(a, b) = log Gamma(a) + log Gamma(b) - log Gamma(a + b) , where Gamma is the Gamma function, obtained through stable computation of log Gamma . distreqx . utils . math . log_beta_multivariate ( a : Array ) -> Array \u00a4 Obtains the log of the multivariate beta function log B(a) . Arguments: a : An array of length K containing positive values. Returns: The value log B(a) = sum_{k=1}^{K} log Gamma(a_k) - log Gamma(sum_{k=1}^{K} a_k) , where Gamma is the Gamma function, obtained through stable computation of log Gamma .","title":"Math"},{"location":"api/utils/math/#math","text":"","title":"Math"},{"location":"api/utils/math/#distreqx.utils.math.multiply_no_nan","text":"","title":"multiply_no_nan"},{"location":"api/utils/math/#distreqx.utils.math.power_no_nan","text":"","title":"power_no_nan"},{"location":"api/utils/math/#distreqx.utils.math.mul_exp","text":"Returns x * exp(logp) with zero output if exp(logp) == 0 . Arguments: x : An array. logp : An array representing logarithms. Returns: The result of x * exp(logp) .","title":"mul_exp()"},{"location":"api/utils/math/#distreqx.utils.math.normalize","text":"Normalizes logits via log_softmax or probabilities to ensure they sum to one. Arguments: probs : Probability values. logits : Logit values. Returns: Normalized probabilities or logits.","title":"normalize()"},{"location":"api/utils/math/#distreqx.utils.math.sum_last","text":"Sums the last ndims axes of array x . Arguments: x : An array. ndims : The number of last dimensions to sum. Returns: The sum of the last ndims dimensions of x .","title":"sum_last()"},{"location":"api/utils/math/#distreqx.utils.math.log_expbig_minus_expsmall","text":"Stable implementation of log(exp(big) - exp(small)) . Arguments: big : First input. small : Second input. It must be small <= big . Returns: The resulting log(exp(big) - exp(small)) .","title":"log_expbig_minus_expsmall()"},{"location":"api/utils/math/#distreqx.utils.math.log_beta","text":"Obtains the log of the beta function log B(a, b) . Arguments: a : First input. It must be positive. b : Second input. It must be positive. Returns: The value log B(a, b) = log Gamma(a) + log Gamma(b) - log Gamma(a + b) , where Gamma is the Gamma function, obtained through stable computation of log Gamma .","title":"log_beta()"},{"location":"api/utils/math/#distreqx.utils.math.log_beta_multivariate","text":"Obtains the log of the multivariate beta function log B(a) . Arguments: a : An array of length K containing positive values. Returns: The value log B(a) = sum_{k=1}^{K} log Gamma(a_k) - log Gamma(sum_{k=1}^{K} a_k) , where Gamma is the Gamma function, obtained through stable computation of log Gamma .","title":"log_beta_multivariate()"},{"location":"examples/01_vae/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Variational Autoencoder \u00a4 In this example we will be implementing a variational autoencoder using distreqx. import equinox as eqx import jax import jax.numpy as jnp import matplotlib.pyplot as plt import optax import tensorflow_datasets as tfds from tqdm.notebook import tqdm from distreqx import distributions First, we need to create a standard small encoder and decoder module. The shapes are hard coded for the MNIST dataset we will be using. class Encoder ( eqx . Module ): encoder : eqx . nn . Linear mean : eqx . nn . Linear std : eqx . nn . Linear def __init__ ( self , key , input_size = 784 , hidden_size = 512 , latent_size = 10 ): keys = jax . random . split ( key , 3 ) self . encoder = eqx . nn . Linear ( input_size , hidden_size , key = keys [ 0 ]) self . mean = eqx . nn . Linear ( hidden_size , latent_size , key = keys [ 1 ]) self . std = eqx . nn . Linear ( hidden_size , latent_size , key = keys [ 2 ]) def __call__ ( self , x ): x = x . flatten () x = self . encoder ( x ) x = jax . nn . relu ( x ) mean = self . mean ( x ) log_stddev = self . std ( x ) stddev = jnp . exp ( log_stddev ) return mean , stddev class Decoder ( eqx . Module ): ln1 : eqx . nn . Linear ln2 : eqx . nn . Linear def __init__ ( self , key , input_size , hidden_size , output_shape = 784 ): keys = jax . random . split ( key , 2 ) self . ln1 = eqx . nn . Linear ( input_size , hidden_size , key = keys [ 0 ]) self . ln2 = eqx . nn . Linear ( hidden_size , output_shape , key = keys [ 1 ]) def __call__ ( self , z ): z = self . ln1 ( z ) z = jax . nn . relu ( z ) logits = self . ln2 ( z ) logits = jnp . reshape ( logits , ( 28 , 28 , 1 )) return logits Next we can construct the VAE object. It consists of an encoder and decoder, the encoder provides the mean and variance of the multivariate Gaussian prior. The output of the decoder represents the logits of a bernoulli distribution over the pixel space. Note that the Independent here is a bit of a legacy artifact. In general, distreqx encourages vmap based approaches to distributions and offloads any batching to the user. However, it is often possible to implicitly batch computations for certain disributions (sometimes even correctly). Independent is merely a helper that sums over dimensions, so even though we don't vmap over the bernoulli (like we often should), we can still sum over batch dimensions (since the event shape of a bernoulli is ()). class VAEOutput ( eqx . Module ): variational_distrib : distributions . AbstractDistribution likelihood_distrib : distributions . AbstractDistribution image : jnp . ndarray class VAE ( eqx . Module ): encoder : Encoder decoder : Decoder def __init__ ( self , key , input_size = 784 , latent_size = 10 , hidden_size = 512 , ): keys = jax . random . split ( key ) self . encoder = Encoder ( keys [ 0 ], input_size , hidden_size , latent_size ) self . decoder = Decoder ( keys [ 1 ], latent_size , hidden_size , input_size ) def __call__ ( self , x , key ): keys = jax . random . split ( key ) x = x . astype ( jnp . float32 ) # q(z|x) = N(mean(x), covariance(x)) mean , stddev = self . encoder ( x ) variational_distrib = distributions . MultivariateNormalDiag ( loc = mean , scale_diag = stddev ) z = variational_distrib . sample ( keys [ 0 ]) # p(x|z) = \\Prod Bernoulli(logits(z)) logits = self . decoder ( z ) likelihood_distrib = distributions . Independent ( distributions . Bernoulli ( logits = logits ) ) # Generate images from the likelihood image = likelihood_distrib . sample ( keys [ 1 ]) return VAEOutput ( variational_distrib , likelihood_distrib , image ) Now we can train our model with the standard ELBO. Keep in mind, here we require some vmap ing over the distribution, since we now have an additional batch dimension (that we do not want to have Independent sum over). def load_dataset ( split , batch_size ): ds = tfds . load ( \"binarized_mnist\" , split = split , shuffle_files = True ) ds = ds . shuffle ( buffer_size = 10 * batch_size ) ds = ds . batch ( batch_size ) ds = ds . prefetch ( buffer_size = 5 ) ds = ds . repeat () return iter ( tfds . as_numpy ( ds )) @eqx . filter_jit def loss_fn ( model , key , batch ): \"\"\"Loss = -ELBO, where ELBO = E_q[log p(x|z)] - KL(q(z|x) || p(z)).\"\"\" outputs = eqx . filter_vmap ( model )( batch , key ) # p(z) = N(0, I) prior_z = distributions . MultivariateNormalDiag ( loc = jnp . zeros ( latent_size ), scale_diag = jnp . ones ( latent_size ) ) # we need to make surve to vmap over the distribution itself! # see also: https://docs.kidger.site/equinox/tricks/#ensembling log_likelihood = eqx . filter_vmap ( lambda x , y : x . log_prob ( y ))( outputs . likelihood_distrib , batch ) kl = outputs . variational_distrib . kl_divergence ( prior_z ) elbo = log_likelihood - kl return - jnp . mean ( elbo ), ( log_likelihood , kl ) @eqx . filter_jit def update ( model , rng_key , opt_state , batch , ): ( val , ( ll , kl )), grads = eqx . filter_value_and_grad ( loss_fn , has_aux = True )( model , rng_key , batch ) updates , new_opt_state = optimizer . update ( grads , opt_state ) model = eqx . apply_updates ( model , updates ) return model , new_opt_state , val batch_size = 128 learning_rate = 0.0005 training_steps = 1000 eval_frequency = 100 latent_size = 2 MNIST_IMAGE_SHAPE = ( 28 , 28 , 1 ) optimizer = optax . adam ( learning_rate ) key = jax . random . PRNGKey ( 0 ) key , subkey = jax . random . split ( key ) model = VAE ( subkey , input_size = 784 , latent_size = latent_size , hidden_size = 512 ) opt_state = optimizer . init ( eqx . filter ( model , eqx . is_array )) train_ds = load_dataset ( tfds . Split . TRAIN , batch_size ) valid_ds = load_dataset ( tfds . Split . TEST , batch_size ) losses = [] for step in tqdm ( range ( training_steps )): key , subkey = jax . random . split ( key ) batch = jnp . array ( next ( valid_ds )[ \"image\" ]) subkey = jax . random . split ( subkey , len ( batch )) # val_loss, (ll, kl) = loss_fn(model, subkey, batch) # break model , opt_state , loss = update ( model , subkey , opt_state , batch ) losses . append ( loss ) if step % eval_frequency == 0 : key , subkey = jax . random . split ( key ) batch = jnp . array ( next ( valid_ds )[ \"image\" ]) subkey = jax . random . split ( subkey , len ( batch )) val_loss , ( ll , kl ) = loss_fn ( model , subkey , batch ) # results = eqx.filter_jit(eqx.filter_vmap(model))(batch[\"image\"], subkey) # plt.imshow(results.image[0]) # plt.show() print ( f \"STEP: { step } ; Validation -ELBO: { val_loss } , LL { ll . mean () } , KL \\ { kl . mean () } \" ) 2024-04-28 18:06:16.840750: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead. 0%| | 0/1000 [00:00<?, ?it/s] STEP: 0; Validation -ELBO: 525.56103515625, LL -525.4020385742188, KL 0.15904900431632996 STEP: 100; Validation -ELBO: 203.7550048828125, LL -197.75645446777344, KL 5.9985527992248535 STEP: 200; Validation -ELBO: 181.37774658203125, LL -174.86788940429688, KL 6.509843349456787 STEP: 300; Validation -ELBO: 188.51040649414062, LL -182.77639770507812, KL 5.734016418457031 STEP: 400; Validation -ELBO: 175.24862670898438, LL -169.42745971679688, KL 5.82118034362793 STEP: 500; Validation -ELBO: 167.22598266601562, LL -161.6207275390625, KL 5.60526180267334 STEP: 600; Validation -ELBO: 173.36790466308594, LL -167.86239624023438, KL 5.505499839782715 STEP: 700; Validation -ELBO: 167.0419464111328, LL -161.61251831054688, KL 5.429437637329102 STEP: 800; Validation -ELBO: 173.220947265625, LL -167.77816772460938, KL 5.442760467529297 STEP: 900; Validation -ELBO: 174.1055450439453, LL -168.64964294433594, KL 5.455894947052002 plt . plot ( losses ) plt . xlabel ( \"Iterations\" ) plt . ylabel ( \"-ELBO\" ) plt . show () For such a small latent space, we can visualize a nice representation of the output. # from: https://keras.io/examples/generative/vae/#display-a-grid-of-sampled-digits import numpy as np def plot_latent_space ( vae , n = 30 , figsize = 15 ): # display a n*n 2D manifold of digits digit_size = 28 scale = 1.0 figure = np . zeros (( digit_size * n , digit_size * n )) # linearly spaced coordinates corresponding to the 2D plot # of digit classes in the latent space grid_x = jnp . linspace ( - scale , scale , n ) grid_y = jnp . linspace ( - scale , scale , n )[:: - 1 ] for i , yi in enumerate ( grid_y ): for j , xi in enumerate ( grid_x ): z_sample = jnp . array ([ xi , yi ]) # convert logits to probs digit = jax . nn . sigmoid ( eqx . filter_jit ( vae . decoder )( z_sample ) . squeeze ()) figure [ i * digit_size : ( i + 1 ) * digit_size , j * digit_size : ( j + 1 ) * digit_size , ] = digit plt . figure ( figsize = ( figsize , figsize )) start_range = digit_size // 2 end_range = n * digit_size + start_range pixel_range = jnp . arange ( start_range , end_range , digit_size ) sample_range_x = jnp . trunc ( 10 * grid_x ) / 10 sample_range_y = jnp . trunc ( 10 * grid_y ) / 10 plt . xticks ( pixel_range , sample_range_x ) plt . yticks ( pixel_range , sample_range_y ) plt . xlabel ( \"z[0]\" ) plt . ylabel ( \"z[1]\" ) plt . imshow ( figure , cmap = \"Greys_r\" ) plt . show () plot_latent_space ( model )","title":"Binary MNIST VAE"},{"location":"examples/01_vae/#variational-autoencoder","text":"In this example we will be implementing a variational autoencoder using distreqx. import equinox as eqx import jax import jax.numpy as jnp import matplotlib.pyplot as plt import optax import tensorflow_datasets as tfds from tqdm.notebook import tqdm from distreqx import distributions First, we need to create a standard small encoder and decoder module. The shapes are hard coded for the MNIST dataset we will be using. class Encoder ( eqx . Module ): encoder : eqx . nn . Linear mean : eqx . nn . Linear std : eqx . nn . Linear def __init__ ( self , key , input_size = 784 , hidden_size = 512 , latent_size = 10 ): keys = jax . random . split ( key , 3 ) self . encoder = eqx . nn . Linear ( input_size , hidden_size , key = keys [ 0 ]) self . mean = eqx . nn . Linear ( hidden_size , latent_size , key = keys [ 1 ]) self . std = eqx . nn . Linear ( hidden_size , latent_size , key = keys [ 2 ]) def __call__ ( self , x ): x = x . flatten () x = self . encoder ( x ) x = jax . nn . relu ( x ) mean = self . mean ( x ) log_stddev = self . std ( x ) stddev = jnp . exp ( log_stddev ) return mean , stddev class Decoder ( eqx . Module ): ln1 : eqx . nn . Linear ln2 : eqx . nn . Linear def __init__ ( self , key , input_size , hidden_size , output_shape = 784 ): keys = jax . random . split ( key , 2 ) self . ln1 = eqx . nn . Linear ( input_size , hidden_size , key = keys [ 0 ]) self . ln2 = eqx . nn . Linear ( hidden_size , output_shape , key = keys [ 1 ]) def __call__ ( self , z ): z = self . ln1 ( z ) z = jax . nn . relu ( z ) logits = self . ln2 ( z ) logits = jnp . reshape ( logits , ( 28 , 28 , 1 )) return logits Next we can construct the VAE object. It consists of an encoder and decoder, the encoder provides the mean and variance of the multivariate Gaussian prior. The output of the decoder represents the logits of a bernoulli distribution over the pixel space. Note that the Independent here is a bit of a legacy artifact. In general, distreqx encourages vmap based approaches to distributions and offloads any batching to the user. However, it is often possible to implicitly batch computations for certain disributions (sometimes even correctly). Independent is merely a helper that sums over dimensions, so even though we don't vmap over the bernoulli (like we often should), we can still sum over batch dimensions (since the event shape of a bernoulli is ()). class VAEOutput ( eqx . Module ): variational_distrib : distributions . AbstractDistribution likelihood_distrib : distributions . AbstractDistribution image : jnp . ndarray class VAE ( eqx . Module ): encoder : Encoder decoder : Decoder def __init__ ( self , key , input_size = 784 , latent_size = 10 , hidden_size = 512 , ): keys = jax . random . split ( key ) self . encoder = Encoder ( keys [ 0 ], input_size , hidden_size , latent_size ) self . decoder = Decoder ( keys [ 1 ], latent_size , hidden_size , input_size ) def __call__ ( self , x , key ): keys = jax . random . split ( key ) x = x . astype ( jnp . float32 ) # q(z|x) = N(mean(x), covariance(x)) mean , stddev = self . encoder ( x ) variational_distrib = distributions . MultivariateNormalDiag ( loc = mean , scale_diag = stddev ) z = variational_distrib . sample ( keys [ 0 ]) # p(x|z) = \\Prod Bernoulli(logits(z)) logits = self . decoder ( z ) likelihood_distrib = distributions . Independent ( distributions . Bernoulli ( logits = logits ) ) # Generate images from the likelihood image = likelihood_distrib . sample ( keys [ 1 ]) return VAEOutput ( variational_distrib , likelihood_distrib , image ) Now we can train our model with the standard ELBO. Keep in mind, here we require some vmap ing over the distribution, since we now have an additional batch dimension (that we do not want to have Independent sum over). def load_dataset ( split , batch_size ): ds = tfds . load ( \"binarized_mnist\" , split = split , shuffle_files = True ) ds = ds . shuffle ( buffer_size = 10 * batch_size ) ds = ds . batch ( batch_size ) ds = ds . prefetch ( buffer_size = 5 ) ds = ds . repeat () return iter ( tfds . as_numpy ( ds )) @eqx . filter_jit def loss_fn ( model , key , batch ): \"\"\"Loss = -ELBO, where ELBO = E_q[log p(x|z)] - KL(q(z|x) || p(z)).\"\"\" outputs = eqx . filter_vmap ( model )( batch , key ) # p(z) = N(0, I) prior_z = distributions . MultivariateNormalDiag ( loc = jnp . zeros ( latent_size ), scale_diag = jnp . ones ( latent_size ) ) # we need to make surve to vmap over the distribution itself! # see also: https://docs.kidger.site/equinox/tricks/#ensembling log_likelihood = eqx . filter_vmap ( lambda x , y : x . log_prob ( y ))( outputs . likelihood_distrib , batch ) kl = outputs . variational_distrib . kl_divergence ( prior_z ) elbo = log_likelihood - kl return - jnp . mean ( elbo ), ( log_likelihood , kl ) @eqx . filter_jit def update ( model , rng_key , opt_state , batch , ): ( val , ( ll , kl )), grads = eqx . filter_value_and_grad ( loss_fn , has_aux = True )( model , rng_key , batch ) updates , new_opt_state = optimizer . update ( grads , opt_state ) model = eqx . apply_updates ( model , updates ) return model , new_opt_state , val batch_size = 128 learning_rate = 0.0005 training_steps = 1000 eval_frequency = 100 latent_size = 2 MNIST_IMAGE_SHAPE = ( 28 , 28 , 1 ) optimizer = optax . adam ( learning_rate ) key = jax . random . PRNGKey ( 0 ) key , subkey = jax . random . split ( key ) model = VAE ( subkey , input_size = 784 , latent_size = latent_size , hidden_size = 512 ) opt_state = optimizer . init ( eqx . filter ( model , eqx . is_array )) train_ds = load_dataset ( tfds . Split . TRAIN , batch_size ) valid_ds = load_dataset ( tfds . Split . TEST , batch_size ) losses = [] for step in tqdm ( range ( training_steps )): key , subkey = jax . random . split ( key ) batch = jnp . array ( next ( valid_ds )[ \"image\" ]) subkey = jax . random . split ( subkey , len ( batch )) # val_loss, (ll, kl) = loss_fn(model, subkey, batch) # break model , opt_state , loss = update ( model , subkey , opt_state , batch ) losses . append ( loss ) if step % eval_frequency == 0 : key , subkey = jax . random . split ( key ) batch = jnp . array ( next ( valid_ds )[ \"image\" ]) subkey = jax . random . split ( subkey , len ( batch )) val_loss , ( ll , kl ) = loss_fn ( model , subkey , batch ) # results = eqx.filter_jit(eqx.filter_vmap(model))(batch[\"image\"], subkey) # plt.imshow(results.image[0]) # plt.show() print ( f \"STEP: { step } ; Validation -ELBO: { val_loss } , LL { ll . mean () } , KL \\ { kl . mean () } \" ) 2024-04-28 18:06:16.840750: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead. 0%| | 0/1000 [00:00<?, ?it/s] STEP: 0; Validation -ELBO: 525.56103515625, LL -525.4020385742188, KL 0.15904900431632996 STEP: 100; Validation -ELBO: 203.7550048828125, LL -197.75645446777344, KL 5.9985527992248535 STEP: 200; Validation -ELBO: 181.37774658203125, LL -174.86788940429688, KL 6.509843349456787 STEP: 300; Validation -ELBO: 188.51040649414062, LL -182.77639770507812, KL 5.734016418457031 STEP: 400; Validation -ELBO: 175.24862670898438, LL -169.42745971679688, KL 5.82118034362793 STEP: 500; Validation -ELBO: 167.22598266601562, LL -161.6207275390625, KL 5.60526180267334 STEP: 600; Validation -ELBO: 173.36790466308594, LL -167.86239624023438, KL 5.505499839782715 STEP: 700; Validation -ELBO: 167.0419464111328, LL -161.61251831054688, KL 5.429437637329102 STEP: 800; Validation -ELBO: 173.220947265625, LL -167.77816772460938, KL 5.442760467529297 STEP: 900; Validation -ELBO: 174.1055450439453, LL -168.64964294433594, KL 5.455894947052002 plt . plot ( losses ) plt . xlabel ( \"Iterations\" ) plt . ylabel ( \"-ELBO\" ) plt . show () For such a small latent space, we can visualize a nice representation of the output. # from: https://keras.io/examples/generative/vae/#display-a-grid-of-sampled-digits import numpy as np def plot_latent_space ( vae , n = 30 , figsize = 15 ): # display a n*n 2D manifold of digits digit_size = 28 scale = 1.0 figure = np . zeros (( digit_size * n , digit_size * n )) # linearly spaced coordinates corresponding to the 2D plot # of digit classes in the latent space grid_x = jnp . linspace ( - scale , scale , n ) grid_y = jnp . linspace ( - scale , scale , n )[:: - 1 ] for i , yi in enumerate ( grid_y ): for j , xi in enumerate ( grid_x ): z_sample = jnp . array ([ xi , yi ]) # convert logits to probs digit = jax . nn . sigmoid ( eqx . filter_jit ( vae . decoder )( z_sample ) . squeeze ()) figure [ i * digit_size : ( i + 1 ) * digit_size , j * digit_size : ( j + 1 ) * digit_size , ] = digit plt . figure ( figsize = ( figsize , figsize )) start_range = digit_size // 2 end_range = n * digit_size + start_range pixel_range = jnp . arange ( start_range , end_range , digit_size ) sample_range_x = jnp . trunc ( 10 * grid_x ) / 10 sample_range_y = jnp . trunc ( 10 * grid_y ) / 10 plt . xticks ( pixel_range , sample_range_x ) plt . yticks ( pixel_range , sample_range_y ) plt . xlabel ( \"z[0]\" ) plt . ylabel ( \"z[1]\" ) plt . imshow ( figure , cmap = \"Greys_r\" ) plt . show () plot_latent_space ( model )","title":"Variational Autoencoder"},{"location":"examples/02_mixture_models/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Mixture Models \u00a4 In this tutorial, we will look at Gaussian and Bernoulli mixture models. These mixture models are defined by mixing distribution (categorical) which is responsible for defining the distribution over the component distributions. We will train these models to generatively model MNIST data sets. We can train them with both expectation maximization (EM) or gradient descent based approaches. import equinox as eqx import jax import matplotlib.pyplot as plt import numpy as np import optax import tensorflow_datasets as tfds from jax import numpy as jnp from jax.scipy.special import expit , logit from tqdm.notebook import tqdm from distreqx.distributions import ( Bernoulli , Categorical , Independent , MixtureSameFamily , Normal , ) N = 2_000 mnist_data = tfds . load ( \"mnist\" )[ \"train\" ] mnist_data = tfds . as_numpy ( mnist_data ) mnist_data = jnp . array ( [ data [ \"image\" ] for count , data in enumerate ( mnist_data ) if count < N ] ) 2025-08-11 12:24:18.473070: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence new_data = mnist_data / 255.0 new_data . shape (2000, 28, 28, 1) class GMM ( eqx . Module ): _model : MixtureSameFamily def __init__ ( self , K , n_vars , rng_key ): mixing_coeffs = jax . random . uniform ( rng_key , ( K ,), minval = 100 , maxval = 200 ) mixing_coeffs = mixing_coeffs / mixing_coeffs . sum () initial_probs = jnp . full (( K , n_vars ), 1.0 / K ) self . _model = MixtureSameFamily ( mixture_distribution = Categorical ( probs = mixing_coeffs ), components_distribution = Independent ( Normal ( initial_probs , 0.2 * jnp . ones_like ( initial_probs )) ), ) @property def mixing_coeffs ( self ): return self . _model . mixture_distribution . probs @property def probs ( self ): return self . _model . components_distribution . distribution . loc @property def model ( self ): return self . _model def responsibilities ( self , observations ): return jnp . nan_to_num ( self . _model . posterior_marginal ( observations ) . probs ) def expected_log_likelihood ( self , observations ): return jnp . nan_to_num ( self . _model . log_prob ( observations )) def em_step ( self , observations ): n_obs , _ = observations . shape res = eqx . filter_vmap ( self . responsibilities )( observations ) sum_res = jnp . sum ( res , axis = 0 ) mus = jax . vmap ( lambda c , d : jax . vmap ( lambda a , b : a * b , in_axes = ( 0 , None ))( c , d ) )( res , observations ) mus = jax . vmap ( lambda a , b : a / b )( jnp . sum ( mus , axis = 0 ), sum_res ) # constant sigma return sum_res / n_obs , mus def plot ( self , n_row , n_col ): if n_row * n_col != len ( self . mixing_coeffs ): raise TypeError ( \"The number of rows and columns does not match with \" \"the number of component distribution.\" ) fig , axes = plt . subplots ( n_row , n_col ) for ( coeff , mean ), ax in zip ( zip ( self . mixing_coeffs , self . probs ), axes . flatten () ): ax . imshow ( mean . reshape (( 28 , 28 )), cmap = \"grey\" ) ax . set_title ( \" %1.2f \" % coeff ) ax . axis ( \"off\" ) fig . tight_layout ( pad = 1.0 ) plt . show () Here we have some manual updates, this is in general not necessary, but can be helpful to have more direct control over the parameters (especially given the nature of modules to be very deep). @eqx . filter_jit def update_model_params ( model , params ): model = eqx . tree_at ( lambda x : x . _model . mixture_distribution . _probs , model , params [ 0 ] ) model = eqx . tree_at ( lambda x : x . _model . components_distribution . distribution . _loc , model , params [ 1 ] ) return model @eqx . filter_jit def train_step ( model , params , observations ): model = update_model_params ( model , params ) log_likelihood = jnp . sum ( eqx . filter_vmap ( model . expected_log_likelihood )( observations ) ) mixing_coeffs , probs = model . em_step ( observations ) return ( mixing_coeffs , probs ), log_likelihood data = new_data . reshape (( N , 784 )) data = (( data > 0.0 ) . astype ( \"int32\" ) + 1e-8 ) * 0.99 key = jax . random . PRNGKey ( 0 ) key , subkey = jax . random . split ( key ) K = 20 model = GMM ( K , 784 , subkey ) params = ( model . mixing_coeffs , model . probs ) batch_size = 2000 inner = N // batch_size outer = 20 losses = [] for epoch in tqdm ( range ( outer )): inner_loss = [] for batch in range ( inner ): key , subkey = jax . random . split ( key ) inds = jax . random . randint ( subkey , minval = 0 , maxval = len ( data ), shape = ( batch_size ,) ) real_batch = data [ inds ] key , subkey = jax . random . split ( key ) params , loss = train_step ( model , params , real_batch ) model = update_model_params ( model , params ) inner_loss . append ( loss ) losses . append ( np . mean ( inner_loss )) if epoch % 10 == 0 : model . plot ( 5 , 4 ) 0%| | 0/20 [00:00<?, ?it/s] Now lets plot the EM loss and the final generated images for the GMM. plt . figure ( figsize = ( 3 , 2 )) plt . plot ( - np . array ( losses )) plt . yscale ( \"log\" ) plt . xlabel ( \"Iteration\" ) plt . ylabel ( \"NLL\" ) plt . title ( \"GMM EM\" ) plt . show () model . plot ( 4 , 5 ) Now we can repeat the process, but with SGD this time instead of EM. Notice the different failure mode? Difficulties in training mixtures models with SGD are well known (and there are many variants of EM to help overcome these failure modes). @eqx . filter_jit def update_model_params ( model , params ): params = ( jax . nn . softmax ( params [ 0 ]), params [ 1 ]) model = eqx . tree_at ( lambda x : x . _model . mixture_distribution . _probs , model , params [ 0 ] ) model = eqx . tree_at ( lambda x : x . _model . components_distribution . distribution . _loc , model , params [ 1 ] ) return model def loss_fn ( model , params , inp ): model = update_model_params ( model , params ) return - model . expected_log_likelihood ( inp ) def vmap_loss ( params , model , batch ): return jnp . mean ( eqx . filter_vmap ( loss_fn , in_axes = ( None , None , 0 ))( model , params , batch ) ) @eqx . filter_jit def step ( model , params , batch , opt_state ): loss , grads = eqx . filter_value_and_grad ( vmap_loss )( params , model , batch ) update , opt_state = optimizer . update ( grads , opt_state , params ) params = eqx . apply_updates ( params , update ) return params , opt_state , loss data = new_data . reshape (( N , 784 )) data = ( data > 0.0 ) . astype ( \"int32\" ) key = jax . random . PRNGKey ( 0 ) key , subkey = jax . random . split ( key ) K = 20 model = GMM ( K , 784 , subkey ) params = ( model . mixing_coeffs , model . probs ) optimizer = optax . adam ( 1e-1 ) opt_state = optimizer . init (( jax . nn . softmax ( model . mixing_coeffs ), model . probs )) batch_size = 1000 inner = N // batch_size outer = 100 losses = [] for epoch in tqdm ( range ( outer )): inner_loss = [] for batch in range ( inner ): key , subkey = jax . random . split ( key ) inds = jax . random . randint ( subkey , minval = 0 , maxval = len ( data ), shape = ( batch_size ,) ) real_batch = data [ inds ] key , subkey = jax . random . split ( key ) params , opt_state , loss = step ( model , params , real_batch , opt_state ) model = update_model_params ( model , params ) inner_loss . append ( loss ) losses . append ( np . mean ( inner_loss )) if epoch % 40 == 0 : model . plot ( 5 , 4 ) 0%| | 0/100 [00:00<?, ?it/s] plt . figure ( figsize = ( 3 , 2 )) plt . plot ( losses ) plt . yscale ( \"log\" ) plt . xlabel ( \"Iteration\" ) plt . ylabel ( \"NLL\" ) plt . title ( \"GMM SGD\" ) plt . show () model . plot ( 4 , 5 ) Bernoulli Mixture Models \u00a4 Now we can repeat the exact process as before, but with the component distribution being a Bernoulli. Once you've completed this tutorial, try it with a different distribution and see how it works! class BMM ( eqx . Module ): _model : MixtureSameFamily def __init__ ( self , K , n_vars , rng_key ): mixing_coeffs = jax . random . uniform ( rng_key , ( K ,), minval = 100 , maxval = 200 ) mixing_coeffs = mixing_coeffs / mixing_coeffs . sum () initial_probs = jnp . full (( K , n_vars ), 1.0 / K ) self . _model = MixtureSameFamily ( mixture_distribution = Categorical ( probs = mixing_coeffs ), components_distribution = Independent ( Bernoulli ( probs = initial_probs )), ) @property def mixing_coeffs ( self ): return self . _model . mixture_distribution . probs @property def probs ( self ): return self . _model . components_distribution . distribution . probs @property def model ( self ): return self . _model def responsibilities ( self , observations ): return jnp . nan_to_num ( self . _model . posterior_marginal ( observations ) . probs ) def expected_log_likelihood ( self , observations ): return jnp . nan_to_num ( self . _model . log_prob ( observations )) def em_step ( self , observations ): n_obs , _ = observations . shape def m_step_per_bernoulli ( responsibility ): norm_const = responsibility . sum () mu = jnp . sum ( responsibility [:, None ] * observations , axis = 0 ) / norm_const return jax . numpy . nan_to_num ( mu ), jax . numpy . nan_to_num ( norm_const ) mus , ns = eqx . filter_vmap ( m_step_per_bernoulli , in_axes = ( 1 ))( eqx . filter_vmap ( self . responsibilities )( observations ) ) return ns / n_obs , mus def plot ( self , n_row , n_col ): if n_row * n_col != len ( self . mixing_coeffs ): raise TypeError ( \"The number of rows and columns does not match with the \" \"number of component distribution.\" ) fig , axes = plt . subplots ( n_row , n_col ) for ( coeff , mean ), ax in zip ( zip ( self . mixing_coeffs , self . probs ), axes . flatten () ): ax . imshow ( mean . reshape (( 28 , 28 )), cmap = \"grey\" ) ax . set_title ( \" %1.2f \" % coeff ) ax . axis ( \"off\" ) fig . tight_layout ( pad = 1.0 ) plt . show () @eqx . filter_jit def update_model_params ( model , params ): model = eqx . tree_at ( lambda x : x . _model . mixture_distribution . _probs , model , params [ 0 ] ) model = eqx . tree_at ( lambda x : x . _model . components_distribution . distribution . _probs , model , params [ 1 ] ) return model @eqx . filter_jit def train_step ( model , params , observations ): model = update_model_params ( model , params ) log_likelihood = jnp . sum ( eqx . filter_vmap ( model . expected_log_likelihood )( observations ) ) mixing_coeffs , probs = model . em_step ( observations ) return ( mixing_coeffs , probs ), log_likelihood data = new_data . reshape (( N , 784 )) data = (( data > 0.0 ) . astype ( \"int32\" ) + 1e-8 ) * 0.99 key = jax . random . PRNGKey ( 0 ) key , subkey = jax . random . split ( key ) K = 20 model = BMM ( K , 784 , subkey ) params = ( model . mixing_coeffs , model . probs ) batch_size = 2000 inner = N // batch_size outer = 20 losses = [] for epoch in tqdm ( range ( outer )): inner_loss = [] for batch in range ( inner ): key , subkey = jax . random . split ( key ) inds = jax . random . randint ( subkey , minval = 0 , maxval = len ( data ), shape = ( batch_size ,) ) real_batch = data [ inds ] key , subkey = jax . random . split ( key ) params , loss = train_step ( model , params , real_batch ) model = update_model_params ( model , params ) inner_loss . append ( loss ) losses . append ( np . mean ( inner_loss )) if epoch % 10 == 0 : model . plot ( 5 , 4 ) 0%| | 0/20 [00:00<?, ?it/s] plt . figure ( figsize = ( 3 , 2 )) plt . plot ( - np . array ( losses )) plt . yscale ( \"log\" ) plt . xlabel ( \"Iteration\" ) plt . ylabel ( \"NLL\" ) plt . title ( \"GMM EM\" ) plt . show () model . plot ( 4 , 5 ) # todo: this is annoying, fix @eqx . filter_jit def update_model_params ( model , params ): params = ( jax . nn . softmax ( params [ 0 ]), expit ( params [ 1 ])) model = eqx . tree_at ( lambda x : x . _model . mixture_distribution . _probs , model , params [ 0 ] ) model = eqx . tree_at ( lambda x : x . _model . components_distribution . distribution . _probs , model , params [ 1 ] ) return model @eqx . filter_jit def step ( model , params , batch , opt_state ): batch = batch . astype ( \"float32\" ) loss , grads = eqx . filter_value_and_grad ( vmap_loss )( params , model , batch ) update , opt_state = optimizer . update ( grads , opt_state , params ) params = eqx . apply_updates ( params , update ) return params , opt_state , loss data = new_data . reshape (( N , 784 )) data = ( data > 0.0 ) . astype ( \"int32\" ) key = jax . random . PRNGKey ( 0 ) key , subkey = jax . random . split ( key ) K = 20 model = BMM ( K , 784 , subkey ) params = ( model . mixing_coeffs , model . probs ) optimizer = optax . adam ( 1e-1 ) opt_state = optimizer . init (( jax . nn . softmax ( model . mixing_coeffs ), logit ( model . probs ))) batch_size = 1000 inner = N // batch_size outer = 100 losses = [] for epoch in tqdm ( range ( outer )): inner_loss = [] for batch in range ( inner ): key , subkey = jax . random . split ( key ) inds = jax . random . randint ( subkey , minval = 0 , maxval = len ( data ), shape = ( batch_size ,) ) real_batch = data [ inds ] key , subkey = jax . random . split ( key ) params , opt_state , loss = step ( model , params , real_batch , opt_state ) model = update_model_params ( model , params ) inner_loss . append ( loss ) losses . append ( np . mean ( inner_loss )) if epoch % 40 == 0 : model . plot ( 5 , 4 ) 0%| | 0/100 [00:00<?, ?it/s] plt . figure ( figsize = ( 3 , 2 )) plt . plot ( losses ) plt . yscale ( \"log\" ) plt . xlabel ( \"Iteration\" ) plt . ylabel ( \"NLL\" ) plt . title ( \"BMM SGD\" ) plt . show () model . plot ( 4 , 5 )","title":"GMM/BMM"},{"location":"examples/02_mixture_models/#mixture-models","text":"In this tutorial, we will look at Gaussian and Bernoulli mixture models. These mixture models are defined by mixing distribution (categorical) which is responsible for defining the distribution over the component distributions. We will train these models to generatively model MNIST data sets. We can train them with both expectation maximization (EM) or gradient descent based approaches. import equinox as eqx import jax import matplotlib.pyplot as plt import numpy as np import optax import tensorflow_datasets as tfds from jax import numpy as jnp from jax.scipy.special import expit , logit from tqdm.notebook import tqdm from distreqx.distributions import ( Bernoulli , Categorical , Independent , MixtureSameFamily , Normal , ) N = 2_000 mnist_data = tfds . load ( \"mnist\" )[ \"train\" ] mnist_data = tfds . as_numpy ( mnist_data ) mnist_data = jnp . array ( [ data [ \"image\" ] for count , data in enumerate ( mnist_data ) if count < N ] ) 2025-08-11 12:24:18.473070: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence new_data = mnist_data / 255.0 new_data . shape (2000, 28, 28, 1) class GMM ( eqx . Module ): _model : MixtureSameFamily def __init__ ( self , K , n_vars , rng_key ): mixing_coeffs = jax . random . uniform ( rng_key , ( K ,), minval = 100 , maxval = 200 ) mixing_coeffs = mixing_coeffs / mixing_coeffs . sum () initial_probs = jnp . full (( K , n_vars ), 1.0 / K ) self . _model = MixtureSameFamily ( mixture_distribution = Categorical ( probs = mixing_coeffs ), components_distribution = Independent ( Normal ( initial_probs , 0.2 * jnp . ones_like ( initial_probs )) ), ) @property def mixing_coeffs ( self ): return self . _model . mixture_distribution . probs @property def probs ( self ): return self . _model . components_distribution . distribution . loc @property def model ( self ): return self . _model def responsibilities ( self , observations ): return jnp . nan_to_num ( self . _model . posterior_marginal ( observations ) . probs ) def expected_log_likelihood ( self , observations ): return jnp . nan_to_num ( self . _model . log_prob ( observations )) def em_step ( self , observations ): n_obs , _ = observations . shape res = eqx . filter_vmap ( self . responsibilities )( observations ) sum_res = jnp . sum ( res , axis = 0 ) mus = jax . vmap ( lambda c , d : jax . vmap ( lambda a , b : a * b , in_axes = ( 0 , None ))( c , d ) )( res , observations ) mus = jax . vmap ( lambda a , b : a / b )( jnp . sum ( mus , axis = 0 ), sum_res ) # constant sigma return sum_res / n_obs , mus def plot ( self , n_row , n_col ): if n_row * n_col != len ( self . mixing_coeffs ): raise TypeError ( \"The number of rows and columns does not match with \" \"the number of component distribution.\" ) fig , axes = plt . subplots ( n_row , n_col ) for ( coeff , mean ), ax in zip ( zip ( self . mixing_coeffs , self . probs ), axes . flatten () ): ax . imshow ( mean . reshape (( 28 , 28 )), cmap = \"grey\" ) ax . set_title ( \" %1.2f \" % coeff ) ax . axis ( \"off\" ) fig . tight_layout ( pad = 1.0 ) plt . show () Here we have some manual updates, this is in general not necessary, but can be helpful to have more direct control over the parameters (especially given the nature of modules to be very deep). @eqx . filter_jit def update_model_params ( model , params ): model = eqx . tree_at ( lambda x : x . _model . mixture_distribution . _probs , model , params [ 0 ] ) model = eqx . tree_at ( lambda x : x . _model . components_distribution . distribution . _loc , model , params [ 1 ] ) return model @eqx . filter_jit def train_step ( model , params , observations ): model = update_model_params ( model , params ) log_likelihood = jnp . sum ( eqx . filter_vmap ( model . expected_log_likelihood )( observations ) ) mixing_coeffs , probs = model . em_step ( observations ) return ( mixing_coeffs , probs ), log_likelihood data = new_data . reshape (( N , 784 )) data = (( data > 0.0 ) . astype ( \"int32\" ) + 1e-8 ) * 0.99 key = jax . random . PRNGKey ( 0 ) key , subkey = jax . random . split ( key ) K = 20 model = GMM ( K , 784 , subkey ) params = ( model . mixing_coeffs , model . probs ) batch_size = 2000 inner = N // batch_size outer = 20 losses = [] for epoch in tqdm ( range ( outer )): inner_loss = [] for batch in range ( inner ): key , subkey = jax . random . split ( key ) inds = jax . random . randint ( subkey , minval = 0 , maxval = len ( data ), shape = ( batch_size ,) ) real_batch = data [ inds ] key , subkey = jax . random . split ( key ) params , loss = train_step ( model , params , real_batch ) model = update_model_params ( model , params ) inner_loss . append ( loss ) losses . append ( np . mean ( inner_loss )) if epoch % 10 == 0 : model . plot ( 5 , 4 ) 0%| | 0/20 [00:00<?, ?it/s] Now lets plot the EM loss and the final generated images for the GMM. plt . figure ( figsize = ( 3 , 2 )) plt . plot ( - np . array ( losses )) plt . yscale ( \"log\" ) plt . xlabel ( \"Iteration\" ) plt . ylabel ( \"NLL\" ) plt . title ( \"GMM EM\" ) plt . show () model . plot ( 4 , 5 ) Now we can repeat the process, but with SGD this time instead of EM. Notice the different failure mode? Difficulties in training mixtures models with SGD are well known (and there are many variants of EM to help overcome these failure modes). @eqx . filter_jit def update_model_params ( model , params ): params = ( jax . nn . softmax ( params [ 0 ]), params [ 1 ]) model = eqx . tree_at ( lambda x : x . _model . mixture_distribution . _probs , model , params [ 0 ] ) model = eqx . tree_at ( lambda x : x . _model . components_distribution . distribution . _loc , model , params [ 1 ] ) return model def loss_fn ( model , params , inp ): model = update_model_params ( model , params ) return - model . expected_log_likelihood ( inp ) def vmap_loss ( params , model , batch ): return jnp . mean ( eqx . filter_vmap ( loss_fn , in_axes = ( None , None , 0 ))( model , params , batch ) ) @eqx . filter_jit def step ( model , params , batch , opt_state ): loss , grads = eqx . filter_value_and_grad ( vmap_loss )( params , model , batch ) update , opt_state = optimizer . update ( grads , opt_state , params ) params = eqx . apply_updates ( params , update ) return params , opt_state , loss data = new_data . reshape (( N , 784 )) data = ( data > 0.0 ) . astype ( \"int32\" ) key = jax . random . PRNGKey ( 0 ) key , subkey = jax . random . split ( key ) K = 20 model = GMM ( K , 784 , subkey ) params = ( model . mixing_coeffs , model . probs ) optimizer = optax . adam ( 1e-1 ) opt_state = optimizer . init (( jax . nn . softmax ( model . mixing_coeffs ), model . probs )) batch_size = 1000 inner = N // batch_size outer = 100 losses = [] for epoch in tqdm ( range ( outer )): inner_loss = [] for batch in range ( inner ): key , subkey = jax . random . split ( key ) inds = jax . random . randint ( subkey , minval = 0 , maxval = len ( data ), shape = ( batch_size ,) ) real_batch = data [ inds ] key , subkey = jax . random . split ( key ) params , opt_state , loss = step ( model , params , real_batch , opt_state ) model = update_model_params ( model , params ) inner_loss . append ( loss ) losses . append ( np . mean ( inner_loss )) if epoch % 40 == 0 : model . plot ( 5 , 4 ) 0%| | 0/100 [00:00<?, ?it/s] plt . figure ( figsize = ( 3 , 2 )) plt . plot ( losses ) plt . yscale ( \"log\" ) plt . xlabel ( \"Iteration\" ) plt . ylabel ( \"NLL\" ) plt . title ( \"GMM SGD\" ) plt . show () model . plot ( 4 , 5 )","title":"Mixture Models"},{"location":"examples/02_mixture_models/#bernoulli-mixture-models","text":"Now we can repeat the exact process as before, but with the component distribution being a Bernoulli. Once you've completed this tutorial, try it with a different distribution and see how it works! class BMM ( eqx . Module ): _model : MixtureSameFamily def __init__ ( self , K , n_vars , rng_key ): mixing_coeffs = jax . random . uniform ( rng_key , ( K ,), minval = 100 , maxval = 200 ) mixing_coeffs = mixing_coeffs / mixing_coeffs . sum () initial_probs = jnp . full (( K , n_vars ), 1.0 / K ) self . _model = MixtureSameFamily ( mixture_distribution = Categorical ( probs = mixing_coeffs ), components_distribution = Independent ( Bernoulli ( probs = initial_probs )), ) @property def mixing_coeffs ( self ): return self . _model . mixture_distribution . probs @property def probs ( self ): return self . _model . components_distribution . distribution . probs @property def model ( self ): return self . _model def responsibilities ( self , observations ): return jnp . nan_to_num ( self . _model . posterior_marginal ( observations ) . probs ) def expected_log_likelihood ( self , observations ): return jnp . nan_to_num ( self . _model . log_prob ( observations )) def em_step ( self , observations ): n_obs , _ = observations . shape def m_step_per_bernoulli ( responsibility ): norm_const = responsibility . sum () mu = jnp . sum ( responsibility [:, None ] * observations , axis = 0 ) / norm_const return jax . numpy . nan_to_num ( mu ), jax . numpy . nan_to_num ( norm_const ) mus , ns = eqx . filter_vmap ( m_step_per_bernoulli , in_axes = ( 1 ))( eqx . filter_vmap ( self . responsibilities )( observations ) ) return ns / n_obs , mus def plot ( self , n_row , n_col ): if n_row * n_col != len ( self . mixing_coeffs ): raise TypeError ( \"The number of rows and columns does not match with the \" \"number of component distribution.\" ) fig , axes = plt . subplots ( n_row , n_col ) for ( coeff , mean ), ax in zip ( zip ( self . mixing_coeffs , self . probs ), axes . flatten () ): ax . imshow ( mean . reshape (( 28 , 28 )), cmap = \"grey\" ) ax . set_title ( \" %1.2f \" % coeff ) ax . axis ( \"off\" ) fig . tight_layout ( pad = 1.0 ) plt . show () @eqx . filter_jit def update_model_params ( model , params ): model = eqx . tree_at ( lambda x : x . _model . mixture_distribution . _probs , model , params [ 0 ] ) model = eqx . tree_at ( lambda x : x . _model . components_distribution . distribution . _probs , model , params [ 1 ] ) return model @eqx . filter_jit def train_step ( model , params , observations ): model = update_model_params ( model , params ) log_likelihood = jnp . sum ( eqx . filter_vmap ( model . expected_log_likelihood )( observations ) ) mixing_coeffs , probs = model . em_step ( observations ) return ( mixing_coeffs , probs ), log_likelihood data = new_data . reshape (( N , 784 )) data = (( data > 0.0 ) . astype ( \"int32\" ) + 1e-8 ) * 0.99 key = jax . random . PRNGKey ( 0 ) key , subkey = jax . random . split ( key ) K = 20 model = BMM ( K , 784 , subkey ) params = ( model . mixing_coeffs , model . probs ) batch_size = 2000 inner = N // batch_size outer = 20 losses = [] for epoch in tqdm ( range ( outer )): inner_loss = [] for batch in range ( inner ): key , subkey = jax . random . split ( key ) inds = jax . random . randint ( subkey , minval = 0 , maxval = len ( data ), shape = ( batch_size ,) ) real_batch = data [ inds ] key , subkey = jax . random . split ( key ) params , loss = train_step ( model , params , real_batch ) model = update_model_params ( model , params ) inner_loss . append ( loss ) losses . append ( np . mean ( inner_loss )) if epoch % 10 == 0 : model . plot ( 5 , 4 ) 0%| | 0/20 [00:00<?, ?it/s] plt . figure ( figsize = ( 3 , 2 )) plt . plot ( - np . array ( losses )) plt . yscale ( \"log\" ) plt . xlabel ( \"Iteration\" ) plt . ylabel ( \"NLL\" ) plt . title ( \"GMM EM\" ) plt . show () model . plot ( 4 , 5 ) # todo: this is annoying, fix @eqx . filter_jit def update_model_params ( model , params ): params = ( jax . nn . softmax ( params [ 0 ]), expit ( params [ 1 ])) model = eqx . tree_at ( lambda x : x . _model . mixture_distribution . _probs , model , params [ 0 ] ) model = eqx . tree_at ( lambda x : x . _model . components_distribution . distribution . _probs , model , params [ 1 ] ) return model @eqx . filter_jit def step ( model , params , batch , opt_state ): batch = batch . astype ( \"float32\" ) loss , grads = eqx . filter_value_and_grad ( vmap_loss )( params , model , batch ) update , opt_state = optimizer . update ( grads , opt_state , params ) params = eqx . apply_updates ( params , update ) return params , opt_state , loss data = new_data . reshape (( N , 784 )) data = ( data > 0.0 ) . astype ( \"int32\" ) key = jax . random . PRNGKey ( 0 ) key , subkey = jax . random . split ( key ) K = 20 model = BMM ( K , 784 , subkey ) params = ( model . mixing_coeffs , model . probs ) optimizer = optax . adam ( 1e-1 ) opt_state = optimizer . init (( jax . nn . softmax ( model . mixing_coeffs ), logit ( model . probs ))) batch_size = 1000 inner = N // batch_size outer = 100 losses = [] for epoch in tqdm ( range ( outer )): inner_loss = [] for batch in range ( inner ): key , subkey = jax . random . split ( key ) inds = jax . random . randint ( subkey , minval = 0 , maxval = len ( data ), shape = ( batch_size ,) ) real_batch = data [ inds ] key , subkey = jax . random . split ( key ) params , opt_state , loss = step ( model , params , real_batch , opt_state ) model = update_model_params ( model , params ) inner_loss . append ( loss ) losses . append ( np . mean ( inner_loss )) if epoch % 40 == 0 : model . plot ( 5 , 4 ) 0%| | 0/100 [00:00<?, ?it/s] plt . figure ( figsize = ( 3 , 2 )) plt . plot ( losses ) plt . yscale ( \"log\" ) plt . xlabel ( \"Iteration\" ) plt . ylabel ( \"NLL\" ) plt . title ( \"BMM SGD\" ) plt . show () model . plot ( 4 , 5 )","title":"Bernoulli Mixture Models"},{"location":"examples/03_normalizing_flow/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Normalizing Flows \u00a4 In this tutorial, adapted from: https://gebob19.github.io/normalizing-flows/, we will implement a simple RealNVP normalizing flow. Normalizing flows are a class of generative models which are advantageous due to their explicit representation of densities and likelihoods, but come at a cost of requiring computable jacobian determinants and invertible layers. For an introduction to normalizing flows, see https://arxiv.org/abs/1912.02762. import equinox as eqx import jax import jax.numpy as jnp import matplotlib.pyplot as plt import optax from sklearn import datasets from sklearn.preprocessing import StandardScaler from distreqx import bijectors , distributions Let's define our simple dataset. n_samples = 20000 noisy_moons = datasets . make_moons ( n_samples = n_samples , noise = 0.05 ) X , y = noisy_moons X = StandardScaler () . fit_transform ( X ) xlim , ylim = [ - 3 , 3 ], [ - 3 , 3 ] plt . scatter ( X [:, 0 ], X [:, 1 ], s = 10 , color = \"red\" ) plt . xlim ( xlim ) plt . ylim ( ylim ) (-3.0, 3.0) Now we can program our custom bijector. class RNVP ( bijectors . AbstractFwdLogDetJacBijector , bijectors . AbstractInvLogDetJacBijector ): _is_constant_jacobian : bool _is_constant_log_det : bool sig_net : eqx . Module mu_net : eqx . Module d : int k : int flip : bool def __init__ ( self , d , k , flip , key , hidden = 32 ): self . _is_constant_jacobian = False self . _is_constant_log_det = False self . flip = flip keys = jax . random . split ( key , 4 ) self . d = d self . k = k self . sig_net = eqx . nn . Sequential ( [ eqx . nn . Linear ( k , hidden , key = keys [ 0 ]), eqx . nn . Lambda ( jax . nn . swish ), eqx . nn . Linear ( hidden , d - k , key = keys [ 1 ]), ] ) self . mu_net = eqx . nn . Sequential ( [ eqx . nn . Linear ( k , hidden , key = keys [ 2 ]), eqx . nn . Lambda ( jax . nn . swish ), eqx . nn . Linear ( hidden , d - k , key = keys [ 3 ]), ] ) def forward_and_log_det ( self , x ): x1 , x2 = x [: self . k ], x [ self . k :] if self . flip : x1 , x2 = x2 , x1 sig = self . sig_net ( x1 ) z1 , z2 = x1 , x2 * jnp . exp ( sig ) + self . mu_net ( x1 ) if self . flip : z1 , z2 = z2 , z1 z_hat = jnp . concatenate ([ z1 , z2 ]) log_det = jnp . sum ( sig ) return z_hat , log_det def inverse ( self , y ): z1 , z2 = y [: self . k ], y [ self . k :] if self . flip : z1 , z2 = z2 , z1 x1 = z1 x2 = ( z2 - self . mu_net ( z1 )) * jnp . exp ( - self . sig_net ( z1 )) if self . flip : x1 , x2 = x2 , x1 x_hat = jnp . concatenate ([ x1 , x2 ]) return x_hat def forward ( self , x ): y , _ = self . forward_and_log_det ( x ) return y def inverse_and_log_det ( self , y ): raise NotImplementedError ( f \"Bijector { self . name } does not implement `inverse_and_log_det`.\" ) def same_as ( self , other ) -> bool : return type ( other ) is RNVP Since we want to stack these together, we can use a chain bijector to accomplish this. n = 3 key = jax . random . key ( 0 ) keys = jax . random . split ( key , n ) bijector_chain = bijectors . Chain ([ RNVP ( 2 , 1 , i % 2 , keys [ i ], 600 ) for i in range ( n )]) Flows map p(x) -> p(z) via a function F (samples are generated via F^-1(z)). In general, p(z) is chosen to have some tractable form for sampling and calculating log probabilities. A common choice is Gaussian, which we go with here. base_distribution = distributions . MultivariateNormalDiag ( jnp . zeros ( 2 )) base_distribution_sample = eqx . filter_vmap ( base_distribution . sample ) base_distribution_log_prob = eqx . filter_vmap ( base_distribution . log_prob ) Here we plot the initial, untrained, samples. num_samples = 2000 base_samples = base_distribution_sample ( jax . random . split ( key , num_samples )) transformed_samples = eqx . filter_vmap ( bijector_chain . inverse )( base_samples ) plt . scatter ( transformed_samples [:, 0 ], transformed_samples [:, 1 ], s = 10 , color = \"blue\" , label = \"Untrained F^-1\" , ) plt . scatter ( base_samples [:, 0 ], base_samples [:, 1 ], s = 10 , color = \"red\" , label = \"Base\" ) plt . legend () plt . xlim ( xlim ) plt . ylim ( ylim ) plt . title ( \"Initial Samples\" ) plt . show () learning_rate = 1e-3 num_iters = 1000 batch_size = 128 optimizer = optax . adam ( learning_rate ) opt_state = optimizer . init ( eqx . filter ( bijector_chain , eqx . is_inexact_array )) def log_prob ( params : bijectors . AbstractBijector , data ): f_inv , log_det = params . forward_and_log_det ( data ) log_prob = base_distribution . log_prob ( f_inv ) + log_det return log_prob def loss ( params , batch ): return - jnp . mean ( eqx . filter_vmap ( log_prob , in_axes = ( None , 0 ))( params , batch )) @eqx . filter_jit def update ( model , batch , opt_state , optimizer ): val , grads = eqx . filter_value_and_grad ( loss )( model , batch ) update , opt_state = optimizer . update ( grads , opt_state ) model = eqx . apply_updates ( model , update ) return model , opt_state , val losses = [] for i in range ( num_iters ): if i % 500 == 0 : print ( i ) batch_indices = jax . random . choice ( key , jnp . arange ( X . shape [ 0 ]), ( batch_size ,), replace = False ) batch = X [ batch_indices ] bijector_chain , opt_state , loss_val = update ( bijector_chain , batch , opt_state , optimizer ) losses . append ( loss_val ) 0 500 plt . plot ( losses ) plt . xlabel ( \"Iterations\" ) plt . ylabel ( \"Loss\" ) plt . title ( \"Training Loss Over Time\" ) plt . show () After training we can plot both F(x) (to see where the true data ends up in our sampled space) and F^-1(z) to generate new samples. trained_params = bijector_chain transformed_samples_trained = eqx . filter_vmap ( bijector_chain . inverse )( base_samples ) plt . scatter ( transformed_samples [:, 0 ], transformed_samples [:, 1 ], s = 10 , label = \"Initial F^-1\" ) plt . scatter ( X [:, 0 ], X [:, 1 ], s = 5 , label = \"Data\" ) plt . scatter ( transformed_samples_trained [:, 0 ], transformed_samples_trained [:, 1 ], s = 10 , label = \"Trained F^-1\" , ) plt . xlim ( xlim ) plt . ylim ( ylim ) plt . legend () plt . show () data_to_noise = eqx . filter_vmap ( bijector_chain . forward )( X ) plt . scatter ( data_to_noise [:, 0 ], data_to_noise [:, 1 ], s = 10 , label = \"Z = F(X)\" ) plt . xlim ( xlim ) plt . ylim ( ylim ) plt . legend () plt . show ()","title":"Normalizing Flows"},{"location":"examples/03_normalizing_flow/#normalizing-flows","text":"In this tutorial, adapted from: https://gebob19.github.io/normalizing-flows/, we will implement a simple RealNVP normalizing flow. Normalizing flows are a class of generative models which are advantageous due to their explicit representation of densities and likelihoods, but come at a cost of requiring computable jacobian determinants and invertible layers. For an introduction to normalizing flows, see https://arxiv.org/abs/1912.02762. import equinox as eqx import jax import jax.numpy as jnp import matplotlib.pyplot as plt import optax from sklearn import datasets from sklearn.preprocessing import StandardScaler from distreqx import bijectors , distributions Let's define our simple dataset. n_samples = 20000 noisy_moons = datasets . make_moons ( n_samples = n_samples , noise = 0.05 ) X , y = noisy_moons X = StandardScaler () . fit_transform ( X ) xlim , ylim = [ - 3 , 3 ], [ - 3 , 3 ] plt . scatter ( X [:, 0 ], X [:, 1 ], s = 10 , color = \"red\" ) plt . xlim ( xlim ) plt . ylim ( ylim ) (-3.0, 3.0) Now we can program our custom bijector. class RNVP ( bijectors . AbstractFwdLogDetJacBijector , bijectors . AbstractInvLogDetJacBijector ): _is_constant_jacobian : bool _is_constant_log_det : bool sig_net : eqx . Module mu_net : eqx . Module d : int k : int flip : bool def __init__ ( self , d , k , flip , key , hidden = 32 ): self . _is_constant_jacobian = False self . _is_constant_log_det = False self . flip = flip keys = jax . random . split ( key , 4 ) self . d = d self . k = k self . sig_net = eqx . nn . Sequential ( [ eqx . nn . Linear ( k , hidden , key = keys [ 0 ]), eqx . nn . Lambda ( jax . nn . swish ), eqx . nn . Linear ( hidden , d - k , key = keys [ 1 ]), ] ) self . mu_net = eqx . nn . Sequential ( [ eqx . nn . Linear ( k , hidden , key = keys [ 2 ]), eqx . nn . Lambda ( jax . nn . swish ), eqx . nn . Linear ( hidden , d - k , key = keys [ 3 ]), ] ) def forward_and_log_det ( self , x ): x1 , x2 = x [: self . k ], x [ self . k :] if self . flip : x1 , x2 = x2 , x1 sig = self . sig_net ( x1 ) z1 , z2 = x1 , x2 * jnp . exp ( sig ) + self . mu_net ( x1 ) if self . flip : z1 , z2 = z2 , z1 z_hat = jnp . concatenate ([ z1 , z2 ]) log_det = jnp . sum ( sig ) return z_hat , log_det def inverse ( self , y ): z1 , z2 = y [: self . k ], y [ self . k :] if self . flip : z1 , z2 = z2 , z1 x1 = z1 x2 = ( z2 - self . mu_net ( z1 )) * jnp . exp ( - self . sig_net ( z1 )) if self . flip : x1 , x2 = x2 , x1 x_hat = jnp . concatenate ([ x1 , x2 ]) return x_hat def forward ( self , x ): y , _ = self . forward_and_log_det ( x ) return y def inverse_and_log_det ( self , y ): raise NotImplementedError ( f \"Bijector { self . name } does not implement `inverse_and_log_det`.\" ) def same_as ( self , other ) -> bool : return type ( other ) is RNVP Since we want to stack these together, we can use a chain bijector to accomplish this. n = 3 key = jax . random . key ( 0 ) keys = jax . random . split ( key , n ) bijector_chain = bijectors . Chain ([ RNVP ( 2 , 1 , i % 2 , keys [ i ], 600 ) for i in range ( n )]) Flows map p(x) -> p(z) via a function F (samples are generated via F^-1(z)). In general, p(z) is chosen to have some tractable form for sampling and calculating log probabilities. A common choice is Gaussian, which we go with here. base_distribution = distributions . MultivariateNormalDiag ( jnp . zeros ( 2 )) base_distribution_sample = eqx . filter_vmap ( base_distribution . sample ) base_distribution_log_prob = eqx . filter_vmap ( base_distribution . log_prob ) Here we plot the initial, untrained, samples. num_samples = 2000 base_samples = base_distribution_sample ( jax . random . split ( key , num_samples )) transformed_samples = eqx . filter_vmap ( bijector_chain . inverse )( base_samples ) plt . scatter ( transformed_samples [:, 0 ], transformed_samples [:, 1 ], s = 10 , color = \"blue\" , label = \"Untrained F^-1\" , ) plt . scatter ( base_samples [:, 0 ], base_samples [:, 1 ], s = 10 , color = \"red\" , label = \"Base\" ) plt . legend () plt . xlim ( xlim ) plt . ylim ( ylim ) plt . title ( \"Initial Samples\" ) plt . show () learning_rate = 1e-3 num_iters = 1000 batch_size = 128 optimizer = optax . adam ( learning_rate ) opt_state = optimizer . init ( eqx . filter ( bijector_chain , eqx . is_inexact_array )) def log_prob ( params : bijectors . AbstractBijector , data ): f_inv , log_det = params . forward_and_log_det ( data ) log_prob = base_distribution . log_prob ( f_inv ) + log_det return log_prob def loss ( params , batch ): return - jnp . mean ( eqx . filter_vmap ( log_prob , in_axes = ( None , 0 ))( params , batch )) @eqx . filter_jit def update ( model , batch , opt_state , optimizer ): val , grads = eqx . filter_value_and_grad ( loss )( model , batch ) update , opt_state = optimizer . update ( grads , opt_state ) model = eqx . apply_updates ( model , update ) return model , opt_state , val losses = [] for i in range ( num_iters ): if i % 500 == 0 : print ( i ) batch_indices = jax . random . choice ( key , jnp . arange ( X . shape [ 0 ]), ( batch_size ,), replace = False ) batch = X [ batch_indices ] bijector_chain , opt_state , loss_val = update ( bijector_chain , batch , opt_state , optimizer ) losses . append ( loss_val ) 0 500 plt . plot ( losses ) plt . xlabel ( \"Iterations\" ) plt . ylabel ( \"Loss\" ) plt . title ( \"Training Loss Over Time\" ) plt . show () After training we can plot both F(x) (to see where the true data ends up in our sampled space) and F^-1(z) to generate new samples. trained_params = bijector_chain transformed_samples_trained = eqx . filter_vmap ( bijector_chain . inverse )( base_samples ) plt . scatter ( transformed_samples [:, 0 ], transformed_samples [:, 1 ], s = 10 , label = \"Initial F^-1\" ) plt . scatter ( X [:, 0 ], X [:, 1 ], s = 5 , label = \"Data\" ) plt . scatter ( transformed_samples_trained [:, 0 ], transformed_samples_trained [:, 1 ], s = 10 , label = \"Trained F^-1\" , ) plt . xlim ( xlim ) plt . ylim ( ylim ) plt . legend () plt . show () data_to_noise = eqx . filter_vmap ( bijector_chain . forward )( X ) plt . scatter ( data_to_noise [:, 0 ], data_to_noise [:, 1 ], s = 10 , label = \"Z = F(X)\" ) plt . xlim ( xlim ) plt . ylim ( ylim ) plt . legend () plt . show ()","title":"Normalizing Flows"},{"location":"misc/faq/","text":"FAQ \u00a4 Why not just use distrax? \u00a4 The simple answer to that question is \"I tried\". Distrax is a the product of a lot of great work, especially helpful for working with TFP, but in the current era of jax packages lacks important elements: It's only semi-maintained (there have been no responses to any issues in the last >6 months) It doesn't always play nice with other jax packages and can be slow (see: #193 , #383 , #252 , #269 , #16 , #16170 ) You need Tensorflow to use it Why use equinox? \u00a4 The Jittable class is basically an equinox module (if you squint) and while we could reimplement a custom Module class (like GPJax does), why reinvent the wheel? Equinox is actively being developed and should it become inactive is still possible to maintain. What about flowjax? \u00a4 When I started this project, I was unaware of flowjax . Although flowjax does provide a lot of advanced tooling for NFs and bijections, there are notable differences. distreqx is less specialized and provides a broader baseline set of tools (e.g. distributions). flowjax has more advanced NF tools. distreqx also adheres to an abstract/final design pattern from the development side. flowjax also approaches the concept of \"transformed\" distributions in a different manner.","title":"FAQ"},{"location":"misc/faq/#faq","text":"","title":"FAQ"},{"location":"misc/faq/#why-not-just-use-distrax","text":"The simple answer to that question is \"I tried\". Distrax is a the product of a lot of great work, especially helpful for working with TFP, but in the current era of jax packages lacks important elements: It's only semi-maintained (there have been no responses to any issues in the last >6 months) It doesn't always play nice with other jax packages and can be slow (see: #193 , #383 , #252 , #269 , #16 , #16170 ) You need Tensorflow to use it","title":"Why not just use distrax?"},{"location":"misc/faq/#why-use-equinox","text":"The Jittable class is basically an equinox module (if you squint) and while we could reimplement a custom Module class (like GPJax does), why reinvent the wheel? Equinox is actively being developed and should it become inactive is still possible to maintain.","title":"Why use equinox?"},{"location":"misc/faq/#what-about-flowjax","text":"When I started this project, I was unaware of flowjax . Although flowjax does provide a lot of advanced tooling for NFs and bijections, there are notable differences. distreqx is less specialized and provides a broader baseline set of tools (e.g. distributions). flowjax has more advanced NF tools. distreqx also adheres to an abstract/final design pattern from the development side. flowjax also approaches the concept of \"transformed\" distributions in a different manner.","title":"What about flowjax?"}]}