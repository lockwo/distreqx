{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"distreqx","text":"<p>distreqx (pronounced \"dist-rex\") is a JAX-based library providing implementations of distributions, bijectors, and tools for statistical and probabilistic machine learning with all benefits of jax (native GPU/TPU acceleration, differentiability, vectorization, distributing workloads, XLA compilation, etc.).</p> <p>Tip</p> <p>New to distreqx? Start with the Examples to see distributions and bijectors in action, then explore the API reference for details.</p> <p>The origin of this package is a reimplementation of distrax, (which is a subset of TensorFlow Probability (TFP), with some new features and emphasis on jax compatibility) using equinox. As a result, much of the original code/comments/documentation/tests are directly taken or adapted from distrax (original distrax copyright available at end of README.)</p> <p>Current features include:</p> <ul> <li>Probability distributions</li> <li>Bijectors</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install distreqx\n</code></pre> <p>or</p> <pre><code>git clone https://github.com/lockwo/distreqx.git\ncd distreqx\npip install -e .\n</code></pre> <p>Requires Python 3.9+, JAX 0.4.11+, and Equinox 0.11.0+.</p>"},{"location":"#quick-example","title":"Quick example","text":"<pre><code>import jax\nfrom jax import numpy as jnp\nfrom distreqx import distributions\n\nkey = jax.random.key(1234)\nmu = jnp.array([-1., 0., 1.])\nsigma = jnp.array([0.1, 0.2, 0.3])\n\ndist = distributions.MultivariateNormalDiag(mu, sigma)\n\nsamples = dist.sample(key)\n\nprint(dist.log_prob(samples))\n</code></pre>"},{"location":"#differences-with-distrax","title":"Differences with Distrax","text":"<ul> <li>No official support/interoperability with TFP</li> <li>The concept of a batch dimension is dropped. If you want to operate on a batch, use <code>vmap</code> (note, this can be used in construction as well, e.g. vmaping the construction of a <code>ScalarAffine</code>)</li> <li>Broader pytree enablement </li> <li>Strict abstract/final design pattern</li> </ul>"},{"location":"api/bijectors/_bijector/","title":"Abstract Bijectors","text":""},{"location":"api/bijectors/_bijector/#distreqx.bijectors.AbstractBijector","title":"<code>distreqx.bijectors.AbstractBijector</code> <code></code>","text":"<p>Differentiable bijection that knows to compute its Jacobian determinant.</p> <p>A bijector implements a differentiable and bijective transformation <code>f</code>, whose inverse is also differentiable (<code>f</code> is called a \"diffeomorphism\"). A bijector can be used to transform a continuous random variable <code>X</code> to a continuous random variable <code>Y = f(X)</code> in the context of <code>TransformedDistribution</code>.</p> <p>Typically, a bijector subclass will implement the following methods:</p> <ul> <li><code>forward_and_log_det(x)</code> (required)</li> <li><code>inverse_and_log_det(y)</code> (optional)</li> </ul> <p>The remaining methods are defined in terms of the above by default.</p> <p>Subclass requirements:</p> <ul> <li>Subclasses must ensure that <code>f</code> is differentiable and bijective, and that   their methods correctly implement <code>f^{-1}</code>, <code>J(f)</code> and <code>J(f^{-1})</code>. Distreqx   will assume these properties hold, and will make no attempt to verify them.</li> </ul>"},{"location":"api/bijectors/_bijector/#distreqx.bijectors.AbstractBijector.__init__","title":"<code>__init__()</code>","text":"<p>Initialize self.  See help(type(self)) for accurate signature.</p>"},{"location":"api/bijectors/_bijector/#distreqx.bijectors.AbstractBijector.forward","title":"<code>forward(x: PyTree) -&gt; PyTree</code>","text":"<p>Computes \\(y = f(x)\\).</p>"},{"location":"api/bijectors/_bijector/#distreqx.bijectors.AbstractBijector.inverse","title":"<code>inverse(y: PyTree) -&gt; PyTree</code>","text":"<p>Computes \\(x = f^{-1}(y)\\).</p>"},{"location":"api/bijectors/_bijector/#distreqx.bijectors.AbstractBijector.forward_log_det_jacobian","title":"<code>forward_log_det_jacobian(x: PyTree) -&gt; PyTree</code>","text":"<p>Computes \\(\\log|\\det J(f)(x)|\\).</p>"},{"location":"api/bijectors/_bijector/#distreqx.bijectors.AbstractBijector.inverse_log_det_jacobian","title":"<code>inverse_log_det_jacobian(y: PyTree) -&gt; PyTree</code>","text":"<p>Computes \\(\\log|\\det J(f^{-1})(y)|\\).</p>"},{"location":"api/bijectors/_bijector/#distreqx.bijectors.AbstractBijector.forward_and_log_det","title":"<code>forward_and_log_det(x: PyTree) -&gt; tuple</code>","text":"<p>Computes \\(y = f(x)\\) and \\(\\log|\\det J(f)(x)|\\).</p>"},{"location":"api/bijectors/_bijector/#distreqx.bijectors.AbstractBijector.inverse_and_log_det","title":"<code>inverse_and_log_det(y: Array) -&gt; tuple</code>","text":"<p>Computes \\(x = f^{-1}(y)\\) and \\(\\log|\\det J(f^{-1})(y)|\\).</p>"},{"location":"api/bijectors/_bijector/#distreqx.bijectors.AbstractBijector.same_as","title":"<code>same_as(other) -&gt; bool</code>","text":"<p>Returns True if this bijector is guaranteed to be the same as <code>other</code>.</p>"},{"location":"api/bijectors/_bijector/#distreqx.bijectors.AbstractInvLogDetJacBijector","title":"<code>distreqx.bijectors.AbstractInvLogDetJacBijector(distreqx.bijectors.AbstractBijector)</code> <code></code>","text":"<p>AbstractBijector + concrete <code>inverse_log_det_jacobian</code>.</p>"},{"location":"api/bijectors/_bijector/#distreqx.bijectors.AbstractInvLogDetJacBijector.inverse_log_det_jacobian","title":"<code>inverse_log_det_jacobian(y: PyTree) -&gt; PyTree</code>","text":"<p>Computes \\(\\log|\\det J(f^{-1})(y)|\\).</p>"},{"location":"api/bijectors/_bijector/#distreqx.bijectors.AbstractFwdLogDetJacBijector","title":"<code>distreqx.bijectors.AbstractFwdLogDetJacBijector(distreqx.bijectors.AbstractBijector)</code> <code></code>","text":"<p>AbstractBijector + concrete <code>forward_log_det_jacobian</code>.</p>"},{"location":"api/bijectors/_bijector/#distreqx.bijectors.AbstractFwdLogDetJacBijector.forward_log_det_jacobian","title":"<code>forward_log_det_jacobian(x: PyTree) -&gt; PyTree</code>","text":"<p>Computes \\(\\log|\\det J(f)(x)|\\).</p>"},{"location":"api/bijectors/_bijector/#distreqx.bijectors.AbstractFowardInverseBijector","title":"<code>distreqx.bijectors.AbstractFowardInverseBijector(distreqx.bijectors.AbstractBijector)</code> <code></code>","text":"<p>AbstractBijector + concrete <code>forward</code> and <code>reverse</code>.</p>"},{"location":"api/bijectors/_bijector/#distreqx.bijectors.AbstractFowardInverseBijector.forward","title":"<code>forward(x: PyTree) -&gt; PyTree</code>","text":"<p>Computes \\(y = f(x)\\).</p>"},{"location":"api/bijectors/_bijector/#distreqx.bijectors.AbstractFowardInverseBijector.inverse","title":"<code>inverse(y: PyTree) -&gt; PyTree</code>","text":"<p>Computes \\(x = f^{-1}(y)\\).</p>"},{"location":"api/bijectors/_bijector/#distreqx.bijectors.AbstractLinearBijector","title":"<code>distreqx.bijectors.AbstractLinearBijector(distreqx.bijectors.AbstractBijector)</code> <code></code>","text":"<p>Base class for linear bijectors.</p> <p>This class provides a base class for bijectors defined as <code>f(x) = Ax</code>, where <code>A</code> is a <code>DxD</code> matrix and <code>x</code> is a <code>D</code>-dimensional vector.</p>"},{"location":"api/bijectors/_bijector/#distreqx.bijectors.AbstractLinearBijector.matrix","title":"<code>matrix</code>  <code>property</code>","text":"<p>The matrix <code>A</code> of the transformation.</p> <p>To be optionally implemented in a subclass.</p> <p>Returns:</p> <ul> <li>An array of shape <code>(D, D)</code>.</li> </ul>"},{"location":"api/bijectors/_bijector/#distreqx.bijectors.AbstractLinearBijector.__init__","title":"<code>__init__()</code>","text":"<p>Initialize self.  See help(type(self)) for accurate signature.</p>"},{"location":"api/bijectors/block/","title":"Block Bijector","text":""},{"location":"api/bijectors/block/#distreqx.bijectors.Block","title":"<code>distreqx.bijectors.Block(distreqx.bijectors.AbstractBijector)</code> <code></code>","text":"<p>A wrapper that promotes a bijector to a block bijector.</p> <p>A block bijector applies a bijector to a k-dimensional array of events, but considers that array of events to be a single event. In practical terms, this means that the log det Jacobian will be summed over its last k dimensions.</p> <p>For example, consider a scalar bijector (such as <code>Tanh</code>) that operates on scalar events. We may want to apply this bijector identically to a 4D array of shape [N, H, W, C] representing a sequence of N images. Doing so naively with a <code>vmap</code> will produce a log det Jacobian of shape [N, H, W, C], because the scalar bijector will assume scalar events and so all 4 dimensions will be considered as batch. To promote the scalar bijector to a \"block scalar\" that operates on the 3D arrays can be done by <code>Block(bijector, ndims=3)</code>. Then, applying the block bijector will produce a log det Jacobian of shape [N] as desired.</p> <p>In general, suppose <code>bijector</code> operates on n-dimensional events. Then, <code>Block(bijector, k)</code> will promote <code>bijector</code> to a block bijector that operates on (k + n)-dimensional events, summing the log det Jacobian over its last k dimensions.</p>"},{"location":"api/bijectors/block/#distreqx.bijectors.Block.__init__","title":"<code>__init__(bijector: distreqx.bijectors.AbstractBijector, ndims: int)</code>","text":"<p>Initializes a Block.</p> <p>Arguments:</p> <ul> <li><code>bijector</code>: the bijector to be promoted to a block bijector. It can be a     distreqx bijector or a callable to be wrapped by <code>Lambda</code>.</li> <li><code>ndims</code>: number of dimensions to promote to event dimensions.</li> </ul>"},{"location":"api/bijectors/chain/","title":"Chain Bijector","text":""},{"location":"api/bijectors/chain/#distreqx.bijectors.Chain","title":"<code>distreqx.bijectors.Chain(distreqx.bijectors.AbstractFwdLogDetJacBijector, distreqx.bijectors.AbstractInvLogDetJacBijector)</code> <code></code>","text":"<p>Composition of a sequence of bijectors into a single bijector.</p> <p>Bijectors are composable: if <code>f</code> and <code>g</code> are bijectors, then <code>g o f</code> is also a bijector. Given a sequence of bijectors <code>[f1, ..., fN]</code>, this class implements the bijector defined by <code>fN o ... o f1</code>.</p> <p>Bijectors are applied in reverse order</p> <p>Given a sequence <code>[f, g]</code>, the <code>Chain</code> bijector computes <code>f(g(x))</code>, not <code>g(f(x))</code>. This matches the mathematical convention for function composition but may be counterintuitive when building layers sequentially.</p> <p>Example</p> <pre><code>layers = []\nlayers.append(f)\nlayers.append(g)\nbijector = distreqx.Chain(layers)\ny = bijector.forward(x)  # y = f(g(x))\n</code></pre>"},{"location":"api/bijectors/chain/#distreqx.bijectors.Chain.__init__","title":"<code>__init__(bijectors: Sequence)</code>","text":"<p>Initializes a Chain bijector.</p> <p>Arguments:</p> <ul> <li><code>bijectors</code>: a sequence of bijectors to be composed into one. Each bijector     can be a distreqx bijector or a callable to be wrapped     by <code>Lambda</code>. The sequence must contain at least one bijector.</li> </ul>"},{"location":"api/bijectors/diag_linear/","title":"Diagonal Linear Bijector","text":""},{"location":"api/bijectors/diag_linear/#distreqx.bijectors.DiagLinear","title":"<code>distreqx.bijectors.DiagLinear(distreqx.bijectors.AbstractLinearBijector)</code> <code></code>","text":"<p>Linear bijector with a diagonal weight matrix.</p> <p>The bijector is defined as \\(f(x) = Ax\\) where \\(A\\) is a \\(D \\times D\\) diagonal matrix. Additional dimensions, if any, index batches.</p> <p>The Jacobian determinant is trivially computed by taking the product of the diagonal entries in \\(A\\). The inverse transformation \\(x = f^{-1}(y)\\) is computed element-wise.</p> <p>The bijector is invertible if and only if the diagonal entries of <code>A</code> are all non-zero. It is the responsibility of the user to make sure that this is the case; the class will make no attempt to verify that the bijector is invertible.</p>"},{"location":"api/bijectors/diag_linear/#distreqx.bijectors.DiagLinear.__init__","title":"<code>__init__(diag: Array)</code>","text":"<p>Initializes the bijector.</p> <p>Arguments:</p> <ul> <li><code>diag</code>: a vector of length D, the diagonal of matrix <code>A</code>.</li> </ul>"},{"location":"api/bijectors/scalar_affine/","title":"Scalar Affine Bijector","text":""},{"location":"api/bijectors/scalar_affine/#distreqx.bijectors.ScalarAffine","title":"<code>distreqx.bijectors.ScalarAffine(distreqx.bijectors.AbstractBijector)</code> <code></code>","text":"<p>An affine bijector that acts elementwise.</p> <p>The bijector is defined as follows:</p> <ul> <li>Forward: \\(y = \\text{scale} \\cdot x + \\text{shift}\\)</li> <li>Forward Jacobian determinant: \\(\\log|\\det J(x)| = \\log|\\text{scale}|\\)</li> <li>Inverse: \\(x = (y - \\text{shift}) / \\text{scale}\\)</li> <li>Inverse Jacobian determinant: \\(\\log|\\det J(y)| = -\\log|\\text{scale}|\\)</li> </ul> <p>where <code>scale</code> and <code>shift</code> are the bijector's parameters.</p>"},{"location":"api/bijectors/scalar_affine/#distreqx.bijectors.ScalarAffine.__init__","title":"<code>__init__(shift: Array, scale: Array | None = None, log_scale: Array | None = None)</code>","text":"<p>Initializes a ScalarAffine bijector.</p> <p>Arguments:</p> <ul> <li><code>shift</code>: the bijector's shift parameter.</li> <li><code>scale</code>: the bijector's scale parameter. NOTE: <code>scale</code> must be non-zero,     otherwise the bijector is not invertible. It is the user's     responsibility to make sure <code>scale</code> is non-zero; the class will     make no attempt to verify this.</li> <li><code>log_scale</code>: the log of the scale parameter. If specified, the     bijector's scale is set equal to <code>exp(log_scale)</code>. Unlike     <code>scale</code>, <code>log_scale</code> is an unconstrained parameter. NOTE: either <code>scale</code>     or <code>log_scale</code> can be specified, but not both. If neither is specified,     the bijector's scale will default to 1.</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: if both <code>scale</code> and <code>log_scale</code> are not None.</li> </ul>"},{"location":"api/bijectors/shift/","title":"Shift  Bijector","text":""},{"location":"api/bijectors/shift/#distreqx.bijectors.Shift","title":"<code>distreqx.bijectors.Shift(distreqx.bijectors.AbstractBijector)</code> <code></code>","text":"<p>Bijector that translates its input elementwise.</p> <p>The bijector is defined as follows:</p> <ul> <li>Forward: \\(y = x + \\text{shift}\\)</li> <li>Forward Jacobian determinant: \\(\\log|\\det J(x)| = 0\\)</li> <li>Inverse: \\(x = y - \\text{shift}\\)</li> <li>Inverse Jacobian determinant: \\(\\log|\\det J(y)| = 0\\)</li> </ul> <p>where <code>shift</code> parameterizes the bijector.</p>"},{"location":"api/bijectors/shift/#distreqx.bijectors.Shift.__init__","title":"<code>__init__(shift: Array)</code>","text":"<p>Initializes a <code>Shift</code> bijector.</p> <p>Arguments:</p> <ul> <li><code>shift</code>: the bijector's shift parameter.</li> </ul>"},{"location":"api/bijectors/sigmoid/","title":"Sigmoid Bijector","text":""},{"location":"api/bijectors/sigmoid/#distreqx.bijectors.Sigmoid","title":"<code>distreqx.bijectors.Sigmoid(distreqx.bijectors.AbstractFowardInverseBijector, distreqx.bijectors.AbstractInvLogDetJacBijector)</code> <code></code>","text":"<p>A bijector that computes the logistic sigmoid.</p> <p>The log-determinant implementation in this bijector is more numerically stable than relying on the automatic differentiation approach used by Lambda, so this bijector should be preferred over Lambda(jax.nn.sigmoid) where possible.</p> <p>Note that the underlying implementation of <code>jax.nn.sigmoid</code> used by the <code>forward</code> function of this bijector does not support inputs of integer type. To invoke the forward function of this bijector on an argument of integer type, it should first be cast explicitly to a floating point type.</p> <p>When the absolute value of the input is large, <code>Sigmoid</code> becomes close to a constant, so that it is not possible to recover the input <code>x</code> from the output <code>y</code> within machine precision. In cases where it is needed to compute both the forward mapping and the backward mapping one after the other to recover the original input <code>x</code>, it is the user's responsibility to simplify the operation to avoid numerical issues. One example of such case is to use the bijector within a <code>Transformed</code> distribution and to obtain the log-probability of samples obtained from the distribution's <code>sample</code> method. For values of the samples for which it is not possible to apply the inverse bijector accurately, <code>log_prob</code> returns NaN. This can be avoided by using <code>sample_and_log_prob</code> instead of <code>sample</code> followed by <code>log_prob</code>.</p>"},{"location":"api/bijectors/sigmoid/#distreqx.bijectors.Sigmoid.is_constant_jacobian","title":"<code>is_constant_jacobian</code>  <code>property</code>","text":"<p>Whether the Jacobian is promised to be constant.</p>"},{"location":"api/bijectors/sigmoid/#distreqx.bijectors.Sigmoid.is_constant_log_det","title":"<code>is_constant_log_det</code>  <code>property</code>","text":"<p>Whether the Jacobian determinant is promised to be constant.</p>"},{"location":"api/bijectors/sigmoid/#distreqx.bijectors.Sigmoid.name","title":"<code>name</code>  <code>property</code>","text":"<p>Name of the bijector.</p>"},{"location":"api/bijectors/sigmoid/#distreqx.bijectors.Sigmoid.forward","title":"<code>forward(x: PyTree) -&gt; PyTree</code>","text":"<p>Computes \\(y = f(x)\\).</p>"},{"location":"api/bijectors/sigmoid/#distreqx.bijectors.Sigmoid.inverse","title":"<code>inverse(y: PyTree) -&gt; PyTree</code>","text":"<p>Computes \\(x = f^{-1}(y)\\).</p>"},{"location":"api/bijectors/sigmoid/#distreqx.bijectors.Sigmoid.inverse_log_det_jacobian","title":"<code>inverse_log_det_jacobian(y: PyTree) -&gt; PyTree</code>","text":"<p>Computes \\(\\log|\\det J(f^{-1})(y)|\\).</p>"},{"location":"api/bijectors/tanh/","title":"TanH Bijector","text":""},{"location":"api/bijectors/tanh/#distreqx.bijectors.Tanh","title":"<code>distreqx.bijectors.Tanh(distreqx.bijectors.AbstractFowardInverseBijector, distreqx.bijectors.AbstractInvLogDetJacBijector)</code> <code></code>","text":"<p>A bijector that computes the hyperbolic tangent.</p> <p>The log-determinant implementation in this bijector is more numerically stable than relying on the automatic differentiation approach used by Lambda, so this bijector should be preferred over Lambda(jnp.tanh) where possible.</p> <p>When the absolute value of the input is large, <code>Tanh</code> becomes close to a constant, so that it is not possible to recover the input <code>x</code> from the output <code>y</code> within machine precision. In cases where it is needed to compute both the forward mapping and the backward mapping one after the other to recover the original input <code>x</code>, it is the user's responsibility to simplify the operation to avoid numerical issues. One example of such case is to use the bijector within a <code>Transformed</code> distribution and to obtain the log-probability of samples obtained from the distribution's <code>sample</code> method. For values of the samples for which it is not possible to apply the inverse bijector accurately, <code>log_prob</code> returns NaN. This can be avoided by using <code>sample_and_log_prob</code> instead of <code>sample</code> followed by <code>log_prob</code>.</p>"},{"location":"api/bijectors/tanh/#distreqx.bijectors.Tanh.is_constant_jacobian","title":"<code>is_constant_jacobian</code>  <code>property</code>","text":"<p>Whether the Jacobian is promised to be constant.</p>"},{"location":"api/bijectors/tanh/#distreqx.bijectors.Tanh.is_constant_log_det","title":"<code>is_constant_log_det</code>  <code>property</code>","text":"<p>Whether the Jacobian determinant is promised to be constant.</p>"},{"location":"api/bijectors/tanh/#distreqx.bijectors.Tanh.name","title":"<code>name</code>  <code>property</code>","text":"<p>Name of the bijector.</p>"},{"location":"api/bijectors/tanh/#distreqx.bijectors.Tanh.forward","title":"<code>forward(x: PyTree) -&gt; PyTree</code>","text":"<p>Computes \\(y = f(x)\\).</p>"},{"location":"api/bijectors/tanh/#distreqx.bijectors.Tanh.inverse","title":"<code>inverse(y: PyTree) -&gt; PyTree</code>","text":"<p>Computes \\(x = f^{-1}(y)\\).</p>"},{"location":"api/bijectors/tanh/#distreqx.bijectors.Tanh.inverse_log_det_jacobian","title":"<code>inverse_log_det_jacobian(y: PyTree) -&gt; PyTree</code>","text":"<p>Computes \\(\\log|\\det J(f^{-1})(y)|\\).</p>"},{"location":"api/bijectors/triangular_linear/","title":"Triangular Linear Bijector","text":""},{"location":"api/bijectors/triangular_linear/#distreqx.bijectors.TriangularLinear","title":"<code>distreqx.bijectors.TriangularLinear(distreqx.bijectors.AbstractLinearBijector)</code> <code></code>","text":"<p>A linear bijector whose weight matrix is triangular.</p> <p>The bijector is defined as \\(f(x) = Ax\\) where \\(A\\) is a \\(D \\times D\\) triangular matrix.</p> <p>The Jacobian determinant can be computed in \\(O(D)\\) as follows:</p> \\[\\log|\\det J(x)| = \\log|\\det A| = \\sum \\log|\\text{diag}(A)|\\] <p>The inverse is computed in \\(O(D^2)\\) by solving the triangular system \\(Ax = y\\).</p> <p>Note</p> <p>The bijector is invertible if and only if all diagonal elements of \\(A\\) are non-zero. It is the responsibility of the user to make sure that this is the case; the class will make no attempt to verify that the bijector is invertible.</p>"},{"location":"api/bijectors/triangular_linear/#distreqx.bijectors.TriangularLinear.__init__","title":"<code>__init__(matrix: Array, is_lower: bool = True)</code>","text":"<p>Initializes a <code>TriangularLinear</code> bijector.</p> <p>Arguments:</p> <ul> <li><code>matrix</code>: a square matrix whose triangular part defines <code>A</code>. Can also be a     batch of matrices. Whether <code>A</code> is the lower or upper triangular part of     <code>matrix</code> is determined by <code>is_lower</code>.</li> <li><code>is_lower</code>: if True, <code>A</code> is set to the lower triangular part of <code>matrix</code>. If     False, <code>A</code> is set to the upper triangular part of <code>matrix</code>.</li> </ul>"},{"location":"api/distributions/_distribution/","title":"Abstract Distributions","text":""},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractDistribution","title":"<code>distreqx.distributions.AbstractDistribution</code> <code></code>","text":"<p>Base class for all distreqx distributions.</p>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractDistribution.log_prob","title":"<code>log_prob(value: PyTree[Array]) -&gt; PyTree[Array]</code>","text":"<p>Calculates the log probability of an event.</p> <p>Arguments:</p> <ul> <li><code>value</code>: An event.</li> </ul> <p>Returns:</p> <ul> <li>The log probability log P(value).</li> </ul>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractDistribution.prob","title":"<code>prob(value: PyTree[Array]) -&gt; PyTree[Array]</code>","text":"<p>Calculates the probability of an event.</p> <p>Arguments:</p> <ul> <li><code>value</code>: An event.</li> </ul> <p>Returns:</p> <ul> <li>The probability P(value).</li> </ul>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractDistribution.cdf","title":"<code>cdf(value: PyTree[Array]) -&gt; PyTree[Array]</code>","text":"<p>Evaluates the cumulative distribution function at <code>value</code>.</p> <p>Arguments:</p> <ul> <li><code>value</code>: An event.</li> </ul> <p>Returns:</p> <ul> <li>The CDF evaluated at value, i.e. P[X &lt;= value].</li> </ul>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractDistribution.survival_function","title":"<code>survival_function(value: PyTree[Array]) -&gt; PyTree[Array]</code>","text":"<p>Evaluates the survival function at <code>value</code>.</p> <p>Note that by default we use a numerically not necessarily stable definition of the survival function in terms of the CDF. More stable definitions should be implemented in subclasses for distributions for which they exist.</p> <p>Arguments:</p> <ul> <li><code>value</code>: An event.</li> </ul> <p>Returns:</p> <ul> <li>The survival function evaluated at <code>value</code>, i.e. P[X &gt; value]</li> </ul>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractDistribution.log_survival_function","title":"<code>log_survival_function(value: PyTree[Array]) -&gt; PyTree[Array]</code>","text":"<p>Evaluates the log of the survival function at <code>value</code>.</p> <p>Note that by default we use a numerically not necessarily stable definition of the log of the survival function in terms of the CDF. More stable definitions should be implemented in subclasses for distributions for which they exist.</p> <p>Arguments:</p> <ul> <li><code>value</code>: An event.</li> </ul> <p>Returns:</p> <ul> <li>The log of the survival function evaluated at <code>value</code>, i.e.     log P[X &gt; value]</li> </ul>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractDistribution.kl_divergence","title":"<code>kl_divergence(other_dist, **kwargs) -&gt; PyTree[Array]</code>","text":"<p>Calculates the KL divergence to another distribution.</p> <p>Arguments:</p> <ul> <li><code>other_dist</code>: A compatible distreqx Distribution.</li> <li><code>kwargs</code>: Additional kwargs.</li> </ul> <p>Returns:</p> <ul> <li>The KL divergence <code>KL(self || other_dist)</code>.</li> </ul>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractDistribution.cross_entropy","title":"<code>cross_entropy(other_dist, **kwargs) -&gt; Array</code>","text":"<p>Calculates the cross entropy to another distribution.</p> <p>Arguments:</p> <ul> <li><code>other_dist</code>: A compatible distreqx Distribution.</li> <li><code>kwargs</code>: Additional kwargs.</li> </ul> <p>Returns:</p> <ul> <li>The cross entropy <code>H(self || other_dist)</code>.</li> </ul>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractSampleLogProbDistribution","title":"<code>distreqx.distributions.AbstractSampleLogProbDistribution(distreqx.distributions.AbstractDistribution)</code> <code></code>","text":"<p>Abstract distribution + concrete <code>sample_and_log_prob</code>.</p>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractProbDistribution","title":"<code>distreqx.distributions.AbstractProbDistribution(distreqx.distributions.AbstractDistribution)</code> <code></code>","text":"<p>Abstract distribution + concrete <code>prob</code>.</p>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractProbDistribution.prob","title":"<code>prob(value: PyTree[Array]) -&gt; PyTree[Array]</code>","text":"<p>Calculates the probability of an event.</p> <p>Arguments:</p> <ul> <li><code>value</code>: An event.</li> </ul> <p>Returns:</p> <ul> <li>The probability P(value).</li> </ul>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractCDFDistribution","title":"<code>distreqx.distributions.AbstractCDFDistribution(distreqx.distributions.AbstractDistribution)</code> <code></code>","text":"<p>Abstract distribution + concrete <code>cdf</code>.</p>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractCDFDistribution.cdf","title":"<code>cdf(value: PyTree[Array]) -&gt; PyTree[Array]</code>","text":"<p>Evaluates the cumulative distribution function at <code>value</code>.</p> <p>Arguments:</p> <ul> <li><code>value</code>: An event.</li> </ul> <p>Returns:</p> <ul> <li>The CDF evaluated at value, i.e. P[X &lt;= value].</li> </ul>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractSTDDistribution","title":"<code>distreqx.distributions.AbstractSTDDistribution(distreqx.distributions.AbstractDistribution)</code> <code></code>","text":"<p>Abstract distribution + concrete <code>stddev</code>.</p>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractSTDDistribution.stddev","title":"<code>stddev() -&gt; PyTree[Array]</code>","text":"<p>Calculate the standard deviation.</p>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractSurvivalDistribution","title":"<code>distreqx.distributions.AbstractSurvivalDistribution(distreqx.distributions.AbstractDistribution)</code> <code></code>","text":"<p>Abstract distribution + concrete <code>survival_function</code> and <code>log_survival_function</code>.</p>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractSurvivalDistribution.survival_function","title":"<code>survival_function(value: PyTree[Array]) -&gt; PyTree[Array]</code>","text":"<p>Evaluates the survival function at <code>value</code>.</p> <p>Note that by default we use a numerically not necessarily stable definition of the survival function in terms of the CDF. More stable definitions should be implemented in subclasses for distributions for which they exist.</p> <p>Arguments:</p> <ul> <li><code>value</code>: An event.</li> </ul> <p>Returns:</p> <ul> <li>The survival function evaluated at <code>value</code>, i.e. P[X &gt; value]</li> </ul>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractSurvivalDistribution.log_survival_function","title":"<code>log_survival_function(value: PyTree[Array]) -&gt; PyTree[Array]</code>","text":"<p>Evaluates the log of the survival function at <code>value</code>.</p> <p>Note that by default we use a numerically not necessarily stable definition of the log of the survival function in terms of the CDF. More stable definitions should be implemented in subclasses for distributions for which they exist.</p> <p>Arguments:</p> <ul> <li><code>value</code>: An event.</li> </ul> <p>Returns:</p> <ul> <li>The log of the survival function evaluated at <code>value</code>, i.e.     log P[X &gt; value]</li> </ul>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractTransformed","title":"<code>distreqx.distributions.AbstractTransformed(distreqx.distributions.AbstractSurvivalDistribution, distreqx.distributions.AbstractProbDistribution)</code> <code></code>","text":"<p>Abstract base class for transformed distributions.</p> <p>See <code>distreqx.distributions.Transformed</code> for full documentation.</p>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractTransformed.dtype","title":"<code>dtype</code>  <code>property</code>","text":"<p>See <code>Distribution.dtype</code>.</p>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractTransformed.event_shape","title":"<code>event_shape</code>  <code>property</code>","text":"<p>See <code>Distribution.event_shape</code>.</p>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractTransformed.log_prob","title":"<code>log_prob(value: Array) -&gt; Array</code>","text":"<p>See <code>Distribution.log_prob</code>.</p>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractTransformed.sample","title":"<code>sample(key: Key[Array, '']) -&gt; Array</code>","text":"<p>Return a sample.</p>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractTransformed.sample_and_log_prob","title":"<code>sample_and_log_prob(key: Key[Array, '']) -&gt; tuple</code>","text":"<p>Return a sample and log prob.</p> <p>This function is more efficient than calling <code>sample</code> and <code>log_prob</code> separately, because it uses only the forward methods of the bijector. It also works for bijectors that don't implement inverse methods.</p> <p>Arguments:</p> <ul> <li><code>key</code>: PRNG key.</li> </ul> <p>Returns:</p> <ul> <li>A tuple of a sample and its log probs.</li> </ul>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractTransformed.entropy","title":"<code>entropy(input_hint: Array | None = None) -&gt; Array</code>","text":"<p>Calculates the Shannon entropy (in Nats).</p> <p>Only works for bijectors with constant Jacobian determinant.</p> <p>Arguments:</p> <ul> <li><code>input_hint</code>: an example sample from the base distribution, used to compute     the constant forward log-determinant. If not specified, it is computed     using a zero array of the shape and dtype of a sample from the base     distribution.</li> </ul> <p>Returns:</p> <ul> <li>The entropy of the distribution.</li> </ul> <p>Raises:</p> <ul> <li><code>NotImplementedError</code>: if bijector's Jacobian determinant is not known to be                        constant.</li> </ul>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractMultivariateNormalFromBijector","title":"<code>distreqx.distributions.AbstractMultivariateNormalFromBijector(distreqx.distributions.AbstractTransformed)</code> <code></code>","text":"<p>AbstractMultivariateNormalFromBijector()</p>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractMultivariateNormalFromBijector.covariance","title":"<code>covariance() -&gt; Array</code>","text":"<p>Calculates the covariance matrix.</p> <p>Returns: - The covariance matrix, of shape <code>k x k</code>.</p>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractMultivariateNormalFromBijector.variance","title":"<code>variance() -&gt; Array</code>","text":"<p>Calculates the variance of all one-dimensional marginals.</p>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractMultivariateNormalFromBijector.stddev","title":"<code>stddev() -&gt; Array</code>","text":"<p>Calculates the standard deviation (the square root of the variance).</p>"},{"location":"api/distributions/_distribution/#distreqx.distributions.AbstractMultivariateNormalFromBijector.kl_divergence","title":"<code>kl_divergence(other_dist, **kwargs) -&gt; Array</code>","text":"<p>Calculates the KL divergence to another distribution.</p> <p>Arguments:</p> <ul> <li><code>other_dist</code>: A compatible disteqx distribution.</li> <li><code>kwargs</code>: Additional kwargs.</li> </ul> <p>Returns:</p> <p>The KL divergence <code>KL(self || other_dist)</code>.</p>"},{"location":"api/distributions/bernoulli/","title":"Bernoulli","text":""},{"location":"api/distributions/bernoulli/#distreqx.distributions.Bernoulli","title":"<code>distreqx.distributions.Bernoulli(distreqx.distributions.AbstractSampleLogProbDistribution, distreqx.distributions.AbstractSTDDistribution, distreqx.distributions.AbstractSurvivalDistribution)</code> <code></code>","text":"<p>Bernoulli distribution of shape dims.</p> <p>Bernoulli distribution with parameter <code>probs</code>, the probability of outcome <code>1</code>.</p>"},{"location":"api/distributions/bernoulli/#distreqx.distributions.Bernoulli.__init__","title":"<code>__init__(logits: Array | None = None, probs: Array | None = None)</code>","text":"<p>Initializes a Bernoulli distribution.</p> <p>Arguments:</p> <ul> <li><code>logits</code>: Logit transform of the probability of a <code>1</code> event (<code>0</code> otherwise),     i.e. <code>probs = sigmoid(logits)</code>. Only one of <code>logits</code> or <code>probs</code> can be     specified.</li> <li><code>probs</code>: Probability of a <code>1</code> event (<code>0</code> otherwise). Only one of <code>logits</code> or     <code>probs</code> can be specified.</li> </ul>"},{"location":"api/distributions/beta/","title":"Beta","text":""},{"location":"api/distributions/beta/#distreqx.distributions.Beta","title":"<code>distreqx.distributions.Beta(distreqx.distributions.AbstractSampleLogProbDistribution, distreqx.distributions.AbstractSTDDistribution, distreqx.distributions.AbstractProbDistribution, distreqx.distributions.AbstractSurvivalDistribution)</code> <code></code>","text":"<p>Beta distribution with parameters <code>alpha</code> and <code>beta</code>.</p> <p>The PDF of a Beta distributed random variable <code>X</code> is defined on the interval <code>0 &lt;= X &lt;= 1</code> and has the form: <pre><code>p(x; alpha, beta) = x ** {alpha - 1} * (1 - x) ** (beta - 1) / B(alpha, beta)\n</code></pre> where <code>B(alpha, beta)</code> is the beta function, and the <code>alpha, beta &gt; 0</code> are the shape parameters.</p> <p>Note that the support of the distribution does not include <code>x = 0</code> or <code>x = 1</code> if <code>alpha &lt; 1</code> or <code>beta &lt; 1</code>, respectively.</p>"},{"location":"api/distributions/beta/#distreqx.distributions.Beta.__init__","title":"<code>__init__(alpha: float | Float[Array, '...'], beta: float | Float[Array, '...'])</code>","text":"<p>Initializes a Beta distribution.</p> <p>Arguments:</p> <ul> <li><code>alpha</code>: Shape parameter <code>alpha</code> of the distribution. Must be positive.</li> <li><code>beta</code>: Shape parameter <code>beta</code> of the distribution. Must be positive.</li> </ul>"},{"location":"api/distributions/categorical/","title":"Categorical","text":""},{"location":"api/distributions/categorical/#distreqx.distributions.Categorical","title":"<code>distreqx.distributions.Categorical(distreqx.distributions.AbstractSTDDistribution, distreqx.distributions.AbstractSampleLogProbDistribution, distreqx.distributions.AbstractSurvivalDistribution)</code> <code></code>","text":"<p>Categorical distribution over integers.</p> <p>The Categorical distribution is parameterized by either probabilities (<code>probs</code>) or unormalized log-probabilities (<code>logits</code>) of a set of <code>K</code> classes. It is defined over the integers <code>{0, 1, ..., K-1}</code>.</p>"},{"location":"api/distributions/categorical/#distreqx.distributions.Categorical.__init__","title":"<code>__init__(logits: Array | None = None, probs: Array | None = None)</code>","text":"<p>Initializes a Categorical distribution.</p> <p>Arguments:</p> <ul> <li><code>logits</code>: Logit transform of the probability of each category. Only one     of <code>logits</code> or <code>probs</code> can be specified.</li> <li><code>probs</code>: Probability of each category. Only one of <code>logits</code> or <code>probs</code> can     be specified.</li> </ul>"},{"location":"api/distributions/categorical/#distreqx.distributions.OneHotCategorical","title":"<code>distreqx.distributions.OneHotCategorical(distreqx.distributions.AbstractSTDDistribution, distreqx.distributions.AbstractSampleLogProbDistribution, distreqx.distributions.AbstractSurvivalDistribution)</code> <code></code>","text":"<p>OneHotCategorical distribution.</p>"},{"location":"api/distributions/categorical/#distreqx.distributions.OneHotCategorical.__init__","title":"<code>__init__(logits: Array | None = None, probs: Array | None = None)</code>","text":"<p>Initializes a OneHotCategorical distribution.</p> <p>Arguments:</p> <ul> <li><code>logits</code>: Logit transform of the probability of each category. Only one     of <code>logits</code> or <code>probs</code> can be specified.</li> <li><code>probs</code>: Probability of each category. Only one of <code>logits</code> or <code>probs</code> can     be specified.</li> </ul>"},{"location":"api/distributions/gamma/","title":"Gamma","text":""},{"location":"api/distributions/gamma/#distreqx.distributions.Gamma","title":"<code>distreqx.distributions.Gamma(distreqx.distributions.AbstractSampleLogProbDistribution, distreqx.distributions.AbstractProbDistribution, distreqx.distributions.AbstractSurvivalDistribution, distreqx.distributions.AbstractCDFDistribution)</code> <code></code>","text":"<p>Gamma distribution with parameters <code>concentration</code> and <code>rate</code>.</p> <p>The PDF of a Gamma distributed random variable \\(X\\) is defined on the interval \\(X &gt; 0\\) and has the form:</p> \\[p(x; \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}     x^{\\alpha - 1} e^{-\\beta x}\\] <p>where \\(\\alpha &gt; 0\\) is the concentration (shape) parameter and \\(\\beta &gt; 0\\) is the rate (inverse scale) parameter.</p>"},{"location":"api/distributions/gamma/#distreqx.distributions.Gamma.__init__","title":"<code>__init__(concentration: float | Float[Array, '...'], rate: float | Float[Array, '...'])</code>","text":"<p>Initializes a Gamma distribution.</p> <p>Arguments:</p> <ul> <li><code>concentration</code>: Concentration (shape) parameter. Must be positive.</li> <li><code>rate</code>: Rate (inverse scale) parameter. Must be positive.</li> </ul>"},{"location":"api/distributions/independent/","title":"Independent Distribution","text":""},{"location":"api/distributions/independent/#distreqx.distributions.Independent","title":"<code>distreqx.distributions.Independent(distreqx.distributions.AbstractProbDistribution, distreqx.distributions.AbstractCDFDistribution, distreqx.distributions.AbstractSurvivalDistribution)</code> <code></code>","text":"<p>Independent distribution obtained from child distributions.</p> <p>Tip</p> <p><code>Independent</code> reinterprets batch dimensions as event dimensions. This is useful when you want to model a multivariate distribution as independent univariate distributions (e.g., diagonal Gaussian) but still want <code>log_prob</code> to return a single scalar per sample.</p>"},{"location":"api/distributions/independent/#distreqx.distributions.Independent.__init__","title":"<code>__init__(distribution: distreqx.distributions.AbstractDistribution)</code>","text":"<p>Initializes an Independent distribution.</p> <p>Arguments:</p> <ul> <li><code>distribution</code>: Base distribution instance.</li> </ul>"},{"location":"api/distributions/mixture_same_family/","title":"Mixture Distribution","text":""},{"location":"api/distributions/mixture_same_family/#distreqx.distributions.MixtureSameFamily","title":"<code>distreqx.distributions.MixtureSameFamily(distreqx.distributions.AbstractSTDDistribution, distreqx.distributions.AbstractSampleLogProbDistribution, distreqx.distributions.AbstractSurvivalDistribution, distreqx.distributions.AbstractProbDistribution, distreqx.distributions.AbstractCDFDistribution)</code> <code></code>","text":"<p>Mixture with components provided from a single vmapped distribution.</p>"},{"location":"api/distributions/mixture_same_family/#distreqx.distributions.MixtureSameFamily.__init__","title":"<code>__init__(mixture_distribution: distreqx.distributions.Categorical, components_distribution: distreqx.distributions.AbstractDistribution)</code>","text":"<p>Initializes a mixture distribution for components of a shared family.</p> <p>Arguments:</p> <ul> <li><code>mixture_distribution</code>: Distribution over selecting components.</li> <li><code>components_distribution</code>: Component distribution.</li> </ul>"},{"location":"api/distributions/mixture_same_family/#distreqx.distributions.MixtureSameFamily.posterior_marginal","title":"<code>posterior_marginal(observation: Array) -&gt; distreqx.distributions.Categorical</code>","text":"<p>Generate the posterior distribution given a datapoint.</p> <p>Arguments:</p> <ul> <li><code>observation</code>: the data point to compute the distribution over</li> </ul> <p>Returns:</p> <p>The computed categorical distribution</p>"},{"location":"api/distributions/mixture_same_family/#distreqx.distributions.MixtureSameFamily.posterior_mode","title":"<code>posterior_mode(observation: Array) -&gt; Array</code>","text":"<p>Compute the most likely component a data point falls into.</p> <p>Arguments:</p> <ul> <li><code>observation</code>: the data point to compute the mode of</li> </ul> <p>Returns:</p> <p>The computed mode</p>"},{"location":"api/distributions/mvn_diag/","title":"Diagonal Multivariate Normal","text":""},{"location":"api/distributions/mvn_diag/#distreqx.distributions.MultivariateNormalDiag","title":"<code>distreqx.distributions.MultivariateNormalDiag(distreqx.distributions.AbstractMultivariateNormalFromBijector)</code> <code></code>","text":"<p>Multivariate normal distribution on <code>R^k</code> with diagonal covariance.</p>"},{"location":"api/distributions/mvn_diag/#distreqx.distributions.MultivariateNormalDiag.__init__","title":"<code>__init__(loc: Array | None = None, scale_diag: Array | None = None)</code>","text":"<p>Initializes a MultivariateNormalDiag distribution.</p> <p>Arguments:</p> <ul> <li><code>loc</code>: Mean vector of the distribution. If not specified, it defaults     to zeros. At least one of <code>loc</code> and <code>scale_diag</code> must be specified.</li> <li><code>scale_diag</code>: Vector of standard deviations.  If not specified, it     defaults to ones. At least one of <code>loc</code> and<code>scale_diag</code> must be specified.</li> </ul>"},{"location":"api/distributions/mvn_from_bijector/","title":"Multivariate Normal from Bijector","text":""},{"location":"api/distributions/mvn_from_bijector/#distreqx.distributions.MultivariateNormalFromBijector","title":"<code>distreqx.distributions.MultivariateNormalFromBijector(distreqx.distributions.AbstractMultivariateNormalFromBijector)</code> <code></code>","text":"<p>Multivariate normal distribution on \\(\\mathbb{R}^k\\).</p> <p>The multivariate normal over \\(x\\) is characterized by an invertible affine transformation \\(x = f(z) = Az + b\\), where \\(z\\) is a random variable that follows a standard multivariate normal on \\(\\mathbb{R}^k\\), i.e., \\(p(z) = \\mathcal{N}(0, I_k)\\), \\(A\\) is a \\(k \\times k\\) transformation matrix, and \\(b\\) is a \\(k\\)-dimensional vector.</p> <p>The resulting PDF on \\(x\\) is a multivariate normal, \\(p(x) = \\mathcal{N}(b, C)\\), where \\(C = AA^T\\) is the covariance matrix.</p> <p>The transformation \\(x = f(z)\\) must be specified by a linear scale bijector implementing the operation \\(Az\\) and a shift (or location) term \\(b\\).</p>"},{"location":"api/distributions/mvn_from_bijector/#distreqx.distributions.MultivariateNormalFromBijector.__init__","title":"<code>__init__(loc: Array, scale: distreqx.bijectors.AbstractLinearBijector)</code>","text":"<p>Initializes the distribution.</p> <p>Arguments:</p> <ul> <li><code>loc</code>: The term <code>b</code>, i.e., the mean of the multivariate normal distribution.</li> <li><code>scale</code>: The bijector specifying the linear transformation <code>A @ z</code>, as     described in the class docstring.</li> </ul>"},{"location":"api/distributions/mvn_tri/","title":"Triangular Multivariate Normal","text":""},{"location":"api/distributions/mvn_tri/#distreqx.distributions.MultivariateNormalTri","title":"<code>distreqx.distributions.MultivariateNormalTri(distreqx.distributions.AbstractMultivariateNormalFromBijector)</code> <code></code>","text":"<p>Multivariate normal distribution on \\(\\mathbb{R}^k\\).</p> <p>The <code>MultivariateNormalTri</code> distribution is parameterized by a \\(k\\)-length location (mean) vector \\(b\\) and a (lower or upper) triangular scale matrix \\(S\\) of size \\(k \\times k\\). The covariance matrix is \\(C = SS^T\\).</p> <p>Note</p> <p>The <code>scale_tri</code> matrix must have non-zero diagonal elements for the distribution to be valid. This class does not verify this condition.</p>"},{"location":"api/distributions/mvn_tri/#distreqx.distributions.MultivariateNormalTri.__init__","title":"<code>__init__(loc: Array | None = None, scale_tri: Array | None = None, is_lower: bool = True)</code>","text":"<p>Initializes a MultivariateNormalTri distribution.</p> <p>Arguments:</p> <ul> <li><code>loc</code>: Mean vector of the distribution of shape <code>k</code>.     If not specified, it defaults to zeros.</li> <li><code>scale_tri</code>: The scale matrix <code>S</code>. It must be a <code>k x k</code> triangular matrix.     If <code>scale_tri</code> is not triangular, the entries above or below the main     diagonal will be ignored. The parameter <code>is_lower</code> specifies if <code>scale_tri</code>     is lower or upper triangular. It is the responsibility of the user to make     sure that <code>scale_tri</code> only contains non-zero elements in its diagonal;     this class makes no attempt to verify that. If <code>scale_tri</code> is not specified,     it defaults to the identity.</li> <li><code>is_lower</code>: Indicates if <code>scale_tri</code> is lower (if True) or upper (if False)     triangular.</li> </ul>"},{"location":"api/distributions/normal/","title":"Normal","text":""},{"location":"api/distributions/normal/#distreqx.distributions.Normal","title":"<code>distreqx.distributions.Normal(distreqx.distributions.AbstractProbDistribution)</code> <code></code>","text":"<p>Normal distribution with location <code>loc</code> and <code>scale</code> parameters.</p>"},{"location":"api/distributions/normal/#distreqx.distributions.Normal.__init__","title":"<code>__init__(loc: Array, scale: Array)</code>","text":"<p>Initializes a Normal distribution.</p> <p>Arguments:</p> <ul> <li><code>loc</code>: Mean of the distribution.</li> <li><code>scale</code>: Standard deviation of the distribution.</li> </ul>"},{"location":"api/distributions/normal/#distreqx.distributions.Normal.entropy","title":"<code>entropy() -&gt; Array</code>","text":"<p>Calculates the Shannon entropy (in nats).</p>"},{"location":"api/distributions/transformed/","title":"Transformed","text":""},{"location":"api/distributions/transformed/#distreqx.distributions.Transformed","title":"<code>distreqx.distributions.Transformed(distreqx.distributions.AbstractTransformed, distreqx.distributions.AbstractSTDDistribution)</code> <code></code>","text":"<p>Distribution of a random variable transformed by a bijective function.</p> <p>Let \\(X\\) be a continuous random variable and \\(Y = f(X)\\) be a random variable transformed by a differentiable bijection \\(f\\) (a \"bijector\"). Given the distribution of \\(X\\) (the \"base distribution\") and the bijector \\(f\\), this class implements the distribution of \\(Y\\) (also known as the pushforward of the base distribution through \\(f\\)).</p> <p>The probability density of \\(Y\\) can be computed by:</p> \\[\\log p(y) = \\log p(x) - \\log|\\det J(f)(x)|\\] <p>where \\(p(x)\\) is the probability density of \\(X\\) (the \"base density\") and \\(J(f)(x)\\) is the Jacobian matrix of \\(f\\), both evaluated at \\(x = f^{-1}(y)\\).</p> <p>Sampling from a Transformed distribution involves two steps: sampling from the base distribution \\(x \\sim p(x)\\) and then evaluating \\(y = f(x)\\). For example:</p> <pre><code>  dist = distrax.Normal(loc=0., scale=1.)\n  bij = distrax.ScalarAffine(shift=jnp.asarray([3., 3., 3.]))\n  transformed_dist = distrax.Transformed(distribution=dist, bijector=bij)\n  samples = transformed_dist.sample(jax.random.key(0))\n  print(samples)  # [2.7941577, 2.7941577, 2.7941577]\n</code></pre> <p>This assumes that the <code>forward</code> function of the bijector is traceable; that is, it is a pure function that does not contain run-time branching. Functions that do not strictly meet this requirement can still be used, but we cannot guarantee that the shapes, dtype, and KL computations involving the transformed distribution can be correctly obtained.</p> <p>Tip</p> <p><code>Transformed</code> is the foundation for building normalizing flows. Chain together multiple bijectors using <code>distreqx.bijectors.Chain</code> to create complex transformations.</p> <p>Warning</p> <p>Computing <code>entropy</code>, <code>mean</code>, and <code>mode</code> only works when the bijector has a constant Jacobian determinant. For bijectors with non-constant Jacobians (e.g., neural network-based flows), these methods will raise <code>NotImplementedError</code>.</p>"},{"location":"api/distributions/transformed/#distreqx.distributions.Transformed.__init__","title":"<code>__init__(distribution: distreqx.distributions.AbstractDistribution, bijector: distreqx.bijectors.AbstractBijector)</code>","text":"<p>Initializes a Transformed distribution.</p> <p>Arguments: - <code>distribution</code>: the base distribution. - <code>bijector</code>: a differentiable bijective transformation. Can be a bijector or     a callable to be wrapped by <code>Lambda</code> bijector.</p>"},{"location":"api/distributions/transformed/#distreqx.distributions.Transformed.mean","title":"<code>mean() -&gt; Array</code>","text":"<p>Calculates the mean.</p>"},{"location":"api/distributions/transformed/#distreqx.distributions.Transformed.mode","title":"<code>mode() -&gt; Array</code>","text":"<p>Calculates the mode.</p>"},{"location":"api/distributions/transformed/#distreqx.distributions.Transformed.kl_divergence","title":"<code>kl_divergence(other_dist, **kwargs) -&gt; Array</code>","text":"<p>Obtains the KL divergence between two Transformed distributions.</p> <p>This computes the KL divergence between two Transformed distributions with the same bijector. If the two Transformed distributions do not have the same bijector, an error is raised. To determine if the bijectors are equal, this method proceeds as follows:</p> <ul> <li> <p>If both bijectors are the same instance of a distreqx bijector, then they are declared equal.</p> </li> <li> <p>If not the same instance, we check if they are equal according to their <code>same_as</code> predicate.</p> </li> <li> <p>Otherwise, the string representation of the Jaxpr of the <code>forward</code> method of each bijector is compared. If both string representations are equal, the bijectors are declared equal.</p> </li> <li> <p>Otherwise, the bijectors cannot be guaranteed to be equal and an error is raised.</p> </li> </ul> <p>Arguments:</p> <ul> <li><code>other_dist</code>: A Transformed distribution.</li> <li><code>input_hint</code>: keyword argument, an example sample from the base distribution,     used to trace the <code>forward</code> method. If not specified, it is computed using     a zero array of the shape and dtype of a sample from the base distribution.</li> </ul> <p>Returns:</p> <ul> <li><code>KL(dist1 || dist2)</code>.</li> </ul> <p>Raises:</p> <ul> <li><code>NotImplementedError</code>: If bijectors are not known to be equal.</li> <li><code>ValueError</code>: If the base distributions do not have the same <code>event_shape</code>.</li> </ul>"},{"location":"api/distributions/uniform/","title":"Uniform","text":""},{"location":"api/distributions/uniform/#distreqx.distributions.Uniform","title":"<code>distreqx.distributions.Uniform(distreqx.distributions.AbstractSTDDistribution, distreqx.distributions.AbstractSurvivalDistribution)</code> <code></code>","text":"<p>Uniform distribution with <code>low</code> and <code>high</code> parameters.</p>"},{"location":"api/distributions/uniform/#distreqx.distributions.Uniform.dtype","title":"<code>dtype</code>  <code>property</code>","text":"<p>Data type of a sample</p>"},{"location":"api/distributions/uniform/#distreqx.distributions.Uniform.name","title":"<code>name</code>  <code>property</code>","text":"<p>Distribution name.</p>"},{"location":"api/distributions/uniform/#distreqx.distributions.Uniform.cross_entropy","title":"<code>cross_entropy(other_dist, **kwargs) -&gt; Array</code>","text":"<p>Calculates the cross entropy to another distribution.</p> <p>Arguments:</p> <ul> <li><code>other_dist</code>: A compatible distreqx Distribution.</li> <li><code>kwargs</code>: Additional kwargs.</li> </ul> <p>Returns:</p> <ul> <li>The cross entropy <code>H(self || other_dist)</code>.</li> </ul>"},{"location":"api/distributions/uniform/#distreqx.distributions.Uniform.log_survival_function","title":"<code>log_survival_function(value: PyTree[Array]) -&gt; PyTree[Array]</code>","text":"<p>Evaluates the log of the survival function at <code>value</code>.</p> <p>Note that by default we use a numerically not necessarily stable definition of the log of the survival function in terms of the CDF. More stable definitions should be implemented in subclasses for distributions for which they exist.</p> <p>Arguments:</p> <ul> <li><code>value</code>: An event.</li> </ul> <p>Returns:</p> <ul> <li>The log of the survival function evaluated at <code>value</code>, i.e.     log P[X &gt; value]</li> </ul>"},{"location":"api/distributions/uniform/#distreqx.distributions.Uniform.stddev","title":"<code>stddev() -&gt; PyTree[Array]</code>","text":"<p>Calculate the standard deviation.</p>"},{"location":"api/distributions/uniform/#distreqx.distributions.Uniform.survival_function","title":"<code>survival_function(value: PyTree[Array]) -&gt; PyTree[Array]</code>","text":"<p>Evaluates the survival function at <code>value</code>.</p> <p>Note that by default we use a numerically not necessarily stable definition of the survival function in terms of the CDF. More stable definitions should be implemented in subclasses for distributions for which they exist.</p> <p>Arguments:</p> <ul> <li><code>value</code>: An event.</li> </ul> <p>Returns:</p> <ul> <li>The survival function evaluated at <code>value</code>, i.e. P[X &gt; value]</li> </ul>"},{"location":"api/utils/math/","title":"Math","text":""},{"location":"api/utils/math/#distreqx.utils.math.multiply_no_nan","title":"<code>distreqx.utils.math.multiply_no_nan(x: Array, y: Array) -&gt; Array</code>","text":"<p>Computes the element-wise product of <code>x</code> and <code>y</code>, returning 0 where <code>y</code> is zero, even if <code>x</code> is NaN or infinite.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First input.</li> <li><code>y</code>: Second input.</li> </ul> <p>Returns:</p> <ul> <li>The product of <code>x</code> and <code>y</code>.</li> </ul> <p>Raises:</p> <ul> <li>ValueError if the shapes of <code>x</code> and <code>y</code> do not match.</li> </ul>"},{"location":"api/utils/math/#distreqx.utils.math.power_no_nan","title":"<code>distreqx.utils.math.power_no_nan(x: Array, y: Array) -&gt; Array</code>","text":"<p>Computes <code>x ** y</code>, ensuring the result is 1.0 when <code>y</code> is zero, following the convention <code>0 ** 0 = 1</code>.</p> <p>Arguments:</p> <ul> <li><code>x</code>: First input.</li> <li><code>y</code>: Second input.</li> </ul> <p>Returns:</p> <ul> <li>The power <code>x ** y</code>.</li> </ul>"},{"location":"api/utils/math/#distreqx.utils.math.mul_exp","title":"<code>distreqx.utils.math.mul_exp(x: Array, logp: Array) -&gt; Array</code>","text":"<p>Returns <code>x * exp(logp)</code> with zero output if <code>exp(logp) == 0</code>.</p> <p>Arguments:</p> <ul> <li><code>x</code>: An array.</li> <li><code>logp</code>: An array representing logarithms.</li> </ul> <p>Returns:</p> <ul> <li>The result of <code>x * exp(logp)</code>.</li> </ul>"},{"location":"api/utils/math/#distreqx.utils.math.normalize","title":"<code>distreqx.utils.math.normalize(*, probs: typing.Optional[Array] = None, logits: typing.Optional[Array] = None) -&gt; Array</code>","text":"<p>Normalizes logits via log_softmax or probabilities to ensure they sum to one.</p> <p>Arguments:</p> <ul> <li><code>probs</code>: Probability values.</li> <li><code>logits</code>: Logit values.</li> </ul> <p>Returns:</p> <ul> <li>Normalized probabilities or logits.</li> </ul>"},{"location":"api/utils/math/#distreqx.utils.math.sum_last","title":"<code>distreqx.utils.math.sum_last(x: Array, ndims: int) -&gt; Array</code>","text":"<p>Sums the last <code>ndims</code> axes of array <code>x</code>.</p> <p>Arguments:</p> <ul> <li><code>x</code>: An array.</li> <li><code>ndims</code>: The number of last dimensions to sum.</li> </ul> <p>Returns:</p> <ul> <li>The sum of the last <code>ndims</code> dimensions of <code>x</code>.</li> </ul>"},{"location":"api/utils/math/#distreqx.utils.math.log_expbig_minus_expsmall","title":"<code>distreqx.utils.math.log_expbig_minus_expsmall(big: Array, small: Array) -&gt; Array</code>","text":"<p>Stable implementation of <code>log(exp(big) - exp(small))</code>.</p> <p>Arguments:</p> <ul> <li><code>big</code>: First input.</li> <li><code>small</code>: Second input. It must be <code>small &lt;= big</code>.</li> </ul> <p>Returns:</p> <ul> <li>The resulting <code>log(exp(big) - exp(small))</code>.</li> </ul>"},{"location":"api/utils/math/#distreqx.utils.math.log_beta","title":"<code>distreqx.utils.math.log_beta(a: Array, b: Array) -&gt; Array</code>","text":"<p>Obtains the log of the beta function <code>log B(a, b)</code>.</p> <p>Arguments:</p> <ul> <li><code>a</code>: First input. It must be positive.</li> <li><code>b</code>: Second input. It must be positive.</li> </ul> <p>Returns:</p> <ul> <li>The value <code>log B(a, b) = log Gamma(a) + log Gamma(b) - log Gamma(a + b)</code>,   where <code>Gamma</code> is the Gamma function, obtained through stable   computation of <code>log Gamma</code>.</li> </ul>"},{"location":"api/utils/math/#distreqx.utils.math.log_beta_multivariate","title":"<code>distreqx.utils.math.log_beta_multivariate(a: Array) -&gt; Array</code>","text":"<p>Obtains the log of the multivariate beta function <code>log B(a)</code>.</p> <p>Arguments:</p> <ul> <li><code>a</code>: An array of length <code>K</code> containing positive values.</li> </ul> <p>Returns:</p> <ul> <li>The value   <code>log B(a) = sum_{k=1}^{K} log Gamma(a_k) - log Gamma(sum_{k=1}^{K} a_k)</code>,   where <code>Gamma</code> is the Gamma function, obtained through stable   computation of <code>log Gamma</code>.</li> </ul>"},{"location":"misc/faq/","title":"FAQ","text":""},{"location":"misc/faq/#why-not-just-use-distrax","title":"Why not just use distrax?","text":"<p>The simple answer to that question is \"I tried\". Distrax is a the product of a lot of great work, especially helpful for working with TFP, but in the current era of jax packages lacks important elements:</p> <ul> <li>It's only semi-maintained (there have been no responses to any issues in the last &gt;6 months)</li> <li>It doesn't always play nice with other jax packages and can be slow (see: #193, #383, #252, #269, #16, #16170)</li> <li>You need Tensorflow to use it </li> </ul>"},{"location":"misc/faq/#why-use-equinox","title":"Why use equinox?","text":"<p>The <code>Jittable</code> class is basically an equinox module (if you squint) and while we could reimplement a custom Module class (like GPJax does), why reinvent the wheel? Equinox is actively being developed and should it become inactive is still possible to maintain.</p>"},{"location":"misc/faq/#what-about-flowjax","title":"What about flowjax?","text":"<p>When I started this project, I was unaware of flowjax. Although flowjax does provide a lot of advanced tooling for NFs and bijections, there are notable differences. <code>distreqx</code> is less specialized and provides a broader baseline set of tools (e.g. distributions). flowjax has more advanced NF tools. <code>distreqx</code> also adheres to an abstract/final design pattern from the development side. flowjax also approaches the concept of \"transformed\" distributions in a different manner.</p>"},{"location":"examples/01_vae/","title":"Variational Autoencoder","text":"<p>In this example we will be implementing a variational autoencoder using distreqx.</p> Reference <pre><code>@inproceedings{kingma2014auto,\n    title={Auto-encoding variational {B}ayes},\n    author={Kingma, Diederik P and Welling, Max},\n    booktitle={International Conference on Learning Representations},\n    year={2014},\n}\n</code></pre> <pre><code>import equinox as eqx\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport optax\nimport tensorflow_datasets as tfds\nfrom tqdm.notebook import tqdm\n\nfrom distreqx import distributions\n</code></pre> <p>First, we need to create a standard small encoder and decoder module. The shapes are hard coded for the MNIST dataset we will be using.</p> <pre><code>class Encoder(eqx.Module):\n    encoder: eqx.nn.Linear\n    mean: eqx.nn.Linear\n    std: eqx.nn.Linear\n\n    def __init__(self, key, input_size=784, hidden_size=512, latent_size=10):\n        keys = jax.random.split(key, 3)\n        self.encoder = eqx.nn.Linear(input_size, hidden_size, key=keys[0])\n        self.mean = eqx.nn.Linear(hidden_size, latent_size, key=keys[1])\n        self.std = eqx.nn.Linear(hidden_size, latent_size, key=keys[2])\n\n    def __call__(self, x):\n        x = x.flatten()\n        x = self.encoder(x)\n        x = jax.nn.relu(x)\n        mean = self.mean(x)\n        log_stddev = self.std(x)\n        stddev = jnp.exp(log_stddev)\n        return mean, stddev\n\n\nclass Decoder(eqx.Module):\n    ln1: eqx.nn.Linear\n    ln2: eqx.nn.Linear\n\n    def __init__(self, key, input_size, hidden_size, output_shape=784):\n        keys = jax.random.split(key, 2)\n        self.ln1 = eqx.nn.Linear(input_size, hidden_size, key=keys[0])\n        self.ln2 = eqx.nn.Linear(hidden_size, output_shape, key=keys[1])\n\n    def __call__(self, z):\n        z = self.ln1(z)\n        z = jax.nn.relu(z)\n        logits = self.ln2(z)\n        logits = jnp.reshape(logits, (28, 28, 1))\n        return logits\n</code></pre> <p>Next we can construct the VAE object. It consists of an encoder and decoder, the encoder provides the mean and variance of the multivariate Gaussian prior. The output of the decoder represents the logits of a bernoulli distribution over the pixel space. Note that the <code>Independent</code> here is a bit of a legacy artifact. In general, <code>distreqx</code> encourages <code>vmap</code> based approaches to distributions and offloads any batching to the user. However, it is often possible to implicitly batch computations for certain disributions (sometimes even correctly). <code>Independent</code> is merely a helper that sums over dimensions, so even though we don't <code>vmap</code> over the bernoulli (like we often should), we can still sum over batch dimensions (since the event shape of a bernoulli is ()).</p> <pre><code>class VAEOutput(eqx.Module):\n    variational_distrib: distributions.AbstractDistribution\n    likelihood_distrib: distributions.AbstractDistribution\n    image: jnp.ndarray\n\n\nclass VAE(eqx.Module):\n    encoder: Encoder\n    decoder: Decoder\n\n    def __init__(\n        self,\n        key,\n        input_size=784,\n        latent_size=10,\n        hidden_size=512,\n    ):\n        keys = jax.random.split(key)\n        self.encoder = Encoder(keys[0], input_size, hidden_size, latent_size)\n        self.decoder = Decoder(keys[1], latent_size, hidden_size, input_size)\n\n    def __call__(self, x, key):\n        keys = jax.random.split(key)\n        x = x.astype(jnp.float32)\n\n        # q(z|x) = N(mean(x), covariance(x))\n        mean, stddev = self.encoder(x)\n        variational_distrib = distributions.MultivariateNormalDiag(\n            loc=mean, scale_diag=stddev\n        )\n        z = variational_distrib.sample(keys[0])\n\n        # p(x|z) = \\Prod Bernoulli(logits(z))\n        logits = self.decoder(z)\n\n        likelihood_distrib = distributions.Independent(\n            distributions.Bernoulli(logits=logits)\n        )\n\n        # Generate images from the likelihood\n        image = likelihood_distrib.sample(keys[1])\n\n        return VAEOutput(variational_distrib, likelihood_distrib, image)\n</code></pre> <p>Now we can train our model with the standard ELBO. Keep in mind, here we require some <code>vmap</code>ing over the distribution, since we now have an additional batch dimension (that we do not want to have <code>Independent</code> sum over).</p> <pre><code>def load_dataset(split, batch_size):\n    ds = tfds.load(\"binarized_mnist\", split=split, shuffle_files=True)\n    ds = ds.shuffle(buffer_size=10 * batch_size)\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(buffer_size=5)\n    ds = ds.repeat()\n    return iter(tfds.as_numpy(ds))\n\n\n@eqx.filter_jit\ndef loss_fn(model, key, batch):\n    \"\"\"Loss = -ELBO, where ELBO = E_q[log p(x|z)] - KL(q(z|x) || p(z)).\"\"\"\n\n    outputs = eqx.filter_vmap(model)(batch, key)\n\n    # p(z) = N(0, I)\n    prior_z = distributions.MultivariateNormalDiag(\n        loc=jnp.zeros(latent_size), scale_diag=jnp.ones(latent_size)\n    )\n    # we need to make surve to vmap over the distribution itself!\n    # see also: https://docs.kidger.site/equinox/tricks/#ensembling\n    log_likelihood = eqx.filter_vmap(lambda x, y: x.log_prob(y))(\n        outputs.likelihood_distrib, batch\n    )\n\n    kl = outputs.variational_distrib.kl_divergence(prior_z)\n    elbo = log_likelihood - kl\n    return -jnp.mean(elbo), (log_likelihood, kl)\n\n\n@eqx.filter_jit\ndef update(\n    model,\n    rng_key,\n    opt_state,\n    batch,\n):\n    (val, (ll, kl)), grads = eqx.filter_value_and_grad(loss_fn, has_aux=True)(\n        model, rng_key, batch\n    )\n    updates, new_opt_state = optimizer.update(grads, opt_state)\n    model = eqx.apply_updates(model, updates)\n    return model, new_opt_state, val\n\n\nbatch_size = 128\nlearning_rate = 0.0005\ntraining_steps = 1000\neval_frequency = 100\nlatent_size = 2\n\nMNIST_IMAGE_SHAPE = (28, 28, 1)\noptimizer = optax.adam(learning_rate)\nkey = jax.random.key(0)\nkey, subkey = jax.random.split(key)\nmodel = VAE(subkey, input_size=784, latent_size=latent_size, hidden_size=512)\n\nopt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n\ntrain_ds = load_dataset(tfds.Split.TRAIN, batch_size)\nvalid_ds = load_dataset(tfds.Split.TEST, batch_size)\n\nlosses = []\n\nfor step in tqdm(range(training_steps)):\n    key, subkey = jax.random.split(key)\n    batch = jnp.array(next(valid_ds)[\"image\"])\n    subkey = jax.random.split(subkey, len(batch))\n    # val_loss, (ll, kl) = loss_fn(model, subkey, batch)\n    # break\n    model, opt_state, loss = update(model, subkey, opt_state, batch)\n    losses.append(loss)\n\n    if step % eval_frequency == 0:\n        key, subkey = jax.random.split(key)\n        batch = jnp.array(next(valid_ds)[\"image\"])\n        subkey = jax.random.split(subkey, len(batch))\n        val_loss, (ll, kl) = loss_fn(model, subkey, batch)\n        # results = eqx.filter_jit(eqx.filter_vmap(model))(batch[\"image\"], subkey)\n        # plt.imshow(results.image[0])\n        # plt.show()\n        print(\n            f\"STEP: {step}; Validation -ELBO: {val_loss}, LL {ll.mean()}, KL \\\n                {kl.mean()}\"\n        )\n</code></pre> <pre><code>plt.plot(losses)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"-ELBO\")\nplt.show()\n</code></pre> <p></p> <p>For such a small latent space, we can visualize a nice representation of the output. </p> <pre><code># from: https://keras.io/examples/generative/vae/#display-a-grid-of-sampled-digits\n\nimport numpy as np\n\n\ndef plot_latent_space(vae, n=30, figsize=15):\n    # display a n*n 2D manifold of digits\n    digit_size = 28\n    scale = 1.0\n    figure = np.zeros((digit_size * n, digit_size * n))\n    # linearly spaced coordinates corresponding to the 2D plot\n    # of digit classes in the latent space\n    grid_x = jnp.linspace(-scale, scale, n)\n    grid_y = jnp.linspace(-scale, scale, n)[::-1]\n\n    for i, yi in enumerate(grid_y):\n        for j, xi in enumerate(grid_x):\n            z_sample = jnp.array([xi, yi])\n            # convert logits to probs\n            digit = jax.nn.sigmoid(eqx.filter_jit(vae.decoder)(z_sample).squeeze())\n            figure[\n                i * digit_size : (i + 1) * digit_size,\n                j * digit_size : (j + 1) * digit_size,\n            ] = digit\n\n    plt.figure(figsize=(figsize, figsize))\n    start_range = digit_size // 2\n    end_range = n * digit_size + start_range\n    pixel_range = jnp.arange(start_range, end_range, digit_size)\n    sample_range_x = jnp.trunc(10 * grid_x) / 10\n    sample_range_y = jnp.trunc(10 * grid_y) / 10\n    plt.xticks(pixel_range, sample_range_x)\n    plt.yticks(pixel_range, sample_range_y)\n    plt.xlabel(\"z[0]\")\n    plt.ylabel(\"z[1]\")\n    plt.imshow(figure, cmap=\"Greys_r\")\n    plt.show()\n\n\nplot_latent_space(model)\n</code></pre> <p></p> <pre><code>\n</code></pre>"},{"location":"examples/02_mixture_models/","title":"Mixture Models","text":"<p>In this tutorial, we will look at Gaussian and Bernoulli mixture models. These mixture models are defined by mixing distribution (categorical) which is responsible for defining the distribution over the component distributions. We will train these models to generatively model MNIST data sets. We can train them with both expectation maximization (EM) or gradient descent based approaches.</p> <pre><code>import equinox as eqx\nimport jax\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optax\nimport tensorflow_datasets as tfds\nfrom jax import numpy as jnp\nfrom jax.scipy.special import expit, logit\nfrom tqdm.notebook import tqdm\n\nfrom distreqx.distributions import (\n    Bernoulli,\n    Categorical,\n    Independent,\n    MixtureSameFamily,\n    Normal,\n)\n</code></pre> <pre><code>N = 2_000\nmnist_data = tfds.load(\"mnist\")[\"train\"]\nmnist_data = tfds.as_numpy(mnist_data)\nmnist_data = jnp.array(\n    [data[\"image\"] for count, data in enumerate(mnist_data) if count &lt; N]\n)\n</code></pre> <pre><code>2025-08-11 12:24:18.473070: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n</code></pre> <pre><code>new_data = mnist_data / 255.0\nnew_data.shape\n</code></pre> <pre><code>(2000, 28, 28, 1)\n</code></pre> <pre><code>class GMM(eqx.Module):\n    _model: MixtureSameFamily\n\n    def __init__(self, K, n_vars, rng_key):\n        mixing_coeffs = jax.random.uniform(rng_key, (K,), minval=100, maxval=200)\n        mixing_coeffs = mixing_coeffs / mixing_coeffs.sum()\n        initial_probs = jnp.full((K, n_vars), 1.0 / K)\n\n        self._model = MixtureSameFamily(\n            mixture_distribution=Categorical(probs=mixing_coeffs),\n            components_distribution=Independent(\n                Normal(initial_probs, 0.2 * jnp.ones_like(initial_probs))\n            ),\n        )\n\n    @property\n    def mixing_coeffs(self):\n        return self._model.mixture_distribution.probs\n\n    @property\n    def probs(self):\n        return self._model.components_distribution.distribution.loc\n\n    @property\n    def model(self):\n        return self._model\n\n    def responsibilities(self, observations):\n        return jnp.nan_to_num(self._model.posterior_marginal(observations).probs)\n\n    def expected_log_likelihood(self, observations):\n        return jnp.nan_to_num(self._model.log_prob(observations))\n\n    def em_step(self, observations):\n        n_obs, _ = observations.shape\n\n        res = eqx.filter_vmap(self.responsibilities)(observations)\n        sum_res = jnp.sum(res, axis=0)\n        mus = jax.vmap(\n            lambda c, d: jax.vmap(lambda a, b: a * b, in_axes=(0, None))(c, d)\n        )(res, observations)\n        mus = jax.vmap(lambda a, b: a / b)(jnp.sum(mus, axis=0), sum_res)\n        # constant sigma\n        return sum_res / n_obs, mus\n\n    def plot(self, n_row, n_col):\n        if n_row * n_col != len(self.mixing_coeffs):\n            raise TypeError(\n                \"The number of rows and columns does not match with \"\n                \"the number of component distribution.\"\n            )\n        fig, axes = plt.subplots(n_row, n_col)\n\n        for (coeff, mean), ax in zip(\n            zip(self.mixing_coeffs, self.probs), axes.flatten()\n        ):\n            ax.imshow(mean.reshape((28, 28)), cmap=\"grey\")\n            ax.set_title(\"%1.2f\" % coeff)\n            ax.axis(\"off\")\n\n        fig.tight_layout(pad=1.0)\n        plt.show()\n</code></pre> <p>Here we have some manual updates, this is in general not necessary, but can be helpful to have more direct control over the parameters (especially given the nature of modules to be very deep).</p> <pre><code>@eqx.filter_jit\ndef update_model_params(model, params):\n    model = eqx.tree_at(\n        lambda x: x._model.mixture_distribution._probs, model, params[0]\n    )\n    model = eqx.tree_at(\n        lambda x: x._model.components_distribution.distribution._loc, model, params[1]\n    )\n    return model\n\n\n@eqx.filter_jit\ndef train_step(model, params, observations):\n    model = update_model_params(model, params)\n    log_likelihood = jnp.sum(\n        eqx.filter_vmap(model.expected_log_likelihood)(observations)\n    )\n    mixing_coeffs, probs = model.em_step(observations)\n    return (mixing_coeffs, probs), log_likelihood\n\n\ndata = new_data.reshape((N, 784))\ndata = ((data &gt; 0.0).astype(\"int32\") + 1e-8) * 0.99\nkey = jax.random.PRNGKey(0)\nkey, subkey = jax.random.split(key)\nK = 20\nmodel = GMM(K, 784, subkey)\nparams = (model.mixing_coeffs, model.probs)\n\nbatch_size = 2000\ninner = N // batch_size\nouter = 20\nlosses = []\n\nfor epoch in tqdm(range(outer)):\n    inner_loss = []\n    for batch in range(inner):\n        key, subkey = jax.random.split(key)\n        inds = jax.random.randint(\n            subkey, minval=0, maxval=len(data), shape=(batch_size,)\n        )\n        real_batch = data[inds]\n        key, subkey = jax.random.split(key)\n        params, loss = train_step(model, params, real_batch)\n        model = update_model_params(model, params)\n        inner_loss.append(loss)\n\n    losses.append(np.mean(inner_loss))\n\n    if epoch % 10 == 0:\n        model.plot(5, 4)\n</code></pre> <pre><code>0%|          | 0/20 [00:00&lt;?, ?it/s]\n</code></pre> <p></p> <p></p> <p>Now lets plot the EM loss and the final generated images for the GMM.</p> <pre><code>plt.figure(figsize=(3, 2))\nplt.plot(-np.array(losses))\nplt.yscale(\"log\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"NLL\")\nplt.title(\"GMM EM\")\nplt.show()\n\nmodel.plot(4, 5)\n</code></pre> <p></p> <p></p> <p>Now we can repeat the process, but with SGD this time instead of EM. Notice the different failure mode? Difficulties in training mixtures models with SGD are well known (and there are many variants of EM to help overcome these failure modes).</p> <pre><code>@eqx.filter_jit\ndef update_model_params(model, params):\n    params = (jax.nn.softmax(params[0]), params[1])\n    model = eqx.tree_at(\n        lambda x: x._model.mixture_distribution._probs, model, params[0]\n    )\n    model = eqx.tree_at(\n        lambda x: x._model.components_distribution.distribution._loc, model, params[1]\n    )\n    return model\n\n\ndef loss_fn(model, params, inp):\n    model = update_model_params(model, params)\n    return -model.expected_log_likelihood(inp)\n\n\ndef vmap_loss(params, model, batch):\n    return jnp.mean(\n        eqx.filter_vmap(loss_fn, in_axes=(None, None, 0))(model, params, batch)\n    )\n\n\n@eqx.filter_jit\ndef step(model, params, batch, opt_state):\n    loss, grads = eqx.filter_value_and_grad(vmap_loss)(params, model, batch)\n    update, opt_state = optimizer.update(grads, opt_state, params)\n    params = eqx.apply_updates(params, update)\n    return params, opt_state, loss\n\n\ndata = new_data.reshape((N, 784))\ndata = (data &gt; 0.0).astype(\"int32\")\nkey = jax.random.PRNGKey(0)\nkey, subkey = jax.random.split(key)\nK = 20\nmodel = GMM(K, 784, subkey)\nparams = (model.mixing_coeffs, model.probs)\noptimizer = optax.adam(1e-1)\nopt_state = optimizer.init((jax.nn.softmax(model.mixing_coeffs), model.probs))\n\nbatch_size = 1000\ninner = N // batch_size\nouter = 100\nlosses = []\n\nfor epoch in tqdm(range(outer)):\n    inner_loss = []\n    for batch in range(inner):\n        key, subkey = jax.random.split(key)\n        inds = jax.random.randint(\n            subkey, minval=0, maxval=len(data), shape=(batch_size,)\n        )\n        real_batch = data[inds]\n        key, subkey = jax.random.split(key)\n        params, opt_state, loss = step(model, params, real_batch, opt_state)\n        model = update_model_params(model, params)\n        inner_loss.append(loss)\n\n    losses.append(np.mean(inner_loss))\n    if epoch % 40 == 0:\n        model.plot(5, 4)\n</code></pre> <pre><code>0%|          | 0/100 [00:00&lt;?, ?it/s]\n</code></pre> <p></p> <p></p> <p></p> <pre><code>plt.figure(figsize=(3, 2))\nplt.plot(losses)\nplt.yscale(\"log\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"NLL\")\nplt.title(\"GMM SGD\")\nplt.show()\n\nmodel.plot(4, 5)\n</code></pre> <p></p> <p></p>"},{"location":"examples/02_mixture_models/#bernoulli-mixture-models","title":"Bernoulli Mixture Models","text":"<p>Now we can repeat the exact process as before, but with the component distribution being a Bernoulli. Once you've completed this tutorial, try it with a different distribution and see how it works!</p> <pre><code>class BMM(eqx.Module):\n    _model: MixtureSameFamily\n\n    def __init__(self, K, n_vars, rng_key):\n        mixing_coeffs = jax.random.uniform(rng_key, (K,), minval=100, maxval=200)\n        mixing_coeffs = mixing_coeffs / mixing_coeffs.sum()\n        initial_probs = jnp.full((K, n_vars), 1.0 / K)\n\n        self._model = MixtureSameFamily(\n            mixture_distribution=Categorical(probs=mixing_coeffs),\n            components_distribution=Independent(Bernoulli(probs=initial_probs)),\n        )\n\n    @property\n    def mixing_coeffs(self):\n        return self._model.mixture_distribution.probs\n\n    @property\n    def probs(self):\n        return self._model.components_distribution.distribution.probs\n\n    @property\n    def model(self):\n        return self._model\n\n    def responsibilities(self, observations):\n        return jnp.nan_to_num(self._model.posterior_marginal(observations).probs)\n\n    def expected_log_likelihood(self, observations):\n        return jnp.nan_to_num(self._model.log_prob(observations))\n\n    def em_step(self, observations):\n        n_obs, _ = observations.shape\n\n        def m_step_per_bernoulli(responsibility):\n            norm_const = responsibility.sum()\n            mu = jnp.sum(responsibility[:, None] * observations, axis=0) / norm_const\n            return jax.numpy.nan_to_num(mu), jax.numpy.nan_to_num(norm_const)\n\n        mus, ns = eqx.filter_vmap(m_step_per_bernoulli, in_axes=(1))(\n            eqx.filter_vmap(self.responsibilities)(observations)\n        )\n        return ns / n_obs, mus\n\n    def plot(self, n_row, n_col):\n        if n_row * n_col != len(self.mixing_coeffs):\n            raise TypeError(\n                \"The number of rows and columns does not match with the \"\n                \"number of component distribution.\"\n            )\n        fig, axes = plt.subplots(n_row, n_col)\n\n        for (coeff, mean), ax in zip(\n            zip(self.mixing_coeffs, self.probs), axes.flatten()\n        ):\n            ax.imshow(mean.reshape((28, 28)), cmap=\"grey\")\n            ax.set_title(\"%1.2f\" % coeff)\n            ax.axis(\"off\")\n\n        fig.tight_layout(pad=1.0)\n        plt.show()\n</code></pre> <pre><code>@eqx.filter_jit\ndef update_model_params(model, params):\n    model = eqx.tree_at(\n        lambda x: x._model.mixture_distribution._probs, model, params[0]\n    )\n    model = eqx.tree_at(\n        lambda x: x._model.components_distribution.distribution._probs, model, params[1]\n    )\n    return model\n\n\n@eqx.filter_jit\ndef train_step(model, params, observations):\n    model = update_model_params(model, params)\n    log_likelihood = jnp.sum(\n        eqx.filter_vmap(model.expected_log_likelihood)(observations)\n    )\n    mixing_coeffs, probs = model.em_step(observations)\n    return (mixing_coeffs, probs), log_likelihood\n\n\ndata = new_data.reshape((N, 784))\ndata = ((data &gt; 0.0).astype(\"int32\") + 1e-8) * 0.99\nkey = jax.random.PRNGKey(0)\nkey, subkey = jax.random.split(key)\nK = 20\nmodel = BMM(K, 784, subkey)\nparams = (model.mixing_coeffs, model.probs)\n\nbatch_size = 2000\ninner = N // batch_size\nouter = 20\nlosses = []\n\nfor epoch in tqdm(range(outer)):\n    inner_loss = []\n    for batch in range(inner):\n        key, subkey = jax.random.split(key)\n        inds = jax.random.randint(\n            subkey, minval=0, maxval=len(data), shape=(batch_size,)\n        )\n        real_batch = data[inds]\n        key, subkey = jax.random.split(key)\n        params, loss = train_step(model, params, real_batch)\n        model = update_model_params(model, params)\n        inner_loss.append(loss)\n\n    losses.append(np.mean(inner_loss))\n\n    if epoch % 10 == 0:\n        model.plot(5, 4)\n</code></pre> <pre><code>0%|          | 0/20 [00:00&lt;?, ?it/s]\n</code></pre> <p></p> <p></p> <pre><code>plt.figure(figsize=(3, 2))\nplt.plot(-np.array(losses))\nplt.yscale(\"log\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"NLL\")\nplt.title(\"GMM EM\")\nplt.show()\nmodel.plot(4, 5)\n</code></pre> <p></p> <p></p> <pre><code># todo: this is annoying, fix\n@eqx.filter_jit\ndef update_model_params(model, params):\n    params = (jax.nn.softmax(params[0]), expit(params[1]))\n    model = eqx.tree_at(\n        lambda x: x._model.mixture_distribution._probs, model, params[0]\n    )\n    model = eqx.tree_at(\n        lambda x: x._model.components_distribution.distribution._probs, model, params[1]\n    )\n    return model\n\n\n@eqx.filter_jit\ndef step(model, params, batch, opt_state):\n    batch = batch.astype(\"float32\")\n    loss, grads = eqx.filter_value_and_grad(vmap_loss)(params, model, batch)\n    update, opt_state = optimizer.update(grads, opt_state, params)\n    params = eqx.apply_updates(params, update)\n    return params, opt_state, loss\n\n\ndata = new_data.reshape((N, 784))\ndata = (data &gt; 0.0).astype(\"int32\")\nkey = jax.random.PRNGKey(0)\nkey, subkey = jax.random.split(key)\nK = 20\nmodel = BMM(K, 784, subkey)\nparams = (model.mixing_coeffs, model.probs)\noptimizer = optax.adam(1e-1)\nopt_state = optimizer.init((jax.nn.softmax(model.mixing_coeffs), logit(model.probs)))\n\nbatch_size = 1000\ninner = N // batch_size\nouter = 100\nlosses = []\n\nfor epoch in tqdm(range(outer)):\n    inner_loss = []\n    for batch in range(inner):\n        key, subkey = jax.random.split(key)\n        inds = jax.random.randint(\n            subkey, minval=0, maxval=len(data), shape=(batch_size,)\n        )\n        real_batch = data[inds]\n        key, subkey = jax.random.split(key)\n        params, opt_state, loss = step(model, params, real_batch, opt_state)\n        model = update_model_params(model, params)\n        inner_loss.append(loss)\n\n    losses.append(np.mean(inner_loss))\n    if epoch % 40 == 0:\n        model.plot(5, 4)\n</code></pre> <pre><code>0%|          | 0/100 [00:00&lt;?, ?it/s]\n</code></pre> <p></p> <p></p> <p></p> <pre><code>plt.figure(figsize=(3, 2))\nplt.plot(losses)\nplt.yscale(\"log\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"NLL\")\nplt.title(\"BMM SGD\")\nplt.show()\nmodel.plot(4, 5)\n</code></pre> <p></p> <p></p> <pre><code>\n</code></pre>"},{"location":"examples/03_normalizing_flow/","title":"Normalizing Flows","text":"<p>In this tutorial, adapted from this blog post, we will implement a simple RealNVP normalizing flow. Normalizing flows are a class of generative models which are advantageous due to their explicit representation of densities and likelihoods, but come at a cost of requiring computable jacobian determinants and invertible layers. For an introduction to normalizing flows, see Papamakarios et al. (2021).</p> References <p>RealNVP: <pre><code>@inproceedings{dinh2017density,\n    title={Density estimation using {Real-NVP}},\n    author={Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},\n    booktitle={International Conference on Learning Representations},\n    year={2017},\n}\n</code></pre></p> <p>Normalizing flows survey: <pre><code>@article{papamakarios2021normalizing,\n    title={Normalizing Flows for Probabilistic Modeling and Inference},\n    author={Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez \n            and Mohamed, Shakir and Lakshminarayanan, Balaji},\n    journal={Journal of Machine Learning Research},\n    volume={22},\n    number={57},\n    pages={1--64},\n    year={2021},\n}\n</code></pre></p> <pre><code>import equinox as eqx\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport optax\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\n\nfrom distreqx import bijectors, distributions\n</code></pre> <p>Let's define our simple dataset.</p> <pre><code>n_samples = 20000\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05)\nX, y = noisy_moons\nX = StandardScaler().fit_transform(X)\nxlim, ylim = [-3, 3], [-3, 3]\nplt.scatter(X[:, 0], X[:, 1], s=10, color=\"red\")\nplt.xlim(xlim)\nplt.ylim(ylim)\n</code></pre> <pre><code>(-3.0, 3.0)\n</code></pre> <p></p> <p>Now we can program our custom bijector.</p> <pre><code>class RNVP(\n    bijectors.AbstractFwdLogDetJacBijector, bijectors.AbstractInvLogDetJacBijector\n):\n    _is_constant_jacobian: bool\n    _is_constant_log_det: bool\n    sig_net: eqx.Module\n    mu_net: eqx.Module\n    d: int\n    k: int\n    flip: bool\n\n    def __init__(self, d, k, flip, key, hidden=32):\n        self._is_constant_jacobian = False\n        self._is_constant_log_det = False\n        self.flip = flip\n        keys = jax.random.split(key, 4)\n\n        self.d = d\n        self.k = k\n\n        self.sig_net = eqx.nn.Sequential(\n            [\n                eqx.nn.Linear(k, hidden, key=keys[0]),\n                eqx.nn.Lambda(jax.nn.swish),\n                eqx.nn.Linear(hidden, d - k, key=keys[1]),\n            ]\n        )\n\n        self.mu_net = eqx.nn.Sequential(\n            [\n                eqx.nn.Linear(k, hidden, key=keys[2]),\n                eqx.nn.Lambda(jax.nn.swish),\n                eqx.nn.Linear(hidden, d - k, key=keys[3]),\n            ]\n        )\n\n    def forward_and_log_det(self, x):\n        x1, x2 = x[: self.k], x[self.k :]\n\n        if self.flip:\n            x1, x2 = x2, x1\n\n        sig = self.sig_net(x1)\n        z1, z2 = x1, x2 * jnp.exp(sig) + self.mu_net(x1)\n\n        if self.flip:\n            z1, z2 = z2, z1\n\n        z_hat = jnp.concatenate([z1, z2])\n        log_det = jnp.sum(sig)\n\n        return z_hat, log_det\n\n    def inverse(self, y):\n        z1, z2 = y[: self.k], y[self.k :]\n\n        if self.flip:\n            z1, z2 = z2, z1\n\n        x1 = z1\n        x2 = (z2 - self.mu_net(z1)) * jnp.exp(-self.sig_net(z1))\n\n        if self.flip:\n            x1, x2 = x2, x1\n\n        x_hat = jnp.concatenate([x1, x2])\n        return x_hat\n\n    def forward(self, x):\n        y, _ = self.forward_and_log_det(x)\n        return y\n\n    def inverse_and_log_det(self, y):\n        raise NotImplementedError(\n            f\"Bijector {self.name} does not implement `inverse_and_log_det`.\"\n        )\n\n    def same_as(self, other) -&gt; bool:\n        return type(other) is RNVP\n</code></pre> <p>Since we want to stack these together, we can use a chain bijector to accomplish this.</p> <pre><code>n = 3\nkey = jax.random.key(0)\nkeys = jax.random.split(key, n)\nbijector_chain = bijectors.Chain([RNVP(2, 1, i % 2, keys[i], 600) for i in range(n)])\n</code></pre> <p>Flows map p(x) -&gt; p(z) via a function F (samples are generated via F^-1(z)). In general, p(z) is chosen to have some tractable form for sampling and calculating log probabilities. A common choice is Gaussian, which we go with here.</p> <pre><code>base_distribution = distributions.MultivariateNormalDiag(jnp.zeros(2))\nbase_distribution_sample = eqx.filter_vmap(base_distribution.sample)\nbase_distribution_log_prob = eqx.filter_vmap(base_distribution.log_prob)\n</code></pre> <p>Here we plot the initial, untrained, samples.</p> <pre><code>num_samples = 2000\nbase_samples = base_distribution_sample(jax.random.split(key, num_samples))\ntransformed_samples = eqx.filter_vmap(bijector_chain.inverse)(base_samples)\n\nplt.scatter(\n    transformed_samples[:, 0],\n    transformed_samples[:, 1],\n    s=10,\n    color=\"blue\",\n    label=\"Untrained F^-1\",\n)\nplt.scatter(base_samples[:, 0], base_samples[:, 1], s=10, color=\"red\", label=\"Base\")\nplt.legend()\nplt.xlim(xlim)\nplt.ylim(ylim)\nplt.title(\"Initial Samples\")\nplt.show()\n</code></pre> <p></p> <pre><code>learning_rate = 1e-3\nnum_iters = 1000\nbatch_size = 128\n\noptimizer = optax.adam(learning_rate)\nopt_state = optimizer.init(eqx.filter(bijector_chain, eqx.is_inexact_array))\n\n\ndef log_prob(params: bijectors.AbstractBijector, data):\n    f_inv, log_det = params.forward_and_log_det(data)\n    log_prob = base_distribution.log_prob(f_inv) + log_det\n    return log_prob\n\n\ndef loss(params, batch):\n    return -jnp.mean(eqx.filter_vmap(log_prob, in_axes=(None, 0))(params, batch))\n\n\n@eqx.filter_jit\ndef update(model, batch, opt_state, optimizer):\n    val, grads = eqx.filter_value_and_grad(loss)(model, batch)\n    update, opt_state = optimizer.update(grads, opt_state)\n    model = eqx.apply_updates(model, update)\n    return model, opt_state, val\n\n\nlosses = []\n\nfor i in range(num_iters):\n    if i % 500 == 0:\n        print(i)\n    batch_indices = jax.random.choice(\n        key, jnp.arange(X.shape[0]), (batch_size,), replace=False\n    )\n    batch = X[batch_indices]\n    bijector_chain, opt_state, loss_val = update(\n        bijector_chain, batch, opt_state, optimizer\n    )\n    losses.append(loss_val)\n</code></pre> <pre><code>0\n500\n</code></pre> <pre><code>plt.plot(losses)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss Over Time\")\nplt.show()\n</code></pre> <p></p> <p>After training we can plot both F(x) (to see where the true data ends up in our sampled space) and F^-1(z) to generate new samples.</p> <pre><code>trained_params = bijector_chain\n\ntransformed_samples_trained = eqx.filter_vmap(bijector_chain.inverse)(base_samples)\n\nplt.scatter(\n    transformed_samples[:, 0], transformed_samples[:, 1], s=10, label=\"Initial F^-1\"\n)\nplt.scatter(X[:, 0], X[:, 1], s=5, label=\"Data\")\nplt.scatter(\n    transformed_samples_trained[:, 0],\n    transformed_samples_trained[:, 1],\n    s=10,\n    label=\"Trained F^-1\",\n)\nplt.xlim(xlim)\nplt.ylim(ylim)\nplt.legend()\nplt.show()\n</code></pre> <p></p> <pre><code>data_to_noise = eqx.filter_vmap(bijector_chain.forward)(X)\n\nplt.scatter(data_to_noise[:, 0], data_to_noise[:, 1], s=10, label=\"Z = F(X)\")\nplt.xlim(xlim)\nplt.ylim(ylim)\nplt.legend()\nplt.show()\n</code></pre> <p></p> <pre><code>\n</code></pre>"}]}